{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSD Trainer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1yKs-aSZWnr",
        "colab_type": "text"
      },
      "source": [
        "#Ensure Version is Correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb9HiC4cXg8i",
        "colab_type": "code",
        "outputId": "414ef63b-f1df-472a-d04b-b00ea27a36ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.__version__ == '1.14.0':\n",
        "  print('Version Check passed')\n",
        "else:\n",
        "  print('Incompatible version')\n",
        "  input('Interrupt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Version Check passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLvyjsZ-dDxB",
        "colab_type": "text"
      },
      "source": [
        "#Compling models and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7nf_n0ZjC3D",
        "colab_type": "code",
        "outputId": "d592e506-ee0a-43b1-8fa9-c98059bd82d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools\n",
        "!pip install pandas\n",
        "!pip install opencv-python\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'models' already exists and is not an empty directory.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python2.7/dist-packages (3.4.5.20)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from opencv-python) (1.16.4)\n",
            "/content/models/research\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0919 16:27:45.677733 140455789946752 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0919 16:27:45.809006 140455789946752 deprecation_wrapper.py:119] From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0919 16:27:45.819181 140455789946752 deprecation_wrapper.py:119] From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Running tests under Python 2.7.15: /usr/bin/python2\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.137s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3U1GIVxlw-M",
        "colab_type": "text"
      },
      "source": [
        "#Download Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW8Y9FQClgQk",
        "colab_type": "code",
        "outputId": "13083aed-1c0a-4894-a9bd-29b513cbc167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "!wget http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
        "!tar -xvf ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n",
            "--2019-09-19 16:27:48--  http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.133.128, 2a00:1450:400c:c07::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.133.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51025348 (49M) [application/x-tar]\n",
            "Saving to: ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz.1’\n",
            "\n",
            "ssdlite_mobilenet_v 100%[===================>]  48.66M  93.1MB/s    in 0.5s    \n",
            "\n",
            "2019-09-19 16:27:49 (93.1 MB/s) - ‘ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz.1’ saved [51025348/51025348]\n",
            "\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/checkpoint\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt.data-00000-of-00001\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt.meta\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt.index\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/saved_model.pb\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/variables/\n",
            "ssdlite_mobilenet_v2_coco_2018_05_09/saved_model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJZhJWLVVXb4",
        "colab_type": "text"
      },
      "source": [
        "#Copy Dataset and API Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5HxSxp1nXwV",
        "colab_type": "code",
        "outputId": "8e5c4200-ecb1-48f0-e4ab-ced53084bf4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content\n",
        "!rm -r ObjDet_Demo\n",
        "!git clone --quiet https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.git\n",
        "!mv /content/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10 /content/ObjDet_Demo\n",
        "\n",
        "%cd /content/ObjDet_Demo\n",
        "!rm -r training/*\n",
        "!rm -r inference_graph/*\n",
        "!rm -r images/*.*\n",
        "!rm -r images/test\n",
        "!rm -r images/train\n",
        "\n",
        "%cd /content/drive/\"My Drive\"/Images\n",
        "!cp -rv test /content/ObjDet_Demo/images/test\n",
        "!cp -rv train /content/ObjDet_Demo/images/train\n",
        "\n",
        "!mv -v /content/ObjDet_Demo/* /content/models/research/object_detection/\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/ObjDet_Demo\n",
            "/content/drive/My Drive/Images\n",
            "'test' -> '/content/ObjDet_Demo/images/test'\n",
            "'test/20190719_175906.xml' -> '/content/ObjDet_Demo/images/test/20190719_175906.xml'\n",
            "'test/20190719_180009.jpg' -> '/content/ObjDet_Demo/images/test/20190719_180009.jpg'\n",
            "'test/20190719_175949.xml' -> '/content/ObjDet_Demo/images/test/20190719_175949.xml'\n",
            "'test/20190719_175949.jpg' -> '/content/ObjDet_Demo/images/test/20190719_175949.jpg'\n",
            "'test/20190719_180009.xml' -> '/content/ObjDet_Demo/images/test/20190719_180009.xml'\n",
            "'test/20190719_180050.jpg' -> '/content/ObjDet_Demo/images/test/20190719_180050.jpg'\n",
            "'test/20190719_180050.xml' -> '/content/ObjDet_Demo/images/test/20190719_180050.xml'\n",
            "'test/20190719_180116.jpg' -> '/content/ObjDet_Demo/images/test/20190719_180116.jpg'\n",
            "'test/20190719_180102.xml' -> '/content/ObjDet_Demo/images/test/20190719_180102.xml'\n",
            "'test/20190719_180102.jpg' -> '/content/ObjDet_Demo/images/test/20190719_180102.jpg'\n",
            "'test/20190719_180116.xml' -> '/content/ObjDet_Demo/images/test/20190719_180116.xml'\n",
            "'test/20190719_135831.xml' -> '/content/ObjDet_Demo/images/test/20190719_135831.xml'\n",
            "'test/20190719_180125.xml' -> '/content/ObjDet_Demo/images/test/20190719_180125.xml'\n",
            "'test/20190719_135808.xml' -> '/content/ObjDet_Demo/images/test/20190719_135808.xml'\n",
            "'test/20190719_180125.jpg' -> '/content/ObjDet_Demo/images/test/20190719_180125.jpg'\n",
            "'test/20190719_135816.xml' -> '/content/ObjDet_Demo/images/test/20190719_135816.xml'\n",
            "'test/20190719_135834.jpg' -> '/content/ObjDet_Demo/images/test/20190719_135834.jpg'\n",
            "'test/20190719_135831.jpg' -> '/content/ObjDet_Demo/images/test/20190719_135831.jpg'\n",
            "'test/20190719_135808.jpg' -> '/content/ObjDet_Demo/images/test/20190719_135808.jpg'\n",
            "'test/20190719_135816.jpg' -> '/content/ObjDet_Demo/images/test/20190719_135816.jpg'\n",
            "'test/20190719_135834.xml' -> '/content/ObjDet_Demo/images/test/20190719_135834.xml'\n",
            "'test/20190719_135847.jpg' -> '/content/ObjDet_Demo/images/test/20190719_135847.jpg'\n",
            "'test/20190719_140049_HDR.xml' -> '/content/ObjDet_Demo/images/test/20190719_140049_HDR.xml'\n",
            "'test/20190719_135847.xml' -> '/content/ObjDet_Demo/images/test/20190719_135847.xml'\n",
            "'test/20190719_140036.xml' -> '/content/ObjDet_Demo/images/test/20190719_140036.xml'\n",
            "'test/20190719_140049_HDR.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140049_HDR.jpg'\n",
            "'test/20190719_140055.xml' -> '/content/ObjDet_Demo/images/test/20190719_140055.xml'\n",
            "'test/20190719_140055.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140055.jpg'\n",
            "'test/20190719_140059.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140059.jpg'\n",
            "'test/20190719_140036.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140036.jpg'\n",
            "'test/20190719_140059.xml' -> '/content/ObjDet_Demo/images/test/20190719_140059.xml'\n",
            "'test/20190719_140156.xml' -> '/content/ObjDet_Demo/images/test/20190719_140156.xml'\n",
            "'test/20190719_140219.xml' -> '/content/ObjDet_Demo/images/test/20190719_140219.xml'\n",
            "'test/20190719_140219.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140219.jpg'\n",
            "'test/20190719_140211.xml' -> '/content/ObjDet_Demo/images/test/20190719_140211.xml'\n",
            "'test/20190719_140211.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140211.jpg'\n",
            "'test/20190719_140235.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140235.jpg'\n",
            "'test/20190719_140235.xml' -> '/content/ObjDet_Demo/images/test/20190719_140235.xml'\n",
            "'test/20190719_175625.jpg' -> '/content/ObjDet_Demo/images/test/20190719_175625.jpg'\n",
            "'test/20190719_140156.jpg' -> '/content/ObjDet_Demo/images/test/20190719_140156.jpg'\n",
            "'test/20190719_175625.xml' -> '/content/ObjDet_Demo/images/test/20190719_175625.xml'\n",
            "'test/20190719_175706.xml' -> '/content/ObjDet_Demo/images/test/20190719_175706.xml'\n",
            "'test/20190719_175706.jpg' -> '/content/ObjDet_Demo/images/test/20190719_175706.jpg'\n",
            "'test/20190719_175654.xml' -> '/content/ObjDet_Demo/images/test/20190719_175654.xml'\n",
            "'test/20190719_175829.xml' -> '/content/ObjDet_Demo/images/test/20190719_175829.xml'\n",
            "'test/20190719_175829.jpg' -> '/content/ObjDet_Demo/images/test/20190719_175829.jpg'\n",
            "'test/20190719_175640.xml' -> '/content/ObjDet_Demo/images/test/20190719_175640.xml'\n",
            "'test/20190719_175654.jpg' -> '/content/ObjDet_Demo/images/test/20190719_175654.jpg'\n",
            "'test/20190719_175906.jpg' -> '/content/ObjDet_Demo/images/test/20190719_175906.jpg'\n",
            "'test/20190719_175640.jpg' -> '/content/ObjDet_Demo/images/test/20190719_175640.jpg'\n",
            "'train' -> '/content/ObjDet_Demo/images/train'\n",
            "'train/20190719_175510.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175510.jpg'\n",
            "'train/20190719_175527.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175527.jpg'\n",
            "'train/20190719_175510.xml' -> '/content/ObjDet_Demo/images/train/20190719_175510.xml'\n",
            "'train/20190719_175527.xml' -> '/content/ObjDet_Demo/images/train/20190719_175527.xml'\n",
            "'train/20190719_175522.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175522.jpg'\n",
            "'train/20190719_175522.xml' -> '/content/ObjDet_Demo/images/train/20190719_175522.xml'\n",
            "'train/20190719_175532.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175532.jpg'\n",
            "'train/20190719_175530.xml' -> '/content/ObjDet_Demo/images/train/20190719_175530.xml'\n",
            "'train/20190719_175530.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175530.jpg'\n",
            "'train/20190719_175532.xml' -> '/content/ObjDet_Demo/images/train/20190719_175532.xml'\n",
            "'train/20190719_175550.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175550.jpg'\n",
            "'train/20190718_231928.xml' -> '/content/ObjDet_Demo/images/train/20190718_231928.xml'\n",
            "'train/20190719_175550.xml' -> '/content/ObjDet_Demo/images/train/20190719_175550.xml'\n",
            "'train/20190719_175608.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175608.jpg'\n",
            "'train/20190718_231928.jpg' -> '/content/ObjDet_Demo/images/train/20190718_231928.jpg'\n",
            "'train/20190719_175608.xml' -> '/content/ObjDet_Demo/images/train/20190719_175608.xml'\n",
            "'train/20190718_231935.xml' -> '/content/ObjDet_Demo/images/train/20190718_231935.xml'\n",
            "'train/20190718_231931.jpg' -> '/content/ObjDet_Demo/images/train/20190718_231931.jpg'\n",
            "'train/20190718_231935.jpg' -> '/content/ObjDet_Demo/images/train/20190718_231935.jpg'\n",
            "'train/20190718_231931.xml' -> '/content/ObjDet_Demo/images/train/20190718_231931.xml'\n",
            "'train/20190718_231940.jpg' -> '/content/ObjDet_Demo/images/train/20190718_231940.jpg'\n",
            "'train/20190718_231953.xml' -> '/content/ObjDet_Demo/images/train/20190718_231953.xml'\n",
            "'train/20190718_231940.xml' -> '/content/ObjDet_Demo/images/train/20190718_231940.xml'\n",
            "'train/20190718_231947.xml' -> '/content/ObjDet_Demo/images/train/20190718_231947.xml'\n",
            "'train/20190718_231947.jpg' -> '/content/ObjDet_Demo/images/train/20190718_231947.jpg'\n",
            "'train/20190718_231953.jpg' -> '/content/ObjDet_Demo/images/train/20190718_231953.jpg'\n",
            "'train/20190718_232020.xml' -> '/content/ObjDet_Demo/images/train/20190718_232020.xml'\n",
            "'train/20190718_232020.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232020.jpg'\n",
            "'train/20190718_232005.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232005.jpg'\n",
            "'train/20190718_232005.xml' -> '/content/ObjDet_Demo/images/train/20190718_232005.xml'\n",
            "'train/20190718_232024.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232024.jpg'\n",
            "'train/20190718_232051.xml' -> '/content/ObjDet_Demo/images/train/20190718_232051.xml'\n",
            "'train/20190718_232051.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232051.jpg'\n",
            "'train/20190718_232054.xml' -> '/content/ObjDet_Demo/images/train/20190718_232054.xml'\n",
            "'train/20190718_232024.xml' -> '/content/ObjDet_Demo/images/train/20190718_232024.xml'\n",
            "'train/20190718_232054.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232054.jpg'\n",
            "'train/20190718_232112.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232112.jpg'\n",
            "'train/20190718_232112.xml' -> '/content/ObjDet_Demo/images/train/20190718_232112.xml'\n",
            "'train/20190718_232105.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232105.jpg'\n",
            "'train/20190718_232105.xml' -> '/content/ObjDet_Demo/images/train/20190718_232105.xml'\n",
            "'train/20190718_232119.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232119.jpg'\n",
            "'train/20190718_232119.xml' -> '/content/ObjDet_Demo/images/train/20190718_232119.xml'\n",
            "'train/20190718_232125.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232125.jpg'\n",
            "'train/20190718_232135.xml' -> '/content/ObjDet_Demo/images/train/20190718_232135.xml'\n",
            "'train/20190718_232135.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232135.jpg'\n",
            "'train/20190718_232125.xml' -> '/content/ObjDet_Demo/images/train/20190718_232125.xml'\n",
            "'train/20190718_232142.xml' -> '/content/ObjDet_Demo/images/train/20190718_232142.xml'\n",
            "'train/20190718_232147.xml' -> '/content/ObjDet_Demo/images/train/20190718_232147.xml'\n",
            "'train/20190718_232147.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232147.jpg'\n",
            "'train/20190718_232142.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232142.jpg'\n",
            "'train/20190718_232217.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232217.jpg'\n",
            "'train/20190718_232222.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232222.jpg'\n",
            "'train/20190718_232234.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232234.jpg'\n",
            "'train/20190718_232217.xml' -> '/content/ObjDet_Demo/images/train/20190718_232217.xml'\n",
            "'train/20190718_232222.xml' -> '/content/ObjDet_Demo/images/train/20190718_232222.xml'\n",
            "'train/20190718_232234.xml' -> '/content/ObjDet_Demo/images/train/20190718_232234.xml'\n",
            "'train/20190718_232248.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232248.jpg'\n",
            "'train/20190718_232310.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232310.jpg'\n",
            "'train/20190718_232248.xml' -> '/content/ObjDet_Demo/images/train/20190718_232248.xml'\n",
            "'train/20190718_232310.xml' -> '/content/ObjDet_Demo/images/train/20190718_232310.xml'\n",
            "'train/20190718_232326.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232326.jpg'\n",
            "'train/20190718_232353.xml' -> '/content/ObjDet_Demo/images/train/20190718_232353.xml'\n",
            "'train/20190718_232335.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232335.jpg'\n",
            "'train/20190718_232335.xml' -> '/content/ObjDet_Demo/images/train/20190718_232335.xml'\n",
            "'train/20190718_232353.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232353.jpg'\n",
            "'train/20190718_232326.xml' -> '/content/ObjDet_Demo/images/train/20190718_232326.xml'\n",
            "'train/20190718_232441.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232441.jpg'\n",
            "'train/20190718_232441.xml' -> '/content/ObjDet_Demo/images/train/20190718_232441.xml'\n",
            "'train/20190718_232428.xml' -> '/content/ObjDet_Demo/images/train/20190718_232428.xml'\n",
            "'train/20190718_232428.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232428.jpg'\n",
            "'train/20190718_232457.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232457.jpg'\n",
            "'train/20190718_232502.xml' -> '/content/ObjDet_Demo/images/train/20190718_232502.xml'\n",
            "'train/20190718_232457.xml' -> '/content/ObjDet_Demo/images/train/20190718_232457.xml'\n",
            "'train/20190718_232508.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232508.jpg'\n",
            "'train/20190718_232502.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232502.jpg'\n",
            "'train/20190718_232508.xml' -> '/content/ObjDet_Demo/images/train/20190718_232508.xml'\n",
            "'train/20190718_232538.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232538.jpg'\n",
            "'train/20190718_232538.xml' -> '/content/ObjDet_Demo/images/train/20190718_232538.xml'\n",
            "'train/20190718_232522.xml' -> '/content/ObjDet_Demo/images/train/20190718_232522.xml'\n",
            "'train/20190718_232522.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232522.jpg'\n",
            "'train/20190718_232637.xml' -> '/content/ObjDet_Demo/images/train/20190718_232637.xml'\n",
            "'train/20190718_232650.xml' -> '/content/ObjDet_Demo/images/train/20190718_232650.xml'\n",
            "'train/20190718_232650.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232650.jpg'\n",
            "'train/20190718_232703.xml' -> '/content/ObjDet_Demo/images/train/20190718_232703.xml'\n",
            "'train/20190718_232714.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232714.jpg'\n",
            "'train/20190718_232637.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232637.jpg'\n",
            "'train/20190718_232738.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232738.jpg'\n",
            "'train/20190718_232703.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232703.jpg'\n",
            "'train/20190718_232714.xml' -> '/content/ObjDet_Demo/images/train/20190718_232714.xml'\n",
            "'train/20190718_232738.xml' -> '/content/ObjDet_Demo/images/train/20190718_232738.xml'\n",
            "'train/20190718_232748.xml' -> '/content/ObjDet_Demo/images/train/20190718_232748.xml'\n",
            "'train/20190718_232801.xml' -> '/content/ObjDet_Demo/images/train/20190718_232801.xml'\n",
            "'train/20190718_232814.xml' -> '/content/ObjDet_Demo/images/train/20190718_232814.xml'\n",
            "'train/20190718_232756.xml' -> '/content/ObjDet_Demo/images/train/20190718_232756.xml'\n",
            "'train/20190718_232814.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232814.jpg'\n",
            "'train/20190718_232805.xml' -> '/content/ObjDet_Demo/images/train/20190718_232805.xml'\n",
            "'train/20190718_232801.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232801.jpg'\n",
            "'train/20190718_232805.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232805.jpg'\n",
            "'train/20190718_232756.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232756.jpg'\n",
            "'train/20190718_232748.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232748.jpg'\n",
            "'train/20190718_232831.xml' -> '/content/ObjDet_Demo/images/train/20190718_232831.xml'\n",
            "'train/20190718_232939.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232939.jpg'\n",
            "'train/20190718_232854.xml' -> '/content/ObjDet_Demo/images/train/20190718_232854.xml'\n",
            "'train/20190718_232854.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232854.jpg'\n",
            "'train/20190718_232957.xml' -> '/content/ObjDet_Demo/images/train/20190718_232957.xml'\n",
            "'train/20190718_233124.xml' -> '/content/ObjDet_Demo/images/train/20190718_233124.xml'\n",
            "'train/20190718_232939.xml' -> '/content/ObjDet_Demo/images/train/20190718_232939.xml'\n",
            "'train/20190718_232957.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232957.jpg'\n",
            "'train/20190718_232831.jpg' -> '/content/ObjDet_Demo/images/train/20190718_232831.jpg'\n",
            "'train/20190718_233124.jpg' -> '/content/ObjDet_Demo/images/train/20190718_233124.jpg'\n",
            "'train/20190718_233135.xml' -> '/content/ObjDet_Demo/images/train/20190718_233135.xml'\n",
            "'train/20190718_233141.xml' -> '/content/ObjDet_Demo/images/train/20190718_233141.xml'\n",
            "'train/20190718_233141.jpg' -> '/content/ObjDet_Demo/images/train/20190718_233141.jpg'\n",
            "'train/20190718_233235.jpg' -> '/content/ObjDet_Demo/images/train/20190718_233235.jpg'\n",
            "'train/20190718_233243.jpg' -> '/content/ObjDet_Demo/images/train/20190718_233243.jpg'\n",
            "'train/20190718_233243.xml' -> '/content/ObjDet_Demo/images/train/20190718_233243.xml'\n",
            "'train/20190718_233235.xml' -> '/content/ObjDet_Demo/images/train/20190718_233235.xml'\n",
            "'train/20190718_233227.jpg' -> '/content/ObjDet_Demo/images/train/20190718_233227.jpg'\n",
            "'train/20190718_233227.xml' -> '/content/ObjDet_Demo/images/train/20190718_233227.xml'\n",
            "'train/20190718_233135.jpg' -> '/content/ObjDet_Demo/images/train/20190718_233135.jpg'\n",
            "'train/20190718_233256.xml' -> '/content/ObjDet_Demo/images/train/20190718_233256.xml'\n",
            "'train/20190719_134607_HDR.jpg' -> '/content/ObjDet_Demo/images/train/20190719_134607_HDR.jpg'\n",
            "'train/20190719_134607_HDR.xml' -> '/content/ObjDet_Demo/images/train/20190719_134607_HDR.xml'\n",
            "'train/20190719_134555.xml' -> '/content/ObjDet_Demo/images/train/20190719_134555.xml'\n",
            "'train/20190719_134517.xml' -> '/content/ObjDet_Demo/images/train/20190719_134517.xml'\n",
            "'train/20190719_134537.xml' -> '/content/ObjDet_Demo/images/train/20190719_134537.xml'\n",
            "'train/20190718_233256.jpg' -> '/content/ObjDet_Demo/images/train/20190718_233256.jpg'\n",
            "'train/20190719_134537.jpg' -> '/content/ObjDet_Demo/images/train/20190719_134537.jpg'\n",
            "'train/20190719_134517.jpg' -> '/content/ObjDet_Demo/images/train/20190719_134517.jpg'\n",
            "'train/20190719_134555.jpg' -> '/content/ObjDet_Demo/images/train/20190719_134555.jpg'\n",
            "'train/20190719_134835.xml' -> '/content/ObjDet_Demo/images/train/20190719_134835.xml'\n",
            "'train/20190719_134838.jpg' -> '/content/ObjDet_Demo/images/train/20190719_134838.jpg'\n",
            "'train/20190719_135145_HDR.xml' -> '/content/ObjDet_Demo/images/train/20190719_135145_HDR.xml'\n",
            "'train/20190719_135127.xml' -> '/content/ObjDet_Demo/images/train/20190719_135127.xml'\n",
            "'train/20190719_135151_HDR.xml' -> '/content/ObjDet_Demo/images/train/20190719_135151_HDR.xml'\n",
            "'train/20190719_134838.xml' -> '/content/ObjDet_Demo/images/train/20190719_134838.xml'\n",
            "'train/20190719_134835.jpg' -> '/content/ObjDet_Demo/images/train/20190719_134835.jpg'\n",
            "'train/20190719_135145_HDR.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135145_HDR.jpg'\n",
            "'train/20190719_135151_HDR.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135151_HDR.jpg'\n",
            "'train/20190719_135127.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135127.jpg'\n",
            "'train/20190719_135208.xml' -> '/content/ObjDet_Demo/images/train/20190719_135208.xml'\n",
            "'train/20190719_135239.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135239.jpg'\n",
            "'train/20190719_135208.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135208.jpg'\n",
            "'train/20190719_135227.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135227.jpg'\n",
            "'train/20190719_135301.xml' -> '/content/ObjDet_Demo/images/train/20190719_135301.xml'\n",
            "'train/20190719_135217.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135217.jpg'\n",
            "'train/20190719_135217.xml' -> '/content/ObjDet_Demo/images/train/20190719_135217.xml'\n",
            "'train/20190719_135227.xml' -> '/content/ObjDet_Demo/images/train/20190719_135227.xml'\n",
            "'train/20190719_135239.xml' -> '/content/ObjDet_Demo/images/train/20190719_135239.xml'\n",
            "'train/20190719_135301.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135301.jpg'\n",
            "'train/20190719_135310.xml' -> '/content/ObjDet_Demo/images/train/20190719_135310.xml'\n",
            "'train/20190719_135317.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135317.jpg'\n",
            "'train/20190719_135425.xml' -> '/content/ObjDet_Demo/images/train/20190719_135425.xml'\n",
            "'train/20190719_135408.xml' -> '/content/ObjDet_Demo/images/train/20190719_135408.xml'\n",
            "'train/20190719_135317.xml' -> '/content/ObjDet_Demo/images/train/20190719_135317.xml'\n",
            "'train/20190719_135351.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135351.jpg'\n",
            "'train/20190719_135351.xml' -> '/content/ObjDet_Demo/images/train/20190719_135351.xml'\n",
            "'train/20190719_135310.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135310.jpg'\n",
            "'train/20190719_135408.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135408.jpg'\n",
            "'train/20190719_135425.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135425.jpg'\n",
            "'train/20190719_135536.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135536.jpg'\n",
            "'train/20190719_135714.xml' -> '/content/ObjDet_Demo/images/train/20190719_135714.xml'\n",
            "'train/20190719_135548.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135548.jpg'\n",
            "'train/20190719_135554.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135554.jpg'\n",
            "'train/20190719_135548.xml' -> '/content/ObjDet_Demo/images/train/20190719_135548.xml'\n",
            "'train/20190719_135536.xml' -> '/content/ObjDet_Demo/images/train/20190719_135536.xml'\n",
            "'train/20190719_135542.xml' -> '/content/ObjDet_Demo/images/train/20190719_135542.xml'\n",
            "'train/20190719_135554.xml' -> '/content/ObjDet_Demo/images/train/20190719_135554.xml'\n",
            "'train/20190719_135542.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135542.jpg'\n",
            "'train/20190719_135714.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135714.jpg'\n",
            "'train/20190719_135717.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135717.jpg'\n",
            "'train/20190719_135753.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135753.jpg'\n",
            "'train/20190719_135745_HDR.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135745_HDR.jpg'\n",
            "'train/20190719_135745_HDR.xml' -> '/content/ObjDet_Demo/images/train/20190719_135745_HDR.xml'\n",
            "'train/20190719_135753.xml' -> '/content/ObjDet_Demo/images/train/20190719_135753.xml'\n",
            "'train/20190719_135801.xml' -> '/content/ObjDet_Demo/images/train/20190719_135801.xml'\n",
            "'train/20190719_135717.xml' -> '/content/ObjDet_Demo/images/train/20190719_135717.xml'\n",
            "'train/20190719_135735.xml' -> '/content/ObjDet_Demo/images/train/20190719_135735.xml'\n",
            "'train/20190719_135735.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135735.jpg'\n",
            "'train/20190719_135801.jpg' -> '/content/ObjDet_Demo/images/train/20190719_135801.jpg'\n",
            "'train/20190719_140952.xml' -> '/content/ObjDet_Demo/images/train/20190719_140952.xml'\n",
            "'train/20190719_140957_HDR.xml' -> '/content/ObjDet_Demo/images/train/20190719_140957_HDR.xml'\n",
            "'train/20190719_141003.xml' -> '/content/ObjDet_Demo/images/train/20190719_141003.xml'\n",
            "'train/20190719_141014.xml' -> '/content/ObjDet_Demo/images/train/20190719_141014.xml'\n",
            "'train/20190719_141007.xml' -> '/content/ObjDet_Demo/images/train/20190719_141007.xml'\n",
            "'train/20190719_140952.jpg' -> '/content/ObjDet_Demo/images/train/20190719_140952.jpg'\n",
            "'train/20190719_141003.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141003.jpg'\n",
            "'train/20190719_140957_HDR.jpg' -> '/content/ObjDet_Demo/images/train/20190719_140957_HDR.jpg'\n",
            "'train/20190719_141007.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141007.jpg'\n",
            "'train/20190719_141014.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141014.jpg'\n",
            "'train/20190719_141023.xml' -> '/content/ObjDet_Demo/images/train/20190719_141023.xml'\n",
            "'train/20190719_141124.xml' -> '/content/ObjDet_Demo/images/train/20190719_141124.xml'\n",
            "'train/20190719_141114.xml' -> '/content/ObjDet_Demo/images/train/20190719_141114.xml'\n",
            "'train/20190719_141042.xml' -> '/content/ObjDet_Demo/images/train/20190719_141042.xml'\n",
            "'train/20190719_141023.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141023.jpg'\n",
            "'train/20190719_141028.xml' -> '/content/ObjDet_Demo/images/train/20190719_141028.xml'\n",
            "'train/20190719_141028.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141028.jpg'\n",
            "'train/20190719_141124.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141124.jpg'\n",
            "'train/20190719_141114.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141114.jpg'\n",
            "'train/20190719_141042.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141042.jpg'\n",
            "'train/20190719_141141.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141141.jpg'\n",
            "'train/20190719_141232.xml' -> '/content/ObjDet_Demo/images/train/20190719_141232.xml'\n",
            "'train/20190719_141232.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141232.jpg'\n",
            "'train/20190719_141152.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141152.jpg'\n",
            "'train/20190719_141141.xml' -> '/content/ObjDet_Demo/images/train/20190719_141141.xml'\n",
            "'train/20190719_141152.xml' -> '/content/ObjDet_Demo/images/train/20190719_141152.xml'\n",
            "'train/20190719_141247.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141247.jpg'\n",
            "'train/20190719_141259.xml' -> '/content/ObjDet_Demo/images/train/20190719_141259.xml'\n",
            "'train/20190719_141259.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141259.jpg'\n",
            "'train/20190719_141247.xml' -> '/content/ObjDet_Demo/images/train/20190719_141247.xml'\n",
            "'train/20190719_141305.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141305.jpg'\n",
            "'train/20190719_141325.xml' -> '/content/ObjDet_Demo/images/train/20190719_141325.xml'\n",
            "'train/20190719_141305.xml' -> '/content/ObjDet_Demo/images/train/20190719_141305.xml'\n",
            "'train/20190719_141315.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141315.jpg'\n",
            "'train/20190719_141325.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141325.jpg'\n",
            "'train/20190719_141315.xml' -> '/content/ObjDet_Demo/images/train/20190719_141315.xml'\n",
            "'train/20190719_141339_HDR.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141339_HDR.jpg'\n",
            "'train/20190719_141339_HDR.xml' -> '/content/ObjDet_Demo/images/train/20190719_141339_HDR.xml'\n",
            "'train/20190719_141352.xml' -> '/content/ObjDet_Demo/images/train/20190719_141352.xml'\n",
            "'train/20190719_141352.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141352.jpg'\n",
            "'train/20190719_141409.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141409.jpg'\n",
            "'train/20190719_141426.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141426.jpg'\n",
            "'train/20190719_141409.xml' -> '/content/ObjDet_Demo/images/train/20190719_141409.xml'\n",
            "'train/20190719_141415.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141415.jpg'\n",
            "'train/20190719_141415.xml' -> '/content/ObjDet_Demo/images/train/20190719_141415.xml'\n",
            "'train/20190719_141449.xml' -> '/content/ObjDet_Demo/images/train/20190719_141449.xml'\n",
            "'train/20190719_141435.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141435.jpg'\n",
            "'train/20190719_141435.xml' -> '/content/ObjDet_Demo/images/train/20190719_141435.xml'\n",
            "'train/20190719_141426.xml' -> '/content/ObjDet_Demo/images/train/20190719_141426.xml'\n",
            "'train/20190719_141449.jpg' -> '/content/ObjDet_Demo/images/train/20190719_141449.jpg'\n",
            "'train/20190719_175414.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175414.jpg'\n",
            "'train/20190719_175429.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175429.jpg'\n",
            "'train/20190719_175429.xml' -> '/content/ObjDet_Demo/images/train/20190719_175429.xml'\n",
            "'train/20190719_175443.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175443.jpg'\n",
            "'train/20190719_175420.xml' -> '/content/ObjDet_Demo/images/train/20190719_175420.xml'\n",
            "'train/20190719_175456.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175456.jpg'\n",
            "'train/20190719_175414.xml' -> '/content/ObjDet_Demo/images/train/20190719_175414.xml'\n",
            "'train/20190719_175420.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175420.jpg'\n",
            "'train/20190719_175443.xml' -> '/content/ObjDet_Demo/images/train/20190719_175443.xml'\n",
            "'train/20190719_175456.xml' -> '/content/ObjDet_Demo/images/train/20190719_175456.xml'\n",
            "'train/20190719_175500.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175500.jpg'\n",
            "'train/20190719_175500.xml' -> '/content/ObjDet_Demo/images/train/20190719_175500.xml'\n",
            "'train/20190719_175504.xml' -> '/content/ObjDet_Demo/images/train/20190719_175504.xml'\n",
            "'train/20190719_175504.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175504.jpg'\n",
            "'train/20190719_175507.xml' -> '/content/ObjDet_Demo/images/train/20190719_175507.xml'\n",
            "'train/20190719_175507.jpg' -> '/content/ObjDet_Demo/images/train/20190719_175507.jpg'\n",
            "mv: cannot move '/content/ObjDet_Demo/doc' to '/content/models/research/object_detection/doc': Directory not empty\n",
            "renamed '/content/ObjDet_Demo/generate_tfrecord.py' -> '/content/models/research/object_detection/generate_tfrecord.py'\n",
            "mv: cannot move '/content/ObjDet_Demo/images' to '/content/models/research/object_detection/images': Directory not empty\n",
            "renamed '/content/ObjDet_Demo/inference_graph' -> '/content/models/research/object_detection/inference_graph'\n",
            "renamed '/content/ObjDet_Demo/LICENSE' -> '/content/models/research/object_detection/LICENSE'\n",
            "renamed '/content/ObjDet_Demo/Object_detection_image.py' -> '/content/models/research/object_detection/Object_detection_image.py'\n",
            "renamed '/content/ObjDet_Demo/Object_detection_video.py' -> '/content/models/research/object_detection/Object_detection_video.py'\n",
            "renamed '/content/ObjDet_Demo/Object_detection_webcam.py' -> '/content/models/research/object_detection/Object_detection_webcam.py'\n",
            "renamed '/content/ObjDet_Demo/README.md' -> '/content/models/research/object_detection/README.md'\n",
            "renamed '/content/ObjDet_Demo/resizer.py' -> '/content/models/research/object_detection/resizer.py'\n",
            "renamed '/content/ObjDet_Demo/sizeChecker.py' -> '/content/models/research/object_detection/sizeChecker.py'\n",
            "renamed '/content/ObjDet_Demo/test1.JPG' -> '/content/models/research/object_detection/test1.JPG'\n",
            "renamed '/content/ObjDet_Demo/test.mov' -> '/content/models/research/object_detection/test.mov'\n",
            "renamed '/content/ObjDet_Demo/training' -> '/content/models/research/object_detection/training'\n",
            "mv: cannot move '/content/ObjDet_Demo/translate' to '/content/models/research/object_detection/translate': Directory not empty\n",
            "renamed '/content/ObjDet_Demo/xml_to_csv.py' -> '/content/models/research/object_detection/xml_to_csv.py'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLCd6gsoMLNc",
        "colab_type": "text"
      },
      "source": [
        "#Build and Install setup.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47wUWx9POVaq",
        "colab_type": "code",
        "outputId": "0cce9fd2-b400-4f7c-cedc-020370014f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/models/research\n",
        "!python setup.py build\n",
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "running build\n",
            "running build_py\n",
            "copying object_detection/xml_to_csv.py -> build/lib.linux-x86_64-2.7/object_detection\n",
            "copying object_detection/Object_detection_webcam.py -> build/lib.linux-x86_64-2.7/object_detection\n",
            "copying object_detection/Object_detection_image.py -> build/lib.linux-x86_64-2.7/object_detection\n",
            "copying object_detection/resizer.py -> build/lib.linux-x86_64-2.7/object_detection\n",
            "copying object_detection/sizeChecker.py -> build/lib.linux-x86_64-2.7/object_detection\n",
            "copying object_detection/Object_detection_video.py -> build/lib.linux-x86_64-2.7/object_detection\n",
            "copying object_detection/generate_tfrecord.py -> build/lib.linux-x86_64-2.7/object_detection\n",
            "copying object_detection/protos/faster_rcnn_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/losses_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/eval_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/ssd_anchor_generator_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/matcher_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/argmax_matcher_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/anchor_generator_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/box_coder_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/train_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/region_similarity_calculator_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/square_box_coder_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/keypoint_box_coder_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/input_reader_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/flexible_grid_anchor_generator_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/string_int_label_map_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/preprocessor_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/calibration_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/optimizer_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/pipeline_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/grid_anchor_generator_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/faster_rcnn_box_coder_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/graph_rewriter_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/hyperparams_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/box_predictor_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/multiscale_anchor_generator_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/bipartite_matcher_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/image_resizer_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/mean_stddev_box_coder_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/ssd_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/post_processing_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "copying object_detection/protos/model_pb2.py -> build/lib.linux-x86_64-2.7/object_detection/protos\n",
            "running egg_info\n",
            "writing requirements to object_detection.egg-info/requires.txt\n",
            "writing object_detection.egg-info/PKG-INFO\n",
            "writing top-level names to object_detection.egg-info/top_level.txt\n",
            "writing dependency_links to object_detection.egg-info/dependency_links.txt\n",
            "writing manifest file 'object_detection.egg-info/SOURCES.txt'\n",
            "copying object_detection/README.md -> build/lib.linux-x86_64-2.7/object_detection\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing requirements to object_detection.egg-info/requires.txt\n",
            "writing object_detection.egg-info/PKG-INFO\n",
            "writing top-level names to object_detection.egg-info/top_level.txt\n",
            "writing dependency_links to object_detection.egg-info/dependency_links.txt\n",
            "writing manifest file 'object_detection.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/model_lib_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/resnet_v1_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/base_models/original_mobilenet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/mobilenet_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/inception_resnet_v2_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/mobilenet_v1.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/resnet_v1.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/model_utils.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/mobilenet_v1_test.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/test_utils.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/mobilenet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/keras_models/inception_resnet_v2.py -> build/bdist.linux-x86_64/egg/object_detection/models/keras_models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_inception_v3_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_inception_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_inception_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_pnas_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/feature_map_generators.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_pnasnet_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_nas_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_inception_v3_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_pnasnet_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/feature_map_generators_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/models/faster_rcnn_nas_feature_extractor.py -> build/bdist.linux-x86_64/egg/object_detection/models\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/test_data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/test_data/pets_examples.record -> build/bdist.linux-x86_64/egg/object_detection/test_data\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_mask_list.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/label_map_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/spatial_transform_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_mask_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/label_map_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/visualization_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_mask_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/variables_helper.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/metrics_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_mask_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/static_shape_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/object_detection_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/dataset_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/per_image_vrd_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/vrd_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/config_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/static_shape.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/json_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/context_manager.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/learning_schedules.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/model_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/category_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/visualization_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/json_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/per_image_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_list.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_mask_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/per_image_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/autoaugment_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/model_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/category_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/test_case.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/variables_helper_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/shape_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/context_manager_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/object_detection_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/test_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/per_image_vrd_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/learning_schedules_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_mask_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/vrd_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/test_utils.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/dataset_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/shape_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/spatial_transform_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/metrics.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/config_util.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/utils/np_box_ops.py -> build/bdist.linux-x86_64/egg/object_detection/utils\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/export_inference_graph.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/export_tflite_ssd_graph_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/exporter.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/matchers/argmax_matcher.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/matchers/bipartite_matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/matchers/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/matchers/bipartite_matcher.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/matchers/argmax_matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/matchers\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/faster_rcnn_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/ssd.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/graph_rewriter.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/input_reader.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/losses_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/square_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/eval_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/ssd_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/bipartite_matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/faster_rcnn_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/argmax_matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/losses.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/keypoint_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/argmax_matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/multiscale_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/optimizer.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/calibration.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/grid_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/string_int_label_map.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/train_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/region_similarity_calculator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/eval.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/square_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/keypoint_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/input_reader_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/flexible_grid_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/string_int_label_map_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/preprocessor_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/matcher.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/flexible_grid_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/calibration_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/preprocessor.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/box_predictor.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/optimizer_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/train.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/mean_stddev_box_coder.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/model.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/pipeline_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/image_resizer.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/grid_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/faster_rcnn_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/graph_rewriter_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/hyperparams_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/post_processing.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/box_predictor_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/multiscale_anchor_generator_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/bipartite_matcher_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/faster_rcnn.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/image_resizer_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/pipeline.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/ssd_anchor_generator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/mean_stddev_box_coder_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/ssd_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/post_processing_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/model_pb2.py -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/region_similarity_calculator.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/protos/hyperparams.proto -> build/bdist.linux-x86_64/egg/object_detection/protos\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/model_hparams.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/oid_bbox_trainable_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/oid_v4_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/kitti_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/mscoco_complete_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/face_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/pet_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/pascal_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/mscoco_minival_ids.txt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/fgvc_2854_classes_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/mscoco_label_map.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data/ava_label_map_v2.1.pbtxt -> build/bdist.linux-x86_64/egg/object_detection/data\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/multiscale_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/multiple_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/flexible_grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/anchor_generators/grid_anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/anchor_generators\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/inputs_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/object_detection_tutorial.ipynb -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/xml_to_csv.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/model_lib.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/tf_example_parser_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/offline_eval_map_corloc_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/calibration_metrics.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/calibration_metrics_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/calibration_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/coco_tools_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/offline_eval_map_corloc.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/oid_vrd_challenge_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/coco_tools.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/coco_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/coco_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/oid_challenge_evaluation_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/io_utils.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/oid_challenge_evaluation_utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/tf_example_parser.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/calibration_evaluation_test.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/metrics/oid_challenge_evaluation.py -> build/bdist.linux-x86_64/egg/object_detection/metrics\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/Object_detection_webcam.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/dockerfiles\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dockerfiles/android/Dockerfile -> build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dockerfiles/android/README.md -> build/bdist.linux-x86_64/egg/object_detection/dockerfiles/android\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/model_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data_decoders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data_decoders/tf_example_decoder.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/data_decoders/tf_example_decoder_test.py -> build/bdist.linux-x86_64/egg/object_detection/data_decoders\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/box_predictor_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/box_predictor_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/image_resizer_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/graph_rewriter_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/input_reader_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/optimizer_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/matcher_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/anchor_generator_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/region_similarity_calculator_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/preprocessor_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/optimizer_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/box_coder_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/dataset_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/losses_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/model_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/dataset_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/calibration_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/matcher_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/post_processing_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/input_reader_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/model_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/hyperparams_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/box_coder_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/post_processing_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/graph_rewriter_builder.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/losses_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/preprocessor_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/hyperparams_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/calibration_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/image_resizer_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/anchor_generator_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/builders/region_similarity_calculator_builder_test.py -> build/bdist.linux-x86_64/egg/object_detection/builders\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/convolutional_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/rfcn_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/convolutional_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/rfcn_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/rfcn_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/rfcn_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/convolutional_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/mask_rcnn_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/convolutional_box_predictor_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/box_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keypoint_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keras_mask_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keras_class_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keras_mask_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keras_box_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/class_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keras_class_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/mask_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keras_box_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/mask_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/class_head_test.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/box_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/heads/keypoint_head.py -> build/bdist.linux-x86_64/egg/object_detection/predictors/heads\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/mask_rcnn_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/predictors/mask_rcnn_keras_box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/predictors\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/utils_test.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/faster_rcnn.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/export_saved_model_tpu.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/faster_rcnn\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/testdata/faster_rcnn/faster_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/faster_rcnn\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/testdata/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/ssd\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/testdata/ssd/ssd_pipeline.config -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/ssd\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/export_saved_model_tpu_lib.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/ssd.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/tpu_exporters/utils.py -> build/bdist.linux-x86_64/egg/object_detection/tpu_exporters\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/preparing_inputs.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/running_on_mobile_tensorflowlite.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/running_notebook.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/running_locally.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/tpu_exporters.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/running_on_cloud.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/configuring_jobs.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/instance_segmentation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/tpu_compatibility.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/defining_your_own_model.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/challenge_evaluation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/running_pets.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/using_your_own_dataset.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/nongroupof_case_eval.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/dogs_detections_output.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/dataset_explorer.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/oid_monkey_3b4168c89cecbc5b.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/kites_detections_output.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/tensorboard.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/groupof_case_eval.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/oid_bus_72e19c28aac34ed8.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/tf-od-api-logo.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/example_cat.jpg -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/kites_with_segment_overlay.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/oxford_pet.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/img/tensorboard2.png -> build/bdist.linux-x86_64/egg/object_detection/g3doc/img\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/faq.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/evaluation_protocols.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/detection_model_zoo.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/oid_inference_and_evaluation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/installation.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/g3doc/exporting_models.md -> build/bdist.linux-x86_64/egg/object_detection/g3doc\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/inference/detection_inference_test.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/inference/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/inference/detection_inference.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/inference/infer_detections.py -> build/bdist.linux-x86_64/egg/object_detection/inference\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/export_tflite_ssd_graph.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/model_lib_v2.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/tf_record_creation_util_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_pet_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_kitti_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/oid_tfrecord_creation.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/tf_record_creation_util.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_oid_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_coco_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/oid_tfrecord_creation_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_kitti_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_pycocotools_package.sh -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/download_and_preprocess_mscoco.sh -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_coco_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_pascal_tf_record_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/create_pascal_tf_record.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py -> build/bdist.linux-x86_64/egg/object_detection/dataset_tools\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/Object_detection_image.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/README.md -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/__init__.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/model_main.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/eval_util_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/resizer.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/eval_util.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/sizeChecker.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/ssd_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/ssd_meta_arch_test_lib.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/ssd_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/faster_rcnn_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/rfcn_meta_arch_test.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/meta_architectures/rfcn_meta_arch.py -> build/bdist.linux-x86_64/egg/object_detection/meta_architectures\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/legacy/evaluator.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/legacy/trainer_test.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/legacy/eval.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/legacy/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/legacy/trainer.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/legacy/train.py -> build/bdist.linux-x86_64/egg/object_detection/legacy\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/samples\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/samples/cloud\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/cloud/cloud.yml -> build/bdist.linux-x86_64/egg/object_detection/samples/cloud\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet101_kitti.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v2_fullyconv_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/rfcn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_nas_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet50_fgvc.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v2_quantized_300x300_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet101_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/rfcn_resnet101_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v2_oid_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet101_voc07.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/mask_rcnn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet50_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet50_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/facessd_mobilenet_v2_quantized_320x320_open_image_v4.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet101_fgvc.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets_inference.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet152_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet101_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_inception_v3_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet152_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/faster_rcnn_resnet101_ava_v2.1.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v2_pets_keras.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_inception_v2_pets.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_resnet101_v1_fpn_shared_box_predictor_oid_512x512_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config -> build/bdist.linux-x86_64/egg/object_detection/samples/configs\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/Object_detection_video.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/export_tflite_ssd_graph_lib.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/standard_fields.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/data_parser.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/box_list_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/multiclass_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/freezable_batch_norm_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/keypoint_ops_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/box_predictor.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/matcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/freezable_batch_norm.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/target_assigner.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/preprocessor.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/region_similarity_calculator_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/box_list_ops.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/batch_multiclass_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/keypoint_ops.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/losses.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/balanced_positive_negative_sampler_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/losses_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/prefetcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/anchor_generator.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/post_processing.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/target_assigner_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/minibatch_sampler_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/balanced_positive_negative_sampler.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/minibatch_sampler.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/matcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/prefetcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/model.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/box_list_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/preprocessor_cache.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/batcher_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/data_decoder.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/class_agnostic_nms_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/batcher.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/box_list.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/region_similarity_calculator.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/core/preprocessor_test.py -> build/bdist.linux-x86_64/egg/object_detection/core\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/mean_stddev_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/faster_rcnn_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/keypoint_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/faster_rcnn_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/mean_stddev_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/__init__.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/square_box_coder_test.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/keypoint_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/box_coders/square_box_coder.py -> build/bdist.linux-x86_64/egg/object_detection/box_coders\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/CONTRIBUTING.md -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/test_images/image1.jpg -> build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/test_images/image2.jpg -> build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/test_images/image_info.txt -> build/bdist.linux-x86_64/egg/object_detection/test_images\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/generate_tfrecord.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "creating build/bdist.linux-x86_64/egg/object_detection/test_ckpt\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/test_ckpt/ssd_inception_v2.pb -> build/bdist.linux-x86_64/egg/object_detection/test_ckpt\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/model_tpu_main.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/exporter_test.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "copying build/lib.linux-x86_64-2.7/object_detection/inputs.py -> build/bdist.linux-x86_64/egg/object_detection\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_v2_test.py to model_lib_v2_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_feature_extractor.py to ssd_mobilenet_v2_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/resnet_v1_test.py to resnet_v1_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/base_models/original_mobilenet_v2.py to original_mobilenet_v2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v2_test.py to mobilenet_v2_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/inception_resnet_v2_test.py to inception_resnet_v2_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v1.py to mobilenet_v1.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/resnet_v1.py to resnet_v1.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/model_utils.py to model_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v1_test.py to mobilenet_v1_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/test_utils.py to test_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/mobilenet_v2.py to mobilenet_v2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/keras_models/inception_resnet_v2.py to inception_resnet_v2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py to ssd_mobilenet_v1_fpn_keras_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py to ssd_resnet_v1_ppn_feature_extractor_testbase.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py to ssd_resnet_v1_fpn_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py to ssd_mobilenet_v2_fpn_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py to faster_rcnn_inception_resnet_v2_keras_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py to ssd_mobilenet_v2_fpn_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_feature_extractor_test.py to ssd_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v3_feature_extractor.py to ssd_inception_v3_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v2_feature_extractor_test.py to ssd_inception_v2_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v2_feature_extractor.py to ssd_inception_v2_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_pnas_feature_extractor.py to faster_rcnn_pnas_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py to faster_rcnn_mobilenet_v1_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py to faster_rcnn_resnet_v1_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/feature_map_generators.py to feature_map_generators.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py to ssd_resnet_v1_fpn_feature_extractor_testbase.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_pnasnet_feature_extractor.py to ssd_pnasnet_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py to faster_rcnn_inception_resnet_v2_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py to ssd_resnet_v1_ppn_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py to ssd_resnet_v1_fpn_keras_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py to ssd_resnet_v1_ppn_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py to ssd_resnet_v1_fpn_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py to faster_rcnn_inception_resnet_v2_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py to faster_rcnn_inception_v2_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py to embedded_ssd_mobilenet_v1_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_nas_feature_extractor_test.py to faster_rcnn_nas_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py to faster_rcnn_inception_v2_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_inception_v3_feature_extractor_test.py to ssd_inception_v3_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py to ssd_mobilenet_v1_ppn_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py to ssd_mobilenet_v1_keras_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_pnasnet_feature_extractor_test.py to ssd_pnasnet_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_feature_extractor.py to ssd_mobilenet_v1_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py to ssd_mobilenet_v2_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/feature_map_generators_test.py to feature_map_generators_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py to faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py to ssd_mobilenet_v1_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py to embedded_ssd_mobilenet_v1_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py to ssd_mobilenet_v1_fpn_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py to ssd_mobilenet_v1_fpn_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py to ssd_mobilenet_v2_keras_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py to faster_rcnn_mobilenet_v1_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py to faster_rcnn_resnet_v1_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py to ssd_mobilenet_v2_fpn_keras_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py to faster_rcnn_pnas_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py to ssd_mobilenet_v1_ppn_feature_extractor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/models/faster_rcnn_nas_feature_extractor.py to faster_rcnn_nas_feature_extractor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list.py to np_box_mask_list.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/label_map_util.py to label_map_util.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/spatial_transform_ops.py to spatial_transform_ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_test.py to np_box_mask_list_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/label_map_util_test.py to label_map_util_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/visualization_utils_test.py to visualization_utils_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_mask_ops.py to np_mask_ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/variables_helper.py to variables_helper.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/metrics_test.py to metrics_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_mask_ops_test.py to np_mask_ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/static_shape_test.py to static_shape_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/object_detection_evaluation_test.py to object_detection_evaluation_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/dataset_util.py to dataset_util.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_vrd_evaluation_test.py to per_image_vrd_evaluation_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/vrd_evaluation_test.py to vrd_evaluation_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/config_util_test.py to config_util_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/static_shape.py to static_shape.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/json_utils_test.py to json_utils_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/context_manager.py to context_manager.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/learning_schedules.py to learning_schedules.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/ops_test.py to ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_test.py to np_box_list_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_ops_test.py to np_box_ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/model_util.py to model_util.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/category_util_test.py to category_util_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/visualization_utils.py to visualization_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/json_utils.py to json_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_evaluation_test.py to per_image_evaluation_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list.py to np_box_list.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_ops_test.py to np_box_mask_list_ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_evaluation.py to per_image_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/autoaugment_utils.py to autoaugment_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/model_util_test.py to model_util_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/category_util.py to category_util.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_case.py to test_case.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_ops.py to np_box_list_ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/variables_helper_test.py to variables_helper_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/shape_utils.py to shape_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/context_manager_test.py to context_manager_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/object_detection_evaluation.py to object_detection_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_utils_test.py to test_utils_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/per_image_vrd_evaluation.py to per_image_vrd_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_list_ops_test.py to np_box_list_ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/learning_schedules_test.py to learning_schedules_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_mask_list_ops.py to np_box_mask_list_ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/vrd_evaluation.py to vrd_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/test_utils.py to test_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/dataset_util_test.py to dataset_util_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/shape_utils_test.py to shape_utils_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/ops.py to ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/spatial_transform_ops_test.py to spatial_transform_ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/metrics.py to metrics.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/config_util.py to config_util.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/utils/np_box_ops.py to np_box_ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_inference_graph.py to export_inference_graph.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph_lib_test.py to export_tflite_ssd_graph_lib_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/exporter.py to exporter.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/argmax_matcher.py to argmax_matcher.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/bipartite_matcher_test.py to bipartite_matcher_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/bipartite_matcher.py to bipartite_matcher.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/matchers/argmax_matcher_test.py to argmax_matcher_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/faster_rcnn_pb2.py to faster_rcnn_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/losses_pb2.py to losses_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/eval_pb2.py to eval_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/ssd_anchor_generator_pb2.py to ssd_anchor_generator_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/matcher_pb2.py to matcher_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/argmax_matcher_pb2.py to argmax_matcher_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/anchor_generator_pb2.py to anchor_generator_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/box_coder_pb2.py to box_coder_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/train_pb2.py to train_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/region_similarity_calculator_pb2.py to region_similarity_calculator_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/square_box_coder_pb2.py to square_box_coder_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/keypoint_box_coder_pb2.py to keypoint_box_coder_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/input_reader_pb2.py to input_reader_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/flexible_grid_anchor_generator_pb2.py to flexible_grid_anchor_generator_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/string_int_label_map_pb2.py to string_int_label_map_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/preprocessor_pb2.py to preprocessor_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/calibration_pb2.py to calibration_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/optimizer_pb2.py to optimizer_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/pipeline_pb2.py to pipeline_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/grid_anchor_generator_pb2.py to grid_anchor_generator_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/faster_rcnn_box_coder_pb2.py to faster_rcnn_box_coder_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/graph_rewriter_pb2.py to graph_rewriter_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/hyperparams_pb2.py to hyperparams_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/box_predictor_pb2.py to box_predictor_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/multiscale_anchor_generator_pb2.py to multiscale_anchor_generator_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/bipartite_matcher_pb2.py to bipartite_matcher_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/image_resizer_pb2.py to image_resizer_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/mean_stddev_box_coder_pb2.py to mean_stddev_box_coder_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/ssd_pb2.py to ssd_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/post_processing_pb2.py to post_processing_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/protos/model_pb2.py to model_pb2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_hparams.py to model_hparams.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiscale_grid_anchor_generator.py to multiscale_grid_anchor_generator.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py to flexible_grid_anchor_generator_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiple_grid_anchor_generator.py to multiple_grid_anchor_generator.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py to multiscale_grid_anchor_generator_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/grid_anchor_generator_test.py to grid_anchor_generator_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/flexible_grid_anchor_generator.py to flexible_grid_anchor_generator.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py to multiple_grid_anchor_generator_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/anchor_generators/grid_anchor_generator.py to grid_anchor_generator.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inputs_test.py to inputs_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/xml_to_csv.py to xml_to_csv.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib.py to model_lib.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/tf_example_parser_test.py to tf_example_parser_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/offline_eval_map_corloc_test.py to offline_eval_map_corloc_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py to oid_vrd_challenge_evaluation_utils_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_metrics.py to calibration_metrics.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py to oid_vrd_challenge_evaluation_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_metrics_test.py to calibration_metrics_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_evaluation.py to calibration_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_tools_test.py to coco_tools_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/offline_eval_map_corloc.py to offline_eval_map_corloc.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_vrd_challenge_evaluation.py to oid_vrd_challenge_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_tools.py to coco_tools.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_evaluation.py to coco_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/coco_evaluation_test.py to coco_evaluation_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation_utils.py to oid_challenge_evaluation_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/io_utils.py to io_utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation_utils_test.py to oid_challenge_evaluation_utils_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/tf_example_parser.py to tf_example_parser.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/calibration_evaluation_test.py to calibration_evaluation_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/metrics/oid_challenge_evaluation.py to oid_challenge_evaluation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/Object_detection_webcam.py to Object_detection_webcam.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_test.py to model_lib_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/tf_example_decoder.py to tf_example_decoder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/data_decoders/tf_example_decoder_test.py to tf_example_decoder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_predictor_builder.py to box_predictor_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_predictor_builder_test.py to box_predictor_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/image_resizer_builder.py to image_resizer_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/graph_rewriter_builder_test.py to graph_rewriter_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/input_reader_builder.py to input_reader_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/optimizer_builder.py to optimizer_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/matcher_builder.py to matcher_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/anchor_generator_builder.py to anchor_generator_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/region_similarity_calculator_builder.py to region_similarity_calculator_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/preprocessor_builder.py to preprocessor_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/optimizer_builder_test.py to optimizer_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_coder_builder_test.py to box_coder_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/dataset_builder_test.py to dataset_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/losses_builder.py to losses_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/model_builder_test.py to model_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/dataset_builder.py to dataset_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/calibration_builder.py to calibration_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/matcher_builder_test.py to matcher_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/post_processing_builder_test.py to post_processing_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/input_reader_builder_test.py to input_reader_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/model_builder.py to model_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/hyperparams_builder.py to hyperparams_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/box_coder_builder.py to box_coder_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/post_processing_builder.py to post_processing_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/graph_rewriter_builder.py to graph_rewriter_builder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/losses_builder_test.py to losses_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/preprocessor_builder_test.py to preprocessor_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/hyperparams_builder_test.py to hyperparams_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/calibration_builder_test.py to calibration_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/image_resizer_builder_test.py to image_resizer_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/anchor_generator_builder_test.py to anchor_generator_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/builders/region_similarity_calculator_builder_test.py to region_similarity_calculator_builder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_keras_box_predictor_test.py to convolutional_keras_box_predictor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_box_predictor_test.py to rfcn_box_predictor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_keras_box_predictor.py to convolutional_keras_box_predictor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_keras_box_predictor.py to rfcn_keras_box_predictor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_box_predictor.py to rfcn_box_predictor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/rfcn_keras_box_predictor_test.py to rfcn_keras_box_predictor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py to mask_rcnn_keras_box_predictor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_box_predictor.py to convolutional_box_predictor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_box_predictor_test.py to mask_rcnn_box_predictor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/convolutional_box_predictor_test.py to convolutional_box_predictor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/box_head_test.py to box_head_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keypoint_head_test.py to keypoint_head_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_mask_head_test.py to keras_mask_head_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_class_head_test.py to keras_class_head_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_mask_head.py to keras_mask_head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_box_head_test.py to keras_box_head_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/head.py to head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/class_head.py to class_head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_class_head.py to keras_class_head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/mask_head.py to mask_head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keras_box_head.py to keras_box_head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/mask_head_test.py to mask_head_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/class_head_test.py to class_head_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/box_head.py to box_head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/heads/keypoint_head.py to keypoint_head.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_box_predictor.py to mask_rcnn_box_predictor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/predictors/mask_rcnn_keras_box_predictor.py to mask_rcnn_keras_box_predictor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/utils_test.py to utils_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/faster_rcnn.py to faster_rcnn.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu.py to export_saved_model_tpu.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py to export_saved_model_tpu_lib_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/testdata/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/export_saved_model_tpu_lib.py to export_saved_model_tpu_lib.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/ssd.py to ssd.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/tpu_exporters/utils.py to utils.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/detection_inference_test.py to detection_inference_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/detection_inference.py to detection_inference.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inference/infer_detections.py to infer_detections.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph.py to export_tflite_ssd_graph.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_lib_v2.py to model_lib_v2.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/tf_record_creation_util_test.py to tf_record_creation_util_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pet_tf_record.py to create_pet_tf_record.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_kitti_tf_record.py to create_kitti_tf_record.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_tfrecord_creation.py to oid_tfrecord_creation.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/tf_record_creation_util.py to tf_record_creation_util.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_oid_tf_record.py to create_oid_tf_record.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_coco_tf_record.py to create_coco_tf_record.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_tfrecord_creation_test.py to oid_tfrecord_creation_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_kitti_tf_record_test.py to create_kitti_tf_record_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py to oid_hierarchical_labels_expansion.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_coco_tf_record_test.py to create_coco_tf_record_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pascal_tf_record_test.py to create_pascal_tf_record_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/create_pascal_tf_record.py to create_pascal_tf_record.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py to oid_hierarchical_labels_expansion_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/Object_detection_image.py to Object_detection_image.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_main.py to model_main.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/eval_util_test.py to eval_util_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/resizer.py to resizer.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/eval_util.py to eval_util.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/sizeChecker.py to sizeChecker.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch.py to ssd_meta_arch.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch_test_lib.py to ssd_meta_arch_test_lib.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py to faster_rcnn_meta_arch_test_lib.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/ssd_meta_arch_test.py to ssd_meta_arch_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch.py to faster_rcnn_meta_arch.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py to faster_rcnn_meta_arch_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/rfcn_meta_arch_test.py to rfcn_meta_arch_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/meta_architectures/rfcn_meta_arch.py to rfcn_meta_arch.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/evaluator.py to evaluator.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/trainer_test.py to trainer_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/eval.py to eval.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/trainer.py to trainer.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/legacy/train.py to train.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/Object_detection_video.py to Object_detection_video.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/export_tflite_ssd_graph_lib.py to export_tflite_ssd_graph_lib.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/standard_fields.py to standard_fields.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/data_parser.py to data_parser.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_ops_test.py to box_list_ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/multiclass_nms_test.py to multiclass_nms_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/freezable_batch_norm_test.py to freezable_batch_norm_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/keypoint_ops_test.py to keypoint_ops_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_predictor.py to box_predictor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/matcher.py to matcher.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/freezable_batch_norm.py to freezable_batch_norm.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/target_assigner.py to target_assigner.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_coder_test.py to box_coder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor.py to preprocessor.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/region_similarity_calculator_test.py to region_similarity_calculator_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_ops.py to box_list_ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batch_multiclass_nms_test.py to batch_multiclass_nms_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/keypoint_ops.py to keypoint_ops.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/losses.py to losses.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/balanced_positive_negative_sampler_test.py to balanced_positive_negative_sampler_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/losses_test.py to losses_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/prefetcher_test.py to prefetcher_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/anchor_generator.py to anchor_generator.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_coder.py to box_coder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/post_processing.py to post_processing.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/target_assigner_test.py to target_assigner_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/minibatch_sampler_test.py to minibatch_sampler_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/balanced_positive_negative_sampler.py to balanced_positive_negative_sampler.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/minibatch_sampler.py to minibatch_sampler.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/matcher_test.py to matcher_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/prefetcher.py to prefetcher.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/model.py to model.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list_test.py to box_list_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor_cache.py to preprocessor_cache.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batcher_test.py to batcher_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/data_decoder.py to data_decoder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/class_agnostic_nms_test.py to class_agnostic_nms_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/batcher.py to batcher.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/box_list.py to box_list.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/region_similarity_calculator.py to region_similarity_calculator.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/core/preprocessor_test.py to preprocessor_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/mean_stddev_box_coder.py to mean_stddev_box_coder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/faster_rcnn_box_coder.py to faster_rcnn_box_coder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/keypoint_box_coder_test.py to keypoint_box_coder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/faster_rcnn_box_coder_test.py to faster_rcnn_box_coder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/mean_stddev_box_coder_test.py to mean_stddev_box_coder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/__init__.py to __init__.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/square_box_coder_test.py to square_box_coder_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/keypoint_box_coder.py to keypoint_box_coder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/box_coders/square_box_coder.py to square_box_coder.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/generate_tfrecord.py to generate_tfrecord.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/model_tpu_main.py to model_tpu_main.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/exporter_test.py to exporter_test.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/object_detection/inputs.py to inputs.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying object_detection.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "object_detection.core.preprocessor: module MAY be using inspect.stack\n",
            "object_detection.utils.autoaugment_utils: module MAY be using inspect.stack\n",
            "creating 'dist/object_detection-0.1-py2.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing object_detection-0.1-py2.7.egg\n",
            "removing '/usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg' (and everything under it)\n",
            "creating /usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg\n",
            "Extracting object_detection-0.1-py2.7.egg to /usr/local/lib/python2.7/dist-packages\n",
            "object-detection 0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg\n",
            "Processing dependencies for object-detection==0.1\n",
            "Searching for Cython==0.29.12\n",
            "Best match: Cython 0.29.12\n",
            "Adding Cython 0.29.12 to easy-install.pth file\n",
            "Installing cython script to /usr/local/bin\n",
            "Installing cygdb script to /usr/local/bin\n",
            "Installing cythonize script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for matplotlib==2.2.4\n",
            "Best match: matplotlib 2.2.4\n",
            "Adding matplotlib 2.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for Pillow==5.1.0\n",
            "Best match: Pillow 5.1.0\n",
            "Adding Pillow 5.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/lib/python2.7/dist-packages\n",
            "Searching for pyparsing==2.4.0\n",
            "Best match: pyparsing 2.4.0\n",
            "Adding pyparsing 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for python-dateutil==2.5.3\n",
            "Best match: python-dateutil 2.5.3\n",
            "Adding python-dateutil 2.5.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for kiwisolver==1.1.0\n",
            "Best match: kiwisolver 1.1.0\n",
            "Adding kiwisolver 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for subprocess32==3.5.4\n",
            "Best match: subprocess32 3.5.4\n",
            "Adding subprocess32 3.5.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for backports.functools-lru-cache==1.5\n",
            "Best match: backports.functools-lru-cache 1.5\n",
            "Adding backports.functools-lru-cache 1.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for numpy==1.16.4\n",
            "Best match: numpy 1.16.4\n",
            "Adding numpy 1.16.4 to easy-install.pth file\n",
            "Installing f2py2 script to /usr/local/bin\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py2.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for cycler==0.10.0\n",
            "Best match: cycler 0.10.0\n",
            "Adding cycler 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Searching for setuptools==41.2.0\n",
            "Best match: setuptools 41.2.0\n",
            "Adding setuptools 41.2.0 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python2.7/dist-packages\n",
            "Finished processing dependencies for object-detection==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBvq894zMlRc",
        "colab_type": "text"
      },
      "source": [
        "#Generate CSV, TfRecords and Label Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW_yIWXQUfO9",
        "colab_type": "code",
        "outputId": "658b4787-16eb-4033-acb6-76589f7ac2a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "!python xml_to_csv.py\n",
        "\n",
        "%cd /content/models/research/object_detection\n",
        "!rm -r generate_tfrecord.py\n",
        "%cd /content/drive/My Drive/Modified Files\n",
        "!cp generate_tfrecord.py /content/models/research/object_detection/\n",
        "%cd /content/models/research/object_detection\n",
        "!python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record\n",
        "\n",
        "%cd /content/models/research/object_detection/training\n",
        "!rm -r ssdlite_mobilenet_v2_coco.config\n",
        "%cd /content/drive/My Drive/Modified Files\n",
        "!cp ssdlite_mobilenet_v2_coco.config /content/models/research/object_detection/training/\n",
        "!cp label_map.pbtxt /content/models/research/object_detection/training\n",
        "%cd /content/models/research/object_detection/training\n",
        "!mv -v label_map.pbtxt labelmap.pbtxt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n",
            "Successfully converted xml to csv.\n",
            "Successfully converted xml to csv.\n",
            "/content/models/research/object_detection\n",
            "/content/drive/My Drive/Modified Files\n",
            "/content/models/research/object_detection\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0919 16:28:55.290638 139859036731264 deprecation_wrapper.py:119] From generate_tfrecord.py:100: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0919 16:28:55.291210 139859036731264 deprecation_wrapper.py:119] From generate_tfrecord.py:86: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0919 16:28:55.367321 139859036731264 deprecation_wrapper.py:119] From generate_tfrecord.py:45: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/train.record\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0919 16:28:59.436266 140081532663680 deprecation_wrapper.py:119] From generate_tfrecord.py:100: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0919 16:28:59.436873 140081532663680 deprecation_wrapper.py:119] From generate_tfrecord.py:86: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0919 16:28:59.457541 140081532663680 deprecation_wrapper.py:119] From generate_tfrecord.py:45: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/models/research/object_detection/test.record\n",
            "/content/models/research/object_detection/training\n",
            "rm: cannot remove 'ssdlite_mobilenet_v2_coco.config': No such file or directory\n",
            "/content/drive/My Drive/Modified Files\n",
            "/content/models/research/object_detection/training\n",
            "renamed 'label_map.pbtxt' -> 'labelmap.pbtxt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCzlocN7MyoM",
        "colab_type": "text"
      },
      "source": [
        "#Copy legacy file and begin training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZYrQ1CUNCot",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUf-3y6G1xRD",
        "colab_type": "code",
        "outputId": "abd1bafc-0f6f-43a3-fe6f-4b68040bec1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/models/research/object_detection/legacy\n",
        "!cp train.py /content/models/research/object_detection/\n",
        "%cd /content/models/research/object_detection/\n",
        "!python train.py --logtostderr --train_dir=/content/models/research/object_detection/training/ --pipeline_config_path=/content/models/research/object_detection/training/ssdlite_mobilenet_v2_coco.config"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection/legacy\n",
            "/content/models/research/object_detection\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0919 16:29:37.081952 140474388072320 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0919 16:29:37.197824 140474388072320 deprecation_wrapper.py:119] From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0919 16:29:37.204699 140474388072320 deprecation_wrapper.py:119] From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0919 16:29:37.242496 140474388072320 deprecation_wrapper.py:119] From train.py:55: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0919 16:29:37.242687 140474388072320 deprecation_wrapper.py:119] From train.py:55: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0919 16:29:37.243197 140474388072320 deprecation_wrapper.py:119] From train.py:184: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0919 16:29:37.243603 140474388072320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/absl/app.py:251: main (from __main__) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use object_detection/model_main.py.\n",
            "W0919 16:29:37.243761 140474388072320 deprecation_wrapper.py:119] From train.py:90: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0919 16:29:37.244010 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0919 16:29:37.247798 140474388072320 deprecation_wrapper.py:119] From train.py:95: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0919 16:29:37.251250 140474388072320 deprecation.py:323] From /content/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.create_global_step\n",
            "W0919 16:29:37.255223 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0919 16:29:37.255397 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0919 16:29:37.267960 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0919 16:29:37.272838 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:71: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0919 16:29:37.272948 140474388072320 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "W0919 16:29:37.280921 140474388072320 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0919 16:29:37.281090 140474388072320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0919 16:29:37.315521 140474388072320 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:155: map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0919 16:29:37.529025 140474388072320 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:43: make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0919 16:29:37.535516 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/dataset_builder.py:44: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0919 16:29:37.540086 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0919 16:29:37.596031 140474388072320 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:196: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0919 16:29:37.608274 140474388072320 deprecation.py:323] From /content/models/research/object_detection/core/box_list_ops.py:206: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0919 16:29:38.449641 140474388072320 deprecation.py:323] From /content/models/research/object_detection/core/batcher.py:101: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0919 16:29:38.453746 140474388072320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:753: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0919 16:29:38.454761 140474388072320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0919 16:29:38.464253 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/prefetcher.py:58: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0919 16:29:39.410100 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0919 16:29:43.686405 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0919 16:29:43.686628 140474388072320 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 16:29:43.804749 140474388072320 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 16:29:43.932101 140474388072320 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 16:29:44.053914 140474388072320 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 16:29:44.171649 140474388072320 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 16:29:44.287070 140474388072320 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0919 16:29:49.224641 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0919 16:29:49.225951 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0919 16:29:49.844701 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:208: The name tf.losses.add_loss is deprecated. Please use tf.compat.v1.losses.add_loss instead.\n",
            "\n",
            "W0919 16:29:49.845607 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/optimizer_builder.py:95: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0919 16:29:49.845832 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0919 16:29:49.855696 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0919 16:29:52.827353 140474388072320 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py:119: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0919 16:29:55.380819 140474388072320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/moving_averages.py:433: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0919 16:30:01.182853 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:353: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0919 16:30:01.986706 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:355: The name tf.losses.get_losses is deprecated. Please use tf.compat.v1.losses.get_losses instead.\n",
            "\n",
            "W0919 16:30:01.989053 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:359: The name tf.losses.get_total_loss is deprecated. Please use tf.compat.v1.losses.get_total_loss instead.\n",
            "\n",
            "W0919 16:30:01.994481 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:368: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0919 16:30:02.006927 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/legacy/trainer.py:376: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0919 16:30:02.985536 140474388072320 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0919 16:30:02.988955 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.989525 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.989645 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.989759 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.989866 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.989974 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.990101 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.990237 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.990345 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.990458 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.990569 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.990673 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.990787 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.990889 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.990991 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.991137 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.991251 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.991363 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/Conv_1/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.991481 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.991595 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.991703 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.991822 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.991926 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.992034 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.992185 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.992285 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.992393 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.992507 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.992619 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.992724 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.992840 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.992942 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.993046 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.993189 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.993292 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.993398 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.993514 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.993623 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.993731 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.993844 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.993948 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.994054 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.994199 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.994301 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.994407 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.994523 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.994637 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.994744 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.994859 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.994962 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.995074 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.995215 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.995317 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.995424 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.995538 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.995651 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.995759 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.995872 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.995978 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.996083 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.996226 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.996328 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.996434 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.996557 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.996661 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.996767 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.996882 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.996984 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.997091 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.997231 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.997335 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.997442 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.997565 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.997668 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.997776 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.997890 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.997994 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.998102 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.998245 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.998347 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.998455 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.998574 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:02.998678 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:02.998784 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:02.998897 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.060992 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.061254 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.061436 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.061580 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.061722 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.061868 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.062024 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.062199 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.062355 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.062491 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.062647 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.062812 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.062952 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.063102 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.063275 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.063412 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.063549 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.063697 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.063833 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.063968 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.064187 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.064451 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.064639 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.064810 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.064960 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.065126 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.065308 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.065463 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.065608 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.065779 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.065928 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.066070 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.066256 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.066401 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.066544 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.066698 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.066842 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.066984 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.067173 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.067333 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.067476 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.067615 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.067723 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.067831 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.067955 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.068065 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.068202 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.068340 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.068447 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.068558 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.068681 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.068790 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.068902 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.069021 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.069147 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.069262 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.069397 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.069503 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.069613 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.069734 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.069839 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.069948 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.070067 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.070198 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.070307 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.070441 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.070547 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.070658 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.070782 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.070888 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.070997 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.071132 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.071245 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.071360 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.071495 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.071599 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.071710 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.071830 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.071938 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.072046 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.072180 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.072297 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.072407 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.072540 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.072647 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.072757 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.072876 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.072984 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.073095 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.073235 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.073343 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.073455 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.073589 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.073692 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.073803 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.073923 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.074029 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.074151 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.074284 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.074390 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.074605 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.074764 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.074877 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.074996 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.075139 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.075243 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.075354 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.075473 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.075587 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.075695 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.075826 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.075930 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.076036 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.076174 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.076282 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.076390 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.076505 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.076625 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.076733 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.076864 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.076967 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.077085 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.167762 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.168056 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.168267 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.168426 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.168575 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.168718 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.168881 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.169039 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.169198 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.169351 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.169502 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.169647 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.169795 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.169935 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.170084 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.170274 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.170416 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.170558 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.170708 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.170847 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.170983 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.171161 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.171303 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.171441 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.171686 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.171833 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.171973 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.172156 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.172298 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.172434 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.172583 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.172719 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.172857 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.173017 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.173192 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.173330 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.173480 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.173619 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.173754 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.173904 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.174051 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.174211 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.174386 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.174526 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.174663 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.174812 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.174952 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.175101 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.175273 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.175411 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.175550 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.175714 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.175853 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.175991 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.176167 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.176309 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.176449 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.176598 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.176737 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.176873 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.177047 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.177211 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.177345 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.177496 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.177634 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.177772 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.177918 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.178065 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.178227 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.178386 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.178505 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.178637 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.178783 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.178920 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.179064 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.179233 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.179369 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.179505 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.179675 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.179815 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.179953 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.180130 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.180270 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.180406 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.180553 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.180691 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.180828 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.180990 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.181154 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.181296 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.181442 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.181580 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.181715 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.181862 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.181998 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.182164 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.182328 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.182467 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.182626 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.182775 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.182919 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.183063 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.183231 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.183368 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.183506 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.183670 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.183810 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.183947 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.184104 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.184271 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.184408 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.184556 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.184695 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.184832 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.184993 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.185158 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.185297 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.185445 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.185584 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.185720 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.185869 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.186005 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.186172 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.186337 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.186480 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.186618 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.186767 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.186907 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.187053 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.187225 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.187355 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.187493 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.187654 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.187794 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.187932 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.188091 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.188251 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.188389 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.188536 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.188674 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.188812 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.188973 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.189141 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.189290 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.189444 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.189584 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.189722 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.189868 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.190007 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.190172 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.190339 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.190479 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.190618 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.190767 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.190903 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.191049 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.191216 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.191356 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.191489 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.191656 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.191796 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.191932 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.192089 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.192265 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.192404 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.192550 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.192691 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.192826 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.192990 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.193156 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.193294 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.193444 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.193582 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.193717 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.193866 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.194005 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.194169 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.194349 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.194489 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.194626 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.194773 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.194911 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.195055 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.195225 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.195364 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.195499 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.195666 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.195804 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.195939 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.196099 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.196258 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.196394 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.196541 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.196679 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.196814 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.196978 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.197144 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.197283 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.197428 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.197565 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.197700 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.197849 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.197984 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.198149 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.198314 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.198451 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.198587 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.198735 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.198873 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.199007 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.199201 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.199341 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.199476 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.199640 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.199779 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.199917 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.200073 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.200233 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.200371 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.200520 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.200659 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.200794 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.200957 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.201123 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.201265 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.201411 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.201548 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.201683 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.201829 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.201965 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.202131 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.202296 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.202434 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.202586 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.202739 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.202877 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.203013 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.203195 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.203331 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.203478 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.203636 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.203775 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.203912 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.204067 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.204230 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.204380 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.204526 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.204665 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.204801 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.204965 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.205135 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.205274 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.205424 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.205560 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.205696 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.205841 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.205977 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.206140 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.206307 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.206445 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.206583 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.206734 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.206875 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.207014 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.207190 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.207331 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.207468 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.207633 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.207773 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.207911 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.208070 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.208229 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.208369 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.208517 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.208655 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.208791 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.208955 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.209122 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.209264 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.209414 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.209553 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.209691 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.209837 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.209975 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.210141 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.210306 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.210449 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.210583 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.210733 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.210870 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.211007 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.211184 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.211323 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.211461 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.211622 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.211760 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.211895 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.212058 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.212215 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.212356 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.212496 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.212635 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.212771 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.212935 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.213087 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.213243 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.213392 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.213531 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.213668 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.213815 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.213954 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.214099 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.214294 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.214436 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.214571 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.214720 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.214857 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.214993 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.215167 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.215435 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.215579 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.215744 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.215883 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.216022 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.216204 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.216344 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.216483 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.216630 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.216768 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.216903 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.217076 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.217242 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.217379 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.217528 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.217668 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.217804 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.217950 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.218096 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.218255 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.218416 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.218556 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.218693 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.218842 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.218980 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.219146 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.219296 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.219434 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.219572 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.219736 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.219877 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.220007 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.220187 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.220329 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.220468 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.220617 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.220753 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.220895 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.221069 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.221229 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.221365 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.221514 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.221652 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.221790 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/beta/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.221937 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.222088 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.222246 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm/gamma/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:03.222410 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/ExponentialMovingAverage] is not available in checkpoint\n",
            "W0919 16:30:03.222564 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp] is not available in checkpoint\n",
            "W0919 16:30:03.222703 140474388072320 variables_helper.py:157] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights/RMSProp_1] is not available in checkpoint\n",
            "W0919 16:30:04.292428 140474388072320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py:742: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-09-19 16:30:06.538790: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-19 16:30:06.539190: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56323526db80 executing computations on platform Host. Devices:\n",
            "2019-09-19 16:30:06.539228: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-19 16:30:06.546206: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-19 16:30:06.667532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 16:30:06.668385: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56323526dd40 executing computations on platform CUDA. Devices:\n",
            "2019-09-19 16:30:06.668418: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-19 16:30:06.669642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 16:30:06.670335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-19 16:30:06.684390: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-19 16:30:06.892398: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-19 16:30:06.972413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-19 16:30:06.999623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-19 16:30:07.211411: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-19 16:30:07.337548: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-19 16:30:07.709428: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-19 16:30:07.709697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 16:30:07.710544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 16:30:07.711235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-19 16:30:07.716030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-19 16:30:07.717675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-19 16:30:07.717711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-19 16:30:07.717728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-19 16:30:07.720121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 16:30:07.720894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 16:30:07.721588: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-19 16:30:07.721648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2019-09-19 16:30:11.394878: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "W0919 16:30:11.749489 140474388072320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0919 16:30:11.750658 140474388072320 saver.py:1280] Restoring parameters from /content/models/research/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt\n",
            "I0919 16:30:12.365645 140474388072320 session_manager.py:500] Running local_init_op.\n",
            "I0919 16:30:12.868603 140474388072320 session_manager.py:502] Done running local_init_op.\n",
            "I0919 16:30:26.499521 140474388072320 learning.py:754] Starting Session.\n",
            "I0919 16:30:26.872509 140471305729792 supervisor.py:1099] global_step/sec: 0\n",
            "I0919 16:30:26.873250 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 16:30:26.877484 140474388072320 learning.py:768] Starting Queues.\n",
            "2019-09-19 16:30:48.244320: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "I0919 16:30:53.831202 140471297337088 supervisor.py:1050] Recording summary at step 0.\n",
            "I0919 16:30:55.573576 140474388072320 learning.py:507] global step 1: loss = 10.8320 (28.308 sec/step)\n",
            "I0919 16:30:57.274913 140474388072320 learning.py:507] global step 2: loss = 10.1483 (1.301 sec/step)\n",
            "I0919 16:30:58.580421 140474388072320 learning.py:507] global step 3: loss = 9.9375 (1.304 sec/step)\n",
            "I0919 16:30:59.937272 140474388072320 learning.py:507] global step 4: loss = 9.5824 (1.355 sec/step)\n",
            "I0919 16:31:01.298587 140474388072320 learning.py:507] global step 5: loss = 9.5027 (1.360 sec/step)\n",
            "I0919 16:31:02.624291 140474388072320 learning.py:507] global step 6: loss = 9.3716 (1.324 sec/step)\n",
            "I0919 16:31:03.951270 140474388072320 learning.py:507] global step 7: loss = 9.2496 (1.325 sec/step)\n",
            "I0919 16:31:05.254997 140474388072320 learning.py:507] global step 8: loss = 9.2070 (1.302 sec/step)\n",
            "I0919 16:31:06.562632 140474388072320 learning.py:507] global step 9: loss = 9.0562 (1.306 sec/step)\n",
            "I0919 16:31:07.910038 140474388072320 learning.py:507] global step 10: loss = 8.7504 (1.346 sec/step)\n",
            "I0919 16:31:09.194328 140474388072320 learning.py:507] global step 11: loss = 8.9319 (1.283 sec/step)\n",
            "I0919 16:31:10.549427 140474388072320 learning.py:507] global step 12: loss = 8.7922 (1.353 sec/step)\n",
            "I0919 16:31:11.870544 140474388072320 learning.py:507] global step 13: loss = 8.6257 (1.319 sec/step)\n",
            "I0919 16:31:13.189450 140474388072320 learning.py:507] global step 14: loss = 8.6427 (1.317 sec/step)\n",
            "I0919 16:31:14.538989 140474388072320 learning.py:507] global step 15: loss = 8.6764 (1.348 sec/step)\n",
            "I0919 16:31:15.827955 140474388072320 learning.py:507] global step 16: loss = 8.4577 (1.287 sec/step)\n",
            "I0919 16:31:17.155706 140474388072320 learning.py:507] global step 17: loss = 8.2341 (1.326 sec/step)\n",
            "I0919 16:31:18.451415 140474388072320 learning.py:507] global step 18: loss = 8.2883 (1.294 sec/step)\n",
            "I0919 16:31:19.760268 140474388072320 learning.py:507] global step 19: loss = 8.2831 (1.307 sec/step)\n",
            "I0919 16:31:21.053016 140474388072320 learning.py:507] global step 20: loss = 8.1428 (1.291 sec/step)\n",
            "I0919 16:31:22.400278 140474388072320 learning.py:507] global step 21: loss = 7.7244 (1.346 sec/step)\n",
            "I0919 16:31:23.750441 140474388072320 learning.py:507] global step 22: loss = 7.9110 (1.349 sec/step)\n",
            "I0919 16:31:25.072714 140474388072320 learning.py:507] global step 23: loss = 8.0037 (1.321 sec/step)\n",
            "I0919 16:31:26.435446 140474388072320 learning.py:507] global step 24: loss = 8.1481 (1.361 sec/step)\n",
            "I0919 16:31:27.801096 140474388072320 learning.py:507] global step 25: loss = 7.9319 (1.364 sec/step)\n",
            "I0919 16:31:29.153317 140474388072320 learning.py:507] global step 26: loss = 7.5879 (1.350 sec/step)\n",
            "I0919 16:31:30.465276 140474388072320 learning.py:507] global step 27: loss = 7.4164 (1.310 sec/step)\n",
            "I0919 16:31:31.795056 140474388072320 learning.py:507] global step 28: loss = 7.7337 (1.328 sec/step)\n",
            "I0919 16:31:33.110400 140474388072320 learning.py:507] global step 29: loss = 7.4766 (1.314 sec/step)\n",
            "I0919 16:31:34.445594 140474388072320 learning.py:507] global step 30: loss = 7.1158 (1.333 sec/step)\n",
            "I0919 16:31:35.766248 140474388072320 learning.py:507] global step 31: loss = 7.2433 (1.319 sec/step)\n",
            "I0919 16:31:37.096075 140474388072320 learning.py:507] global step 32: loss = 7.4671 (1.328 sec/step)\n",
            "I0919 16:31:38.411539 140474388072320 learning.py:507] global step 33: loss = 7.1909 (1.314 sec/step)\n",
            "I0919 16:31:39.719029 140474388072320 learning.py:507] global step 34: loss = 6.8610 (1.306 sec/step)\n",
            "I0919 16:31:41.008719 140474388072320 learning.py:507] global step 35: loss = 6.7091 (1.288 sec/step)\n",
            "I0919 16:31:42.325680 140474388072320 learning.py:507] global step 36: loss = 6.7838 (1.315 sec/step)\n",
            "I0919 16:31:43.655488 140474388072320 learning.py:507] global step 37: loss = 6.7968 (1.328 sec/step)\n",
            "I0919 16:31:45.001179 140474388072320 learning.py:507] global step 38: loss = 6.8989 (1.344 sec/step)\n",
            "I0919 16:31:46.290498 140474388072320 learning.py:507] global step 39: loss = 6.9581 (1.288 sec/step)\n",
            "I0919 16:31:47.642007 140474388072320 learning.py:507] global step 40: loss = 6.7324 (1.350 sec/step)\n",
            "I0919 16:31:48.924343 140474388072320 learning.py:507] global step 41: loss = 6.5874 (1.281 sec/step)\n",
            "I0919 16:31:50.240149 140474388072320 learning.py:507] global step 42: loss = 6.2777 (1.314 sec/step)\n",
            "I0919 16:31:51.529783 140474388072320 learning.py:507] global step 43: loss = 6.2043 (1.288 sec/step)\n",
            "I0919 16:31:52.833950 140474388072320 learning.py:507] global step 44: loss = 6.5079 (1.303 sec/step)\n",
            "I0919 16:31:54.159816 140474388072320 learning.py:507] global step 45: loss = 6.1668 (1.324 sec/step)\n",
            "I0919 16:31:55.452598 140474388072320 learning.py:507] global step 46: loss = 6.3067 (1.291 sec/step)\n",
            "I0919 16:31:56.783921 140474388072320 learning.py:507] global step 47: loss = 6.0144 (1.329 sec/step)\n",
            "I0919 16:31:58.109693 140474388072320 learning.py:507] global step 48: loss = 6.2595 (1.324 sec/step)\n",
            "I0919 16:31:59.409568 140474388072320 learning.py:507] global step 49: loss = 5.9992 (1.298 sec/step)\n",
            "I0919 16:32:00.696254 140474388072320 learning.py:507] global step 50: loss = 6.0186 (1.284 sec/step)\n",
            "I0919 16:32:02.001274 140474388072320 learning.py:507] global step 51: loss = 5.6680 (1.303 sec/step)\n",
            "I0919 16:32:03.284887 140474388072320 learning.py:507] global step 52: loss = 6.4781 (1.282 sec/step)\n",
            "I0919 16:32:04.593135 140474388072320 learning.py:507] global step 53: loss = 5.8090 (1.306 sec/step)\n",
            "I0919 16:32:05.955701 140474388072320 learning.py:507] global step 54: loss = 6.0506 (1.361 sec/step)\n",
            "I0919 16:32:07.301002 140474388072320 learning.py:507] global step 55: loss = 5.5642 (1.344 sec/step)\n",
            "I0919 16:32:08.644881 140474388072320 learning.py:507] global step 56: loss = 5.5262 (1.342 sec/step)\n",
            "I0919 16:32:09.978728 140474388072320 learning.py:507] global step 57: loss = 5.2502 (1.332 sec/step)\n",
            "I0919 16:32:11.309746 140474388072320 learning.py:507] global step 58: loss = 5.7251 (1.329 sec/step)\n",
            "I0919 16:32:12.639950 140474388072320 learning.py:507] global step 59: loss = 5.7377 (1.328 sec/step)\n",
            "I0919 16:32:13.993134 140474388072320 learning.py:507] global step 60: loss = 5.9665 (1.352 sec/step)\n",
            "I0919 16:32:15.309268 140474388072320 learning.py:507] global step 61: loss = 5.4997 (1.314 sec/step)\n",
            "I0919 16:32:16.662240 140474388072320 learning.py:507] global step 62: loss = 5.6254 (1.351 sec/step)\n",
            "I0919 16:32:17.978849 140474388072320 learning.py:507] global step 63: loss = 5.7807 (1.315 sec/step)\n",
            "I0919 16:32:19.278729 140474388072320 learning.py:507] global step 64: loss = 5.1341 (1.298 sec/step)\n",
            "I0919 16:32:20.577949 140474388072320 learning.py:507] global step 65: loss = 5.3457 (1.298 sec/step)\n",
            "I0919 16:32:21.862747 140474388072320 learning.py:507] global step 66: loss = 5.2213 (1.283 sec/step)\n",
            "I0919 16:32:23.195768 140474388072320 learning.py:507] global step 67: loss = 5.2697 (1.331 sec/step)\n",
            "I0919 16:32:24.499072 140474388072320 learning.py:507] global step 68: loss = 4.7668 (1.302 sec/step)\n",
            "I0919 16:32:25.811372 140474388072320 learning.py:507] global step 69: loss = 5.0839 (1.311 sec/step)\n",
            "I0919 16:32:27.137319 140471305729792 supervisor.py:1099] global_step/sec: 0.573734\n",
            "I0919 16:32:27.217247 140474388072320 learning.py:507] global step 70: loss = 5.2157 (1.336 sec/step)\n",
            "I0919 16:32:28.880580 140471297337088 supervisor.py:1050] Recording summary at step 70.\n",
            "I0919 16:32:29.306873 140474388072320 learning.py:507] global step 71: loss = 4.8716 (1.947 sec/step)\n",
            "I0919 16:32:30.639744 140474388072320 learning.py:507] global step 72: loss = 4.7763 (1.331 sec/step)\n",
            "I0919 16:32:31.945260 140474388072320 learning.py:507] global step 73: loss = 4.8521 (1.304 sec/step)\n",
            "I0919 16:32:33.321213 140474388072320 learning.py:507] global step 74: loss = 4.8418 (1.374 sec/step)\n",
            "I0919 16:32:34.628494 140474388072320 learning.py:507] global step 75: loss = 4.6604 (1.306 sec/step)\n",
            "I0919 16:32:35.935370 140474388072320 learning.py:507] global step 76: loss = 4.7376 (1.305 sec/step)\n",
            "I0919 16:32:37.295090 140474388072320 learning.py:507] global step 77: loss = 5.1992 (1.357 sec/step)\n",
            "I0919 16:32:38.607399 140474388072320 learning.py:507] global step 78: loss = 4.6572 (1.311 sec/step)\n",
            "I0919 16:32:39.907655 140474388072320 learning.py:507] global step 79: loss = 4.5099 (1.299 sec/step)\n",
            "I0919 16:32:41.209543 140474388072320 learning.py:507] global step 80: loss = 4.7035 (1.300 sec/step)\n",
            "I0919 16:32:42.512890 140474388072320 learning.py:507] global step 81: loss = 4.4260 (1.302 sec/step)\n",
            "I0919 16:32:43.864466 140474388072320 learning.py:507] global step 82: loss = 4.6618 (1.350 sec/step)\n",
            "I0919 16:32:45.170785 140474388072320 learning.py:507] global step 83: loss = 4.5221 (1.305 sec/step)\n",
            "I0919 16:32:46.467801 140474388072320 learning.py:507] global step 84: loss = 4.4172 (1.295 sec/step)\n",
            "I0919 16:32:47.788506 140474388072320 learning.py:507] global step 85: loss = 4.5498 (1.319 sec/step)\n",
            "I0919 16:32:49.109077 140474388072320 learning.py:507] global step 86: loss = 4.0751 (1.319 sec/step)\n",
            "I0919 16:32:50.460089 140474388072320 learning.py:507] global step 87: loss = 4.3681 (1.349 sec/step)\n",
            "I0919 16:32:51.782465 140474388072320 learning.py:507] global step 88: loss = 4.3304 (1.321 sec/step)\n",
            "I0919 16:32:53.094815 140474388072320 learning.py:507] global step 89: loss = 4.1129 (1.311 sec/step)\n",
            "I0919 16:32:54.392776 140474388072320 learning.py:507] global step 90: loss = 4.2046 (1.296 sec/step)\n",
            "I0919 16:32:55.682487 140474388072320 learning.py:507] global step 91: loss = 4.1136 (1.288 sec/step)\n",
            "I0919 16:32:57.007034 140474388072320 learning.py:507] global step 92: loss = 4.5377 (1.323 sec/step)\n",
            "I0919 16:32:58.356769 140474388072320 learning.py:507] global step 93: loss = 3.9264 (1.347 sec/step)\n",
            "I0919 16:32:59.661757 140474388072320 learning.py:507] global step 94: loss = 3.9470 (1.303 sec/step)\n",
            "I0919 16:33:00.951050 140474388072320 learning.py:507] global step 95: loss = 3.9503 (1.287 sec/step)\n",
            "I0919 16:33:02.284399 140474388072320 learning.py:507] global step 96: loss = 4.0855 (1.331 sec/step)\n",
            "I0919 16:33:03.580534 140474388072320 learning.py:507] global step 97: loss = 4.0050 (1.294 sec/step)\n",
            "I0919 16:33:04.887212 140474388072320 learning.py:507] global step 98: loss = 4.0590 (1.305 sec/step)\n",
            "I0919 16:33:06.176429 140474388072320 learning.py:507] global step 99: loss = 3.7121 (1.288 sec/step)\n",
            "I0919 16:33:07.486882 140474388072320 learning.py:507] global step 100: loss = 3.9100 (1.309 sec/step)\n",
            "I0919 16:33:08.803939 140474388072320 learning.py:507] global step 101: loss = 4.4197 (1.316 sec/step)\n",
            "I0919 16:33:10.103998 140474388072320 learning.py:507] global step 102: loss = 3.7978 (1.298 sec/step)\n",
            "I0919 16:33:11.411237 140474388072320 learning.py:507] global step 103: loss = 3.8463 (1.305 sec/step)\n",
            "I0919 16:33:12.697948 140474388072320 learning.py:507] global step 104: loss = 3.6827 (1.285 sec/step)\n",
            "I0919 16:33:14.036890 140474388072320 learning.py:507] global step 105: loss = 3.8160 (1.337 sec/step)\n",
            "I0919 16:33:15.322839 140474388072320 learning.py:507] global step 106: loss = 3.7787 (1.284 sec/step)\n",
            "I0919 16:33:16.614876 140474388072320 learning.py:507] global step 107: loss = 3.7024 (1.290 sec/step)\n",
            "I0919 16:33:18.016819 140474388072320 learning.py:507] global step 108: loss = 3.5534 (1.400 sec/step)\n",
            "I0919 16:33:19.325702 140474388072320 learning.py:507] global step 109: loss = 3.3700 (1.307 sec/step)\n",
            "I0919 16:33:20.634746 140474388072320 learning.py:507] global step 110: loss = 3.6446 (1.307 sec/step)\n",
            "I0919 16:33:21.964492 140474388072320 learning.py:507] global step 111: loss = 3.6119 (1.328 sec/step)\n",
            "I0919 16:33:23.299897 140474388072320 learning.py:507] global step 112: loss = 3.3949 (1.334 sec/step)\n",
            "I0919 16:33:24.622078 140474388072320 learning.py:507] global step 113: loss = 3.2046 (1.320 sec/step)\n",
            "I0919 16:33:25.932890 140474388072320 learning.py:507] global step 114: loss = 3.1674 (1.309 sec/step)\n",
            "I0919 16:33:27.299787 140474388072320 learning.py:507] global step 115: loss = 3.4923 (1.365 sec/step)\n",
            "I0919 16:33:28.578670 140474388072320 learning.py:507] global step 116: loss = 3.0531 (1.277 sec/step)\n",
            "I0919 16:33:29.878914 140474388072320 learning.py:507] global step 117: loss = 3.8134 (1.298 sec/step)\n",
            "I0919 16:33:31.177582 140474388072320 learning.py:507] global step 118: loss = 3.9152 (1.296 sec/step)\n",
            "I0919 16:33:32.506736 140474388072320 learning.py:507] global step 119: loss = 3.2059 (1.327 sec/step)\n",
            "I0919 16:33:33.799731 140474388072320 learning.py:507] global step 120: loss = 3.2683 (1.291 sec/step)\n",
            "I0919 16:33:35.087867 140474388072320 learning.py:507] global step 121: loss = 3.4883 (1.286 sec/step)\n",
            "I0919 16:33:36.432792 140474388072320 learning.py:507] global step 122: loss = 3.9049 (1.343 sec/step)\n",
            "I0919 16:33:37.819799 140474388072320 learning.py:507] global step 123: loss = 3.6626 (1.385 sec/step)\n",
            "I0919 16:33:39.216065 140474388072320 learning.py:507] global step 124: loss = 3.3449 (1.394 sec/step)\n",
            "I0919 16:33:40.593137 140474388072320 learning.py:507] global step 125: loss = 3.3706 (1.375 sec/step)\n",
            "I0919 16:33:41.909955 140474388072320 learning.py:507] global step 126: loss = 3.5329 (1.315 sec/step)\n",
            "I0919 16:33:43.243062 140474388072320 learning.py:507] global step 127: loss = 3.5863 (1.331 sec/step)\n",
            "I0919 16:33:44.549182 140474388072320 learning.py:507] global step 128: loss = 3.0412 (1.305 sec/step)\n",
            "I0919 16:33:45.843126 140474388072320 learning.py:507] global step 129: loss = 3.0181 (1.292 sec/step)\n",
            "I0919 16:33:47.160300 140474388072320 learning.py:507] global step 130: loss = 3.7755 (1.315 sec/step)\n",
            "I0919 16:33:48.470289 140474388072320 learning.py:507] global step 131: loss = 3.3305 (1.308 sec/step)\n",
            "I0919 16:33:49.771189 140474388072320 learning.py:507] global step 132: loss = 2.9225 (1.299 sec/step)\n",
            "I0919 16:33:51.059179 140474388072320 learning.py:507] global step 133: loss = 3.0927 (1.286 sec/step)\n",
            "I0919 16:33:52.354928 140474388072320 learning.py:507] global step 134: loss = 3.1667 (1.294 sec/step)\n",
            "I0919 16:33:53.659781 140474388072320 learning.py:507] global step 135: loss = 3.2400 (1.303 sec/step)\n",
            "I0919 16:33:55.002907 140474388072320 learning.py:507] global step 136: loss = 2.9542 (1.341 sec/step)\n",
            "I0919 16:33:56.280858 140474388072320 learning.py:507] global step 137: loss = 3.0608 (1.276 sec/step)\n",
            "I0919 16:33:57.611457 140474388072320 learning.py:507] global step 138: loss = 3.2184 (1.329 sec/step)\n",
            "I0919 16:33:58.941473 140474388072320 learning.py:507] global step 139: loss = 4.2663 (1.328 sec/step)\n",
            "I0919 16:34:00.277616 140474388072320 learning.py:507] global step 140: loss = 2.9850 (1.335 sec/step)\n",
            "I0919 16:34:01.587205 140474388072320 learning.py:507] global step 141: loss = 3.3218 (1.308 sec/step)\n",
            "I0919 16:34:02.910947 140474388072320 learning.py:507] global step 142: loss = 2.9395 (1.322 sec/step)\n",
            "I0919 16:34:04.203022 140474388072320 learning.py:507] global step 143: loss = 3.4020 (1.290 sec/step)\n",
            "I0919 16:34:05.491631 140474388072320 learning.py:507] global step 144: loss = 2.8058 (1.287 sec/step)\n",
            "I0919 16:34:06.807298 140474388072320 learning.py:507] global step 145: loss = 3.3245 (1.314 sec/step)\n",
            "I0919 16:34:08.113972 140474388072320 learning.py:507] global step 146: loss = 3.0861 (1.305 sec/step)\n",
            "I0919 16:34:09.393059 140474388072320 learning.py:507] global step 147: loss = 2.9603 (1.277 sec/step)\n",
            "I0919 16:34:10.682845 140474388072320 learning.py:507] global step 148: loss = 2.7204 (1.288 sec/step)\n",
            "I0919 16:34:12.008419 140474388072320 learning.py:507] global step 149: loss = 3.2884 (1.324 sec/step)\n",
            "I0919 16:34:13.366215 140474388072320 learning.py:507] global step 150: loss = 3.1699 (1.356 sec/step)\n",
            "I0919 16:34:14.664335 140474388072320 learning.py:507] global step 151: loss = 2.8721 (1.296 sec/step)\n",
            "I0919 16:34:15.960273 140474388072320 learning.py:507] global step 152: loss = 2.8610 (1.294 sec/step)\n",
            "I0919 16:34:17.321069 140474388072320 learning.py:507] global step 153: loss = 2.8244 (1.359 sec/step)\n",
            "I0919 16:34:18.629188 140474388072320 learning.py:507] global step 154: loss = 2.8399 (1.306 sec/step)\n",
            "I0919 16:34:19.946003 140474388072320 learning.py:507] global step 155: loss = 2.8303 (1.315 sec/step)\n",
            "I0919 16:34:21.252835 140474388072320 learning.py:507] global step 156: loss = 3.0605 (1.305 sec/step)\n",
            "I0919 16:34:22.535454 140474388072320 learning.py:507] global step 157: loss = 2.4754 (1.281 sec/step)\n",
            "I0919 16:34:23.830501 140474388072320 learning.py:507] global step 158: loss = 2.9279 (1.293 sec/step)\n",
            "I0919 16:34:25.133917 140474388072320 learning.py:507] global step 159: loss = 2.7696 (1.302 sec/step)\n",
            "I0919 16:34:26.440779 140474388072320 learning.py:507] global step 160: loss = 2.6486 (1.305 sec/step)\n",
            "I0919 16:34:27.169281 140471305729792 supervisor.py:1099] global_step/sec: 0.758131\n",
            "I0919 16:34:28.030090 140474388072320 learning.py:507] global step 161: loss = 2.9442 (1.568 sec/step)\n",
            "I0919 16:34:29.474773 140471297337088 supervisor.py:1050] Recording summary at step 161.\n",
            "I0919 16:34:29.918714 140474388072320 learning.py:507] global step 162: loss = 2.8307 (1.886 sec/step)\n",
            "I0919 16:34:31.224843 140474388072320 learning.py:507] global step 163: loss = 2.6291 (1.304 sec/step)\n",
            "I0919 16:34:32.550228 140474388072320 learning.py:507] global step 164: loss = 2.6267 (1.324 sec/step)\n",
            "I0919 16:34:33.893755 140474388072320 learning.py:507] global step 165: loss = 2.6526 (1.342 sec/step)\n",
            "I0919 16:34:35.175708 140474388072320 learning.py:507] global step 166: loss = 2.7551 (1.280 sec/step)\n",
            "I0919 16:34:36.475739 140474388072320 learning.py:507] global step 167: loss = 2.5288 (1.298 sec/step)\n",
            "I0919 16:34:37.784354 140474388072320 learning.py:507] global step 168: loss = 3.0290 (1.307 sec/step)\n",
            "I0919 16:34:39.117813 140474388072320 learning.py:507] global step 169: loss = 3.0587 (1.332 sec/step)\n",
            "I0919 16:34:40.403904 140474388072320 learning.py:507] global step 170: loss = 2.8348 (1.284 sec/step)\n",
            "I0919 16:34:41.714462 140474388072320 learning.py:507] global step 171: loss = 2.9971 (1.309 sec/step)\n",
            "I0919 16:34:42.992966 140474388072320 learning.py:507] global step 172: loss = 2.8860 (1.277 sec/step)\n",
            "I0919 16:34:44.290762 140474388072320 learning.py:507] global step 173: loss = 3.1812 (1.296 sec/step)\n",
            "I0919 16:34:45.641659 140474388072320 learning.py:507] global step 174: loss = 2.6263 (1.346 sec/step)\n",
            "I0919 16:34:46.995430 140474388072320 learning.py:507] global step 175: loss = 2.5831 (1.352 sec/step)\n",
            "I0919 16:34:48.295540 140474388072320 learning.py:507] global step 176: loss = 2.5788 (1.298 sec/step)\n",
            "I0919 16:34:49.605454 140474388072320 learning.py:507] global step 177: loss = 2.5836 (1.308 sec/step)\n",
            "I0919 16:34:50.884318 140474388072320 learning.py:507] global step 178: loss = 2.6296 (1.277 sec/step)\n",
            "I0919 16:34:52.199981 140474388072320 learning.py:507] global step 179: loss = 3.1657 (1.314 sec/step)\n",
            "I0919 16:34:53.501581 140474388072320 learning.py:507] global step 180: loss = 3.1611 (1.300 sec/step)\n",
            "I0919 16:34:54.810041 140474388072320 learning.py:507] global step 181: loss = 3.3462 (1.307 sec/step)\n",
            "I0919 16:34:56.116884 140474388072320 learning.py:507] global step 182: loss = 2.6852 (1.305 sec/step)\n",
            "I0919 16:34:57.410214 140474388072320 learning.py:507] global step 183: loss = 2.7767 (1.292 sec/step)\n",
            "I0919 16:34:58.726964 140474388072320 learning.py:507] global step 184: loss = 2.6540 (1.315 sec/step)\n",
            "I0919 16:35:00.001056 140474388072320 learning.py:507] global step 185: loss = 2.8449 (1.272 sec/step)\n",
            "I0919 16:35:01.317646 140474388072320 learning.py:507] global step 186: loss = 2.5047 (1.314 sec/step)\n",
            "I0919 16:35:02.630644 140474388072320 learning.py:507] global step 187: loss = 2.5835 (1.311 sec/step)\n",
            "I0919 16:35:03.930579 140474388072320 learning.py:507] global step 188: loss = 2.6862 (1.298 sec/step)\n",
            "I0919 16:35:05.221043 140474388072320 learning.py:507] global step 189: loss = 2.7659 (1.288 sec/step)\n",
            "I0919 16:35:06.521681 140474388072320 learning.py:507] global step 190: loss = 2.4629 (1.299 sec/step)\n",
            "I0919 16:35:07.857300 140474388072320 learning.py:507] global step 191: loss = 2.5253 (1.334 sec/step)\n",
            "I0919 16:35:09.163990 140474388072320 learning.py:507] global step 192: loss = 2.4251 (1.305 sec/step)\n",
            "I0919 16:35:10.476530 140474388072320 learning.py:507] global step 193: loss = 2.5220 (1.311 sec/step)\n",
            "I0919 16:35:11.758593 140474388072320 learning.py:507] global step 194: loss = 2.6952 (1.280 sec/step)\n",
            "I0919 16:35:13.045449 140474388072320 learning.py:507] global step 195: loss = 2.3739 (1.285 sec/step)\n",
            "I0919 16:35:14.337229 140474388072320 learning.py:507] global step 196: loss = 2.6217 (1.290 sec/step)\n",
            "I0919 16:35:15.621866 140474388072320 learning.py:507] global step 197: loss = 2.8733 (1.283 sec/step)\n",
            "I0919 16:35:16.908709 140474388072320 learning.py:507] global step 198: loss = 2.3156 (1.285 sec/step)\n",
            "I0919 16:35:18.229079 140474388072320 learning.py:507] global step 199: loss = 2.4424 (1.319 sec/step)\n",
            "I0919 16:35:19.550839 140474388072320 learning.py:507] global step 200: loss = 3.1079 (1.320 sec/step)\n",
            "I0919 16:35:20.934448 140474388072320 learning.py:507] global step 201: loss = 2.2960 (1.382 sec/step)\n",
            "I0919 16:35:22.285037 140474388072320 learning.py:507] global step 202: loss = 2.6480 (1.349 sec/step)\n",
            "I0919 16:35:23.603813 140474388072320 learning.py:507] global step 203: loss = 2.3777 (1.317 sec/step)\n",
            "I0919 16:35:24.921291 140474388072320 learning.py:507] global step 204: loss = 2.2838 (1.316 sec/step)\n",
            "I0919 16:35:26.204385 140474388072320 learning.py:507] global step 205: loss = 2.5177 (1.282 sec/step)\n",
            "I0919 16:35:27.509897 140474388072320 learning.py:507] global step 206: loss = 2.5588 (1.304 sec/step)\n",
            "I0919 16:35:28.827481 140474388072320 learning.py:507] global step 207: loss = 2.7759 (1.316 sec/step)\n",
            "I0919 16:35:30.104304 140474388072320 learning.py:507] global step 208: loss = 2.3991 (1.275 sec/step)\n",
            "I0919 16:35:31.411469 140474388072320 learning.py:507] global step 209: loss = 2.3976 (1.305 sec/step)\n",
            "I0919 16:35:32.728985 140474388072320 learning.py:507] global step 210: loss = 2.7503 (1.316 sec/step)\n",
            "I0919 16:35:34.013194 140474388072320 learning.py:507] global step 211: loss = 2.2654 (1.282 sec/step)\n",
            "I0919 16:35:35.310931 140474388072320 learning.py:507] global step 212: loss = 2.4476 (1.296 sec/step)\n",
            "I0919 16:35:36.646922 140474388072320 learning.py:507] global step 213: loss = 2.4940 (1.334 sec/step)\n",
            "I0919 16:35:37.996006 140474388072320 learning.py:507] global step 214: loss = 2.8442 (1.347 sec/step)\n",
            "I0919 16:35:39.327682 140474388072320 learning.py:507] global step 215: loss = 2.2865 (1.330 sec/step)\n",
            "I0919 16:35:40.666079 140474388072320 learning.py:507] global step 216: loss = 2.2160 (1.337 sec/step)\n",
            "I0919 16:35:41.968178 140474388072320 learning.py:507] global step 217: loss = 2.5286 (1.300 sec/step)\n",
            "I0919 16:35:43.293270 140474388072320 learning.py:507] global step 218: loss = 2.1811 (1.323 sec/step)\n",
            "I0919 16:35:44.593463 140474388072320 learning.py:507] global step 219: loss = 2.2773 (1.299 sec/step)\n",
            "I0919 16:35:45.861011 140474388072320 learning.py:507] global step 220: loss = 2.6675 (1.266 sec/step)\n",
            "I0919 16:35:47.166191 140474388072320 learning.py:507] global step 221: loss = 2.5811 (1.304 sec/step)\n",
            "I0919 16:35:48.465992 140474388072320 learning.py:507] global step 222: loss = 2.2199 (1.298 sec/step)\n",
            "I0919 16:35:49.755799 140474388072320 learning.py:507] global step 223: loss = 2.2187 (1.288 sec/step)\n",
            "I0919 16:35:51.111774 140474388072320 learning.py:507] global step 224: loss = 2.4453 (1.354 sec/step)\n",
            "I0919 16:35:52.405252 140474388072320 learning.py:507] global step 225: loss = 2.7114 (1.291 sec/step)\n",
            "I0919 16:35:53.715104 140474388072320 learning.py:507] global step 226: loss = 2.1980 (1.308 sec/step)\n",
            "I0919 16:35:55.015173 140474388072320 learning.py:507] global step 227: loss = 2.3750 (1.298 sec/step)\n",
            "I0919 16:35:56.339548 140474388072320 learning.py:507] global step 228: loss = 2.3297 (1.323 sec/step)\n",
            "I0919 16:35:57.669792 140474388072320 learning.py:507] global step 229: loss = 2.2716 (1.329 sec/step)\n",
            "I0919 16:35:58.966792 140474388072320 learning.py:507] global step 230: loss = 2.4131 (1.295 sec/step)\n",
            "I0919 16:36:00.252942 140474388072320 learning.py:507] global step 231: loss = 2.3553 (1.284 sec/step)\n",
            "I0919 16:36:01.546250 140474388072320 learning.py:507] global step 232: loss = 1.9081 (1.291 sec/step)\n",
            "I0919 16:36:02.837884 140474388072320 learning.py:507] global step 233: loss = 2.0771 (1.290 sec/step)\n",
            "I0919 16:36:04.128638 140474388072320 learning.py:507] global step 234: loss = 2.3389 (1.289 sec/step)\n",
            "I0919 16:36:05.474193 140474388072320 learning.py:507] global step 235: loss = 2.3390 (1.344 sec/step)\n",
            "I0919 16:36:06.770200 140474388072320 learning.py:507] global step 236: loss = 1.9561 (1.294 sec/step)\n",
            "I0919 16:36:08.085640 140474388072320 learning.py:507] global step 237: loss = 2.1513 (1.314 sec/step)\n",
            "I0919 16:36:09.408874 140474388072320 learning.py:507] global step 238: loss = 2.1521 (1.321 sec/step)\n",
            "I0919 16:36:10.751468 140474388072320 learning.py:507] global step 239: loss = 2.2570 (1.341 sec/step)\n",
            "I0919 16:36:12.024867 140474388072320 learning.py:507] global step 240: loss = 2.1939 (1.272 sec/step)\n",
            "I0919 16:36:13.314846 140474388072320 learning.py:507] global step 241: loss = 2.1052 (1.288 sec/step)\n",
            "I0919 16:36:14.627338 140474388072320 learning.py:507] global step 242: loss = 2.9221 (1.311 sec/step)\n",
            "I0919 16:36:15.971574 140474388072320 learning.py:507] global step 243: loss = 2.2435 (1.342 sec/step)\n",
            "I0919 16:36:17.290055 140474388072320 learning.py:507] global step 244: loss = 2.4153 (1.317 sec/step)\n",
            "I0919 16:36:18.627931 140474388072320 learning.py:507] global step 245: loss = 1.8784 (1.336 sec/step)\n",
            "I0919 16:36:19.937288 140474388072320 learning.py:507] global step 246: loss = 2.3402 (1.308 sec/step)\n",
            "I0919 16:36:21.237754 140474388072320 learning.py:507] global step 247: loss = 1.6909 (1.299 sec/step)\n",
            "I0919 16:36:22.530399 140474388072320 learning.py:507] global step 248: loss = 2.1044 (1.291 sec/step)\n",
            "I0919 16:36:23.851433 140474388072320 learning.py:507] global step 249: loss = 2.5989 (1.319 sec/step)\n",
            "I0919 16:36:25.161659 140474388072320 learning.py:507] global step 250: loss = 2.0993 (1.308 sec/step)\n",
            "I0919 16:36:26.529661 140474388072320 learning.py:507] global step 251: loss = 2.2623 (1.366 sec/step)\n",
            "I0919 16:36:27.344630 140471305729792 supervisor.py:1099] global_step/sec: 0.757227\n",
            "I0919 16:36:27.844022 140474388072320 learning.py:507] global step 252: loss = 2.0531 (1.312 sec/step)\n",
            "I0919 16:36:29.963192 140474388072320 learning.py:507] global step 253: loss = 2.5024 (2.117 sec/step)\n",
            "I0919 16:36:29.982566 140471297337088 supervisor.py:1050] Recording summary at step 253.\n",
            "I0919 16:36:31.282907 140474388072320 learning.py:507] global step 254: loss = 1.9643 (1.317 sec/step)\n",
            "I0919 16:36:32.608650 140474388072320 learning.py:507] global step 255: loss = 2.2324 (1.324 sec/step)\n",
            "I0919 16:36:33.910800 140474388072320 learning.py:507] global step 256: loss = 2.3448 (1.301 sec/step)\n",
            "I0919 16:36:35.215339 140474388072320 learning.py:507] global step 257: loss = 2.0569 (1.303 sec/step)\n",
            "I0919 16:36:36.512702 140474388072320 learning.py:507] global step 258: loss = 2.2339 (1.296 sec/step)\n",
            "I0919 16:36:37.801373 140474388072320 learning.py:507] global step 259: loss = 2.6573 (1.287 sec/step)\n",
            "I0919 16:36:39.169384 140474388072320 learning.py:507] global step 260: loss = 2.3920 (1.366 sec/step)\n",
            "I0919 16:36:40.472999 140474388072320 learning.py:507] global step 261: loss = 2.3801 (1.302 sec/step)\n",
            "I0919 16:36:41.767509 140474388072320 learning.py:507] global step 262: loss = 2.1235 (1.293 sec/step)\n",
            "I0919 16:36:43.091845 140474388072320 learning.py:507] global step 263: loss = 2.1973 (1.323 sec/step)\n",
            "I0919 16:36:44.369905 140474388072320 learning.py:507] global step 264: loss = 1.9385 (1.276 sec/step)\n",
            "I0919 16:36:45.671833 140474388072320 learning.py:507] global step 265: loss = 1.9830 (1.300 sec/step)\n",
            "I0919 16:36:46.970690 140474388072320 learning.py:507] global step 266: loss = 1.9325 (1.297 sec/step)\n",
            "I0919 16:36:48.355287 140474388072320 learning.py:507] global step 267: loss = 2.1776 (1.382 sec/step)\n",
            "I0919 16:36:49.668144 140474388072320 learning.py:507] global step 268: loss = 2.2175 (1.307 sec/step)\n",
            "I0919 16:36:50.956238 140474388072320 learning.py:507] global step 269: loss = 2.2609 (1.285 sec/step)\n",
            "I0919 16:36:52.295239 140474388072320 learning.py:507] global step 270: loss = 1.9991 (1.337 sec/step)\n",
            "I0919 16:36:53.575387 140474388072320 learning.py:507] global step 271: loss = 2.5576 (1.279 sec/step)\n",
            "I0919 16:36:54.953262 140474388072320 learning.py:507] global step 272: loss = 2.4350 (1.376 sec/step)\n",
            "I0919 16:36:56.283070 140474388072320 learning.py:507] global step 273: loss = 1.8299 (1.328 sec/step)\n",
            "I0919 16:36:57.584814 140474388072320 learning.py:507] global step 274: loss = 2.3797 (1.300 sec/step)\n",
            "I0919 16:36:58.922266 140474388072320 learning.py:507] global step 275: loss = 1.9512 (1.336 sec/step)\n",
            "I0919 16:37:00.265588 140474388072320 learning.py:507] global step 276: loss = 1.9442 (1.342 sec/step)\n",
            "I0919 16:37:01.577215 140474388072320 learning.py:507] global step 277: loss = 2.0720 (1.310 sec/step)\n",
            "I0919 16:37:02.899194 140474388072320 learning.py:507] global step 278: loss = 1.8634 (1.320 sec/step)\n",
            "I0919 16:37:04.232572 140474388072320 learning.py:507] global step 279: loss = 2.2188 (1.332 sec/step)\n",
            "I0919 16:37:05.534101 140474388072320 learning.py:507] global step 280: loss = 1.9896 (1.300 sec/step)\n",
            "I0919 16:37:06.854511 140474388072320 learning.py:507] global step 281: loss = 1.9125 (1.318 sec/step)\n",
            "I0919 16:37:08.174493 140474388072320 learning.py:507] global step 282: loss = 2.0915 (1.318 sec/step)\n",
            "I0919 16:37:09.476263 140474388072320 learning.py:507] global step 283: loss = 1.9460 (1.300 sec/step)\n",
            "I0919 16:37:10.758593 140474388072320 learning.py:507] global step 284: loss = 2.1807 (1.280 sec/step)\n",
            "I0919 16:37:12.065243 140474388072320 learning.py:507] global step 285: loss = 2.3387 (1.305 sec/step)\n",
            "I0919 16:37:13.369203 140474388072320 learning.py:507] global step 286: loss = 2.0070 (1.302 sec/step)\n",
            "I0919 16:37:14.687755 140474388072320 learning.py:507] global step 287: loss = 1.7426 (1.316 sec/step)\n",
            "I0919 16:37:16.002435 140474388072320 learning.py:507] global step 288: loss = 2.1519 (1.313 sec/step)\n",
            "I0919 16:37:17.299724 140474388072320 learning.py:507] global step 289: loss = 1.7184 (1.295 sec/step)\n",
            "I0919 16:37:18.611630 140474388072320 learning.py:507] global step 290: loss = 2.4806 (1.310 sec/step)\n",
            "I0919 16:37:19.913625 140474388072320 learning.py:507] global step 291: loss = 1.9332 (1.300 sec/step)\n",
            "I0919 16:37:21.189310 140474388072320 learning.py:507] global step 292: loss = 1.9364 (1.274 sec/step)\n",
            "I0919 16:37:22.500504 140474388072320 learning.py:507] global step 293: loss = 2.1630 (1.309 sec/step)\n",
            "I0919 16:37:23.826267 140474388072320 learning.py:507] global step 294: loss = 1.8933 (1.324 sec/step)\n",
            "I0919 16:37:25.159401 140474388072320 learning.py:507] global step 295: loss = 2.2410 (1.331 sec/step)\n",
            "I0919 16:37:26.518733 140474388072320 learning.py:507] global step 296: loss = 2.0205 (1.357 sec/step)\n",
            "I0919 16:37:27.883387 140474388072320 learning.py:507] global step 297: loss = 1.9717 (1.363 sec/step)\n",
            "I0919 16:37:29.168930 140474388072320 learning.py:507] global step 298: loss = 2.2448 (1.284 sec/step)\n",
            "I0919 16:37:30.476963 140474388072320 learning.py:507] global step 299: loss = 2.2209 (1.306 sec/step)\n",
            "I0919 16:37:31.780951 140474388072320 learning.py:507] global step 300: loss = 2.0485 (1.302 sec/step)\n",
            "I0919 16:37:33.076497 140474388072320 learning.py:507] global step 301: loss = 1.9759 (1.293 sec/step)\n",
            "I0919 16:37:34.358002 140474388072320 learning.py:507] global step 302: loss = 2.2531 (1.279 sec/step)\n",
            "I0919 16:37:35.683276 140474388072320 learning.py:507] global step 303: loss = 1.6470 (1.324 sec/step)\n",
            "I0919 16:37:36.961632 140474388072320 learning.py:507] global step 304: loss = 2.0779 (1.277 sec/step)\n",
            "I0919 16:37:38.240942 140474388072320 learning.py:507] global step 305: loss = 2.4779 (1.278 sec/step)\n",
            "I0919 16:37:39.619332 140474388072320 learning.py:507] global step 306: loss = 2.5121 (1.376 sec/step)\n",
            "I0919 16:37:40.920924 140474388072320 learning.py:507] global step 307: loss = 1.9303 (1.299 sec/step)\n",
            "I0919 16:37:42.228810 140474388072320 learning.py:507] global step 308: loss = 1.8488 (1.306 sec/step)\n",
            "I0919 16:37:43.532802 140474388072320 learning.py:507] global step 309: loss = 2.0187 (1.302 sec/step)\n",
            "I0919 16:37:44.826706 140474388072320 learning.py:507] global step 310: loss = 1.9604 (1.292 sec/step)\n",
            "I0919 16:37:46.109457 140474388072320 learning.py:507] global step 311: loss = 1.9683 (1.281 sec/step)\n",
            "I0919 16:37:47.425286 140474388072320 learning.py:507] global step 312: loss = 2.1008 (1.314 sec/step)\n",
            "I0919 16:37:48.725518 140474388072320 learning.py:507] global step 313: loss = 1.8320 (1.299 sec/step)\n",
            "I0919 16:37:50.045665 140474388072320 learning.py:507] global step 314: loss = 1.8763 (1.318 sec/step)\n",
            "I0919 16:37:51.393577 140474388072320 learning.py:507] global step 315: loss = 1.8891 (1.346 sec/step)\n",
            "I0919 16:37:52.752494 140474388072320 learning.py:507] global step 316: loss = 1.9947 (1.357 sec/step)\n",
            "I0919 16:37:54.108971 140474388072320 learning.py:507] global step 317: loss = 1.8170 (1.355 sec/step)\n",
            "I0919 16:37:55.447457 140474388072320 learning.py:507] global step 318: loss = 2.5745 (1.336 sec/step)\n",
            "I0919 16:37:56.713093 140474388072320 learning.py:507] global step 319: loss = 2.0665 (1.264 sec/step)\n",
            "I0919 16:37:58.064105 140474388072320 learning.py:507] global step 320: loss = 1.8165 (1.349 sec/step)\n",
            "I0919 16:37:59.375910 140474388072320 learning.py:507] global step 321: loss = 1.9399 (1.310 sec/step)\n",
            "I0919 16:38:00.667979 140474388072320 learning.py:507] global step 322: loss = 1.5708 (1.290 sec/step)\n",
            "I0919 16:38:01.949407 140474388072320 learning.py:507] global step 323: loss = 1.5376 (1.280 sec/step)\n",
            "I0919 16:38:03.254968 140474388072320 learning.py:507] global step 324: loss = 2.0944 (1.304 sec/step)\n",
            "I0919 16:38:04.587363 140474388072320 learning.py:507] global step 325: loss = 1.7519 (1.331 sec/step)\n",
            "I0919 16:38:05.925714 140474388072320 learning.py:507] global step 326: loss = 2.1589 (1.337 sec/step)\n",
            "I0919 16:38:07.239778 140474388072320 learning.py:507] global step 327: loss = 1.9767 (1.312 sec/step)\n",
            "I0919 16:38:08.554200 140474388072320 learning.py:507] global step 328: loss = 1.9135 (1.312 sec/step)\n",
            "I0919 16:38:09.849409 140474388072320 learning.py:507] global step 329: loss = 1.9082 (1.293 sec/step)\n",
            "I0919 16:38:11.148356 140474388072320 learning.py:507] global step 330: loss = 1.9937 (1.297 sec/step)\n",
            "I0919 16:38:12.476950 140474388072320 learning.py:507] global step 331: loss = 2.0188 (1.327 sec/step)\n",
            "I0919 16:38:13.764987 140474388072320 learning.py:507] global step 332: loss = 1.7722 (1.286 sec/step)\n",
            "I0919 16:38:15.058783 140474388072320 learning.py:507] global step 333: loss = 1.9866 (1.292 sec/step)\n",
            "I0919 16:38:16.366739 140474388072320 learning.py:507] global step 334: loss = 2.0332 (1.306 sec/step)\n",
            "I0919 16:38:17.689960 140474388072320 learning.py:507] global step 335: loss = 2.0517 (1.322 sec/step)\n",
            "I0919 16:38:19.033735 140474388072320 learning.py:507] global step 336: loss = 1.6458 (1.342 sec/step)\n",
            "I0919 16:38:20.348683 140474388072320 learning.py:507] global step 337: loss = 2.1296 (1.313 sec/step)\n",
            "I0919 16:38:21.633984 140474388072320 learning.py:507] global step 338: loss = 1.9758 (1.283 sec/step)\n",
            "I0919 16:38:23.003848 140474388072320 learning.py:507] global step 339: loss = 1.9908 (1.368 sec/step)\n",
            "I0919 16:38:24.295485 140474388072320 learning.py:507] global step 340: loss = 2.0641 (1.290 sec/step)\n",
            "I0919 16:38:25.582778 140474388072320 learning.py:507] global step 341: loss = 1.6967 (1.285 sec/step)\n",
            "I0919 16:38:26.872350 140471305729792 supervisor.py:1099] global_step/sec: 0.752963\n",
            "I0919 16:38:26.906697 140474388072320 learning.py:507] global step 342: loss = 1.6832 (1.314 sec/step)\n",
            "I0919 16:38:29.151489 140471297337088 supervisor.py:1050] Recording summary at step 343.\n",
            "I0919 16:38:29.169440 140474388072320 learning.py:507] global step 343: loss = 1.7919 (2.260 sec/step)\n",
            "I0919 16:38:30.446445 140474388072320 learning.py:507] global step 344: loss = 1.6551 (1.275 sec/step)\n",
            "I0919 16:38:31.717633 140474388072320 learning.py:507] global step 345: loss = 2.1276 (1.270 sec/step)\n",
            "I0919 16:38:33.001811 140474388072320 learning.py:507] global step 346: loss = 1.8667 (1.283 sec/step)\n",
            "I0919 16:38:34.314299 140474388072320 learning.py:507] global step 347: loss = 1.6265 (1.311 sec/step)\n",
            "I0919 16:38:35.617501 140474388072320 learning.py:507] global step 348: loss = 1.7787 (1.301 sec/step)\n",
            "I0919 16:38:36.893800 140474388072320 learning.py:507] global step 349: loss = 1.5629 (1.275 sec/step)\n",
            "I0919 16:38:38.225157 140474388072320 learning.py:507] global step 350: loss = 1.6443 (1.330 sec/step)\n",
            "I0919 16:38:39.533627 140474388072320 learning.py:507] global step 351: loss = 1.7168 (1.307 sec/step)\n",
            "I0919 16:38:40.846241 140474388072320 learning.py:507] global step 352: loss = 1.8908 (1.311 sec/step)\n",
            "I0919 16:38:42.176101 140474388072320 learning.py:507] global step 353: loss = 1.8400 (1.328 sec/step)\n",
            "I0919 16:38:43.484630 140474388072320 learning.py:507] global step 354: loss = 1.9348 (1.307 sec/step)\n",
            "I0919 16:38:44.814450 140474388072320 learning.py:507] global step 355: loss = 2.4598 (1.328 sec/step)\n",
            "I0919 16:38:46.127480 140474388072320 learning.py:507] global step 356: loss = 2.0988 (1.311 sec/step)\n",
            "I0919 16:38:47.446330 140474388072320 learning.py:507] global step 357: loss = 1.7207 (1.317 sec/step)\n",
            "I0919 16:38:48.733484 140474388072320 learning.py:507] global step 358: loss = 1.7606 (1.285 sec/step)\n",
            "I0919 16:38:50.072585 140474388072320 learning.py:507] global step 359: loss = 1.9243 (1.337 sec/step)\n",
            "I0919 16:38:51.452275 140474388072320 learning.py:507] global step 360: loss = 1.8774 (1.378 sec/step)\n",
            "I0919 16:38:52.757848 140474388072320 learning.py:507] global step 361: loss = 1.4912 (1.304 sec/step)\n",
            "I0919 16:38:54.062815 140474388072320 learning.py:507] global step 362: loss = 1.7988 (1.303 sec/step)\n",
            "I0919 16:38:55.359294 140474388072320 learning.py:507] global step 363: loss = 1.6891 (1.295 sec/step)\n",
            "I0919 16:38:56.648904 140474388072320 learning.py:507] global step 364: loss = 1.7305 (1.288 sec/step)\n",
            "I0919 16:38:57.967961 140474388072320 learning.py:507] global step 365: loss = 1.8501 (1.317 sec/step)\n",
            "I0919 16:38:59.298329 140474388072320 learning.py:507] global step 366: loss = 1.7671 (1.328 sec/step)\n",
            "I0919 16:39:00.639914 140474388072320 learning.py:507] global step 367: loss = 1.7240 (1.340 sec/step)\n",
            "I0919 16:39:01.953993 140474388072320 learning.py:507] global step 368: loss = 2.0525 (1.312 sec/step)\n",
            "I0919 16:39:03.254523 140474388072320 learning.py:507] global step 369: loss = 1.7416 (1.298 sec/step)\n",
            "I0919 16:39:04.531266 140474388072320 learning.py:507] global step 370: loss = 1.6692 (1.274 sec/step)\n",
            "I0919 16:39:05.858688 140474388072320 learning.py:507] global step 371: loss = 2.0030 (1.326 sec/step)\n",
            "I0919 16:39:07.153208 140474388072320 learning.py:507] global step 372: loss = 1.9259 (1.293 sec/step)\n",
            "I0919 16:39:08.486887 140474388072320 learning.py:507] global step 373: loss = 1.5991 (1.332 sec/step)\n",
            "I0919 16:39:09.794162 140474388072320 learning.py:507] global step 374: loss = 1.7287 (1.305 sec/step)\n",
            "I0919 16:39:11.103903 140474388072320 learning.py:507] global step 375: loss = 1.5877 (1.308 sec/step)\n",
            "I0919 16:39:12.427346 140474388072320 learning.py:507] global step 376: loss = 1.8400 (1.322 sec/step)\n",
            "I0919 16:39:13.759577 140474388072320 learning.py:507] global step 377: loss = 2.1225 (1.331 sec/step)\n",
            "I0919 16:39:15.104669 140474388072320 learning.py:507] global step 378: loss = 1.4343 (1.343 sec/step)\n",
            "I0919 16:39:16.430986 140474388072320 learning.py:507] global step 379: loss = 1.9956 (1.325 sec/step)\n",
            "I0919 16:39:17.720638 140474388072320 learning.py:507] global step 380: loss = 1.8595 (1.288 sec/step)\n",
            "I0919 16:39:19.023857 140474388072320 learning.py:507] global step 381: loss = 1.5933 (1.301 sec/step)\n",
            "I0919 16:39:20.341309 140474388072320 learning.py:507] global step 382: loss = 1.7972 (1.316 sec/step)\n",
            "I0919 16:39:21.632864 140474388072320 learning.py:507] global step 383: loss = 1.7044 (1.290 sec/step)\n",
            "I0919 16:39:22.913049 140474388072320 learning.py:507] global step 384: loss = 1.6406 (1.279 sec/step)\n",
            "I0919 16:39:24.211807 140474388072320 learning.py:507] global step 385: loss = 1.7886 (1.297 sec/step)\n",
            "I0919 16:39:25.503566 140474388072320 learning.py:507] global step 386: loss = 2.1750 (1.290 sec/step)\n",
            "I0919 16:39:26.824408 140474388072320 learning.py:507] global step 387: loss = 1.6917 (1.319 sec/step)\n",
            "I0919 16:39:28.105736 140474388072320 learning.py:507] global step 388: loss = 1.5975 (1.280 sec/step)\n",
            "I0919 16:39:29.385085 140474388072320 learning.py:507] global step 389: loss = 1.5807 (1.278 sec/step)\n",
            "I0919 16:39:30.668840 140474388072320 learning.py:507] global step 390: loss = 1.9985 (1.282 sec/step)\n",
            "I0919 16:39:31.979821 140474388072320 learning.py:507] global step 391: loss = 1.7333 (1.309 sec/step)\n",
            "I0919 16:39:33.272242 140474388072320 learning.py:507] global step 392: loss = 1.9411 (1.291 sec/step)\n",
            "I0919 16:39:34.575841 140474388072320 learning.py:507] global step 393: loss = 1.5109 (1.302 sec/step)\n",
            "I0919 16:39:35.891027 140474388072320 learning.py:507] global step 394: loss = 1.8221 (1.313 sec/step)\n",
            "I0919 16:39:37.192521 140474388072320 learning.py:507] global step 395: loss = 1.8279 (1.300 sec/step)\n",
            "I0919 16:39:38.485851 140474388072320 learning.py:507] global step 396: loss = 1.8810 (1.292 sec/step)\n",
            "I0919 16:39:39.818537 140474388072320 learning.py:507] global step 397: loss = 1.7311 (1.331 sec/step)\n",
            "I0919 16:39:41.120321 140474388072320 learning.py:507] global step 398: loss = 1.7492 (1.300 sec/step)\n",
            "I0919 16:39:42.425832 140474388072320 learning.py:507] global step 399: loss = 1.7198 (1.304 sec/step)\n",
            "I0919 16:39:43.770776 140474388072320 learning.py:507] global step 400: loss = 2.4587 (1.343 sec/step)\n",
            "I0919 16:39:45.060331 140474388072320 learning.py:507] global step 401: loss = 1.9902 (1.288 sec/step)\n",
            "I0919 16:39:46.357470 140474388072320 learning.py:507] global step 402: loss = 1.5712 (1.295 sec/step)\n",
            "I0919 16:39:47.651705 140474388072320 learning.py:507] global step 403: loss = 1.7169 (1.293 sec/step)\n",
            "I0919 16:39:48.933608 140474388072320 learning.py:507] global step 404: loss = 1.7080 (1.280 sec/step)\n",
            "I0919 16:39:50.220541 140474388072320 learning.py:507] global step 405: loss = 1.5208 (1.285 sec/step)\n",
            "I0919 16:39:51.493176 140474388072320 learning.py:507] global step 406: loss = 1.6800 (1.271 sec/step)\n",
            "I0919 16:39:52.785341 140474388072320 learning.py:507] global step 407: loss = 1.3836 (1.290 sec/step)\n",
            "I0919 16:39:54.110860 140474388072320 learning.py:507] global step 408: loss = 1.6077 (1.324 sec/step)\n",
            "I0919 16:39:55.445457 140474388072320 learning.py:507] global step 409: loss = 1.6837 (1.333 sec/step)\n",
            "I0919 16:39:56.771689 140474388072320 learning.py:507] global step 410: loss = 1.6772 (1.325 sec/step)\n",
            "I0919 16:39:58.062874 140474388072320 learning.py:507] global step 411: loss = 2.0097 (1.290 sec/step)\n",
            "I0919 16:39:59.374258 140474388072320 learning.py:507] global step 412: loss = 1.7452 (1.310 sec/step)\n",
            "I0919 16:40:00.698441 140474388072320 learning.py:507] global step 413: loss = 2.0158 (1.323 sec/step)\n",
            "I0919 16:40:02.034229 140474388072320 learning.py:507] global step 414: loss = 1.6069 (1.334 sec/step)\n",
            "I0919 16:40:03.404170 140474388072320 learning.py:507] global step 415: loss = 1.5042 (1.368 sec/step)\n",
            "I0919 16:40:04.714639 140474388072320 learning.py:507] global step 416: loss = 2.0323 (1.309 sec/step)\n",
            "I0919 16:40:06.050930 140474388072320 learning.py:507] global step 417: loss = 1.5662 (1.334 sec/step)\n",
            "I0919 16:40:07.341871 140474388072320 learning.py:507] global step 418: loss = 2.0527 (1.289 sec/step)\n",
            "I0919 16:40:08.689404 140474388072320 learning.py:507] global step 419: loss = 1.7100 (1.346 sec/step)\n",
            "I0919 16:40:10.001617 140474388072320 learning.py:507] global step 420: loss = 1.4591 (1.311 sec/step)\n",
            "I0919 16:40:11.322814 140474388072320 learning.py:507] global step 421: loss = 2.0101 (1.319 sec/step)\n",
            "I0919 16:40:12.631521 140474388072320 learning.py:507] global step 422: loss = 1.7814 (1.307 sec/step)\n",
            "I0919 16:40:13.962169 140474388072320 learning.py:507] global step 423: loss = 1.5766 (1.329 sec/step)\n",
            "I0919 16:40:15.328274 140474388072320 learning.py:507] global step 424: loss = 1.5104 (1.364 sec/step)\n",
            "I0919 16:40:16.637198 140474388072320 learning.py:507] global step 425: loss = 1.7429 (1.307 sec/step)\n",
            "I0919 16:40:17.905603 140474388072320 learning.py:507] global step 426: loss = 2.1372 (1.267 sec/step)\n",
            "I0919 16:40:19.182787 140474388072320 learning.py:507] global step 427: loss = 1.3946 (1.275 sec/step)\n",
            "I0919 16:40:20.512728 140474388072320 learning.py:507] global step 428: loss = 1.9067 (1.328 sec/step)\n",
            "I0919 16:40:21.824959 140474388072320 learning.py:507] global step 429: loss = 1.6023 (1.311 sec/step)\n",
            "I0919 16:40:23.121257 140474388072320 learning.py:507] global step 430: loss = 1.5264 (1.295 sec/step)\n",
            "I0919 16:40:24.431096 140474388072320 learning.py:507] global step 431: loss = 1.6820 (1.308 sec/step)\n",
            "I0919 16:40:25.739264 140474388072320 learning.py:507] global step 432: loss = 1.7361 (1.306 sec/step)\n",
            "I0919 16:40:26.872493 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 16:40:26.873301 140471305729792 supervisor.py:1099] global_step/sec: 0.758328\n",
            "I0919 16:40:27.371192 140474388072320 learning.py:507] global step 433: loss = 1.5549 (1.326 sec/step)\n",
            "I0919 16:40:30.572518 140471297337088 supervisor.py:1050] Recording summary at step 434.\n",
            "I0919 16:40:30.584852 140474388072320 learning.py:507] global step 434: loss = 1.6979 (3.187 sec/step)\n",
            "I0919 16:40:32.574613 140474388072320 learning.py:507] global step 435: loss = 1.3576 (1.987 sec/step)\n",
            "I0919 16:40:33.882596 140474388072320 learning.py:507] global step 436: loss = 1.6916 (1.306 sec/step)\n",
            "I0919 16:40:35.215032 140474388072320 learning.py:507] global step 437: loss = 1.5744 (1.331 sec/step)\n",
            "I0919 16:40:36.536099 140474388072320 learning.py:507] global step 438: loss = 1.8021 (1.319 sec/step)\n",
            "I0919 16:40:37.869872 140474388072320 learning.py:507] global step 439: loss = 1.4110 (1.332 sec/step)\n",
            "I0919 16:40:39.213745 140474388072320 learning.py:507] global step 440: loss = 1.4310 (1.342 sec/step)\n",
            "I0919 16:40:40.527010 140474388072320 learning.py:507] global step 441: loss = 1.7529 (1.311 sec/step)\n",
            "I0919 16:40:41.831032 140474388072320 learning.py:507] global step 442: loss = 1.6256 (1.302 sec/step)\n",
            "I0919 16:40:43.125643 140474388072320 learning.py:507] global step 443: loss = 1.4522 (1.293 sec/step)\n",
            "I0919 16:40:44.401779 140474388072320 learning.py:507] global step 444: loss = 1.4842 (1.274 sec/step)\n",
            "I0919 16:40:45.703598 140474388072320 learning.py:507] global step 445: loss = 1.3744 (1.300 sec/step)\n",
            "I0919 16:40:47.029218 140474388072320 learning.py:507] global step 446: loss = 1.6079 (1.324 sec/step)\n",
            "I0919 16:40:48.369359 140474388072320 learning.py:507] global step 447: loss = 1.6206 (1.339 sec/step)\n",
            "I0919 16:40:49.697242 140474388072320 learning.py:507] global step 448: loss = 1.6686 (1.326 sec/step)\n",
            "I0919 16:40:50.973896 140474388072320 learning.py:507] global step 449: loss = 1.6767 (1.275 sec/step)\n",
            "I0919 16:40:52.272588 140474388072320 learning.py:507] global step 450: loss = 1.5635 (1.297 sec/step)\n",
            "I0919 16:40:53.544810 140474388072320 learning.py:507] global step 451: loss = 1.7097 (1.271 sec/step)\n",
            "I0919 16:40:54.860291 140474388072320 learning.py:507] global step 452: loss = 1.5615 (1.314 sec/step)\n",
            "I0919 16:40:56.191929 140474388072320 learning.py:507] global step 453: loss = 1.6843 (1.330 sec/step)\n",
            "I0919 16:40:57.488821 140474388072320 learning.py:507] global step 454: loss = 1.6020 (1.295 sec/step)\n",
            "I0919 16:40:58.778985 140474388072320 learning.py:507] global step 455: loss = 2.0500 (1.289 sec/step)\n",
            "I0919 16:41:00.109751 140474388072320 learning.py:507] global step 456: loss = 1.6009 (1.329 sec/step)\n",
            "I0919 16:41:01.417829 140474388072320 learning.py:507] global step 457: loss = 1.8013 (1.306 sec/step)\n",
            "I0919 16:41:02.728224 140474388072320 learning.py:507] global step 458: loss = 1.2949 (1.309 sec/step)\n",
            "I0919 16:41:04.015274 140474388072320 learning.py:507] global step 459: loss = 1.5730 (1.285 sec/step)\n",
            "I0919 16:41:05.330537 140474388072320 learning.py:507] global step 460: loss = 1.4206 (1.313 sec/step)\n",
            "I0919 16:41:06.668193 140474388072320 learning.py:507] global step 461: loss = 1.6868 (1.336 sec/step)\n",
            "I0919 16:41:07.951226 140474388072320 learning.py:507] global step 462: loss = 1.9593 (1.281 sec/step)\n",
            "I0919 16:41:09.245265 140474388072320 learning.py:507] global step 463: loss = 1.6594 (1.292 sec/step)\n",
            "I0919 16:41:10.544811 140474388072320 learning.py:507] global step 464: loss = 1.9206 (1.298 sec/step)\n",
            "I0919 16:41:11.829582 140474388072320 learning.py:507] global step 465: loss = 1.5772 (1.283 sec/step)\n",
            "I0919 16:41:13.126729 140474388072320 learning.py:507] global step 466: loss = 1.6576 (1.295 sec/step)\n",
            "I0919 16:41:14.447542 140474388072320 learning.py:507] global step 467: loss = 1.3438 (1.319 sec/step)\n",
            "I0919 16:41:15.787905 140474388072320 learning.py:507] global step 468: loss = 1.7122 (1.339 sec/step)\n",
            "I0919 16:41:17.100270 140474388072320 learning.py:507] global step 469: loss = 1.4810 (1.311 sec/step)\n",
            "I0919 16:41:18.403857 140474388072320 learning.py:507] global step 470: loss = 1.8665 (1.301 sec/step)\n",
            "I0919 16:41:19.718152 140474388072320 learning.py:507] global step 471: loss = 1.4298 (1.312 sec/step)\n",
            "I0919 16:41:21.002090 140474388072320 learning.py:507] global step 472: loss = 1.7930 (1.282 sec/step)\n",
            "I0919 16:41:22.313467 140474388072320 learning.py:507] global step 473: loss = 1.6275 (1.310 sec/step)\n",
            "I0919 16:41:23.623732 140474388072320 learning.py:507] global step 474: loss = 1.6946 (1.308 sec/step)\n",
            "I0919 16:41:24.975991 140474388072320 learning.py:507] global step 475: loss = 1.5359 (1.351 sec/step)\n",
            "I0919 16:41:26.272874 140474388072320 learning.py:507] global step 476: loss = 1.5857 (1.295 sec/step)\n",
            "I0919 16:41:27.587996 140474388072320 learning.py:507] global step 477: loss = 1.4145 (1.314 sec/step)\n",
            "I0919 16:41:28.924223 140474388072320 learning.py:507] global step 478: loss = 1.6261 (1.334 sec/step)\n",
            "I0919 16:41:30.235858 140474388072320 learning.py:507] global step 479: loss = 1.5308 (1.310 sec/step)\n",
            "I0919 16:41:31.576742 140474388072320 learning.py:507] global step 480: loss = 1.5451 (1.339 sec/step)\n",
            "I0919 16:41:32.890192 140474388072320 learning.py:507] global step 481: loss = 1.5768 (1.311 sec/step)\n",
            "I0919 16:41:34.210159 140474388072320 learning.py:507] global step 482: loss = 1.7223 (1.318 sec/step)\n",
            "I0919 16:41:35.499759 140474388072320 learning.py:507] global step 483: loss = 1.6168 (1.288 sec/step)\n",
            "I0919 16:41:36.802515 140474388072320 learning.py:507] global step 484: loss = 1.6333 (1.301 sec/step)\n",
            "I0919 16:41:38.094633 140474388072320 learning.py:507] global step 485: loss = 1.4536 (1.290 sec/step)\n",
            "I0919 16:41:39.413736 140474388072320 learning.py:507] global step 486: loss = 1.8762 (1.317 sec/step)\n",
            "I0919 16:41:40.683939 140474388072320 learning.py:507] global step 487: loss = 1.4558 (1.268 sec/step)\n",
            "I0919 16:41:41.997307 140474388072320 learning.py:507] global step 488: loss = 1.6982 (1.311 sec/step)\n",
            "I0919 16:41:43.312215 140474388072320 learning.py:507] global step 489: loss = 1.5568 (1.313 sec/step)\n",
            "I0919 16:41:44.639868 140474388072320 learning.py:507] global step 490: loss = 1.4521 (1.326 sec/step)\n",
            "I0919 16:41:45.935206 140474388072320 learning.py:507] global step 491: loss = 1.6309 (1.293 sec/step)\n",
            "I0919 16:41:47.254132 140474388072320 learning.py:507] global step 492: loss = 1.5237 (1.317 sec/step)\n",
            "I0919 16:41:48.558787 140474388072320 learning.py:507] global step 493: loss = 1.5771 (1.303 sec/step)\n",
            "I0919 16:41:49.908993 140474388072320 learning.py:507] global step 494: loss = 1.4059 (1.348 sec/step)\n",
            "I0919 16:41:51.208900 140474388072320 learning.py:507] global step 495: loss = 1.4898 (1.298 sec/step)\n",
            "I0919 16:41:52.532542 140474388072320 learning.py:507] global step 496: loss = 1.5297 (1.322 sec/step)\n",
            "I0919 16:41:53.836373 140474388072320 learning.py:507] global step 497: loss = 1.3263 (1.302 sec/step)\n",
            "I0919 16:41:55.135158 140474388072320 learning.py:507] global step 498: loss = 1.5592 (1.297 sec/step)\n",
            "I0919 16:41:56.445867 140474388072320 learning.py:507] global step 499: loss = 1.6953 (1.309 sec/step)\n",
            "I0919 16:41:57.804428 140474388072320 learning.py:507] global step 500: loss = 1.5092 (1.357 sec/step)\n",
            "I0919 16:41:59.102428 140474388072320 learning.py:507] global step 501: loss = 1.3979 (1.296 sec/step)\n",
            "I0919 16:42:00.418801 140474388072320 learning.py:507] global step 502: loss = 1.4816 (1.315 sec/step)\n",
            "I0919 16:42:01.702857 140474388072320 learning.py:507] global step 503: loss = 1.7630 (1.283 sec/step)\n",
            "I0919 16:42:03.031627 140474388072320 learning.py:507] global step 504: loss = 1.7757 (1.326 sec/step)\n",
            "I0919 16:42:04.355997 140474388072320 learning.py:507] global step 505: loss = 1.3679 (1.323 sec/step)\n",
            "I0919 16:42:05.663916 140474388072320 learning.py:507] global step 506: loss = 1.5417 (1.306 sec/step)\n",
            "I0919 16:42:06.951836 140474388072320 learning.py:507] global step 507: loss = 1.5035 (1.286 sec/step)\n",
            "I0919 16:42:08.284496 140474388072320 learning.py:507] global step 508: loss = 1.2721 (1.331 sec/step)\n",
            "I0919 16:42:09.612673 140474388072320 learning.py:507] global step 509: loss = 1.5257 (1.326 sec/step)\n",
            "I0919 16:42:10.939826 140474388072320 learning.py:507] global step 510: loss = 1.4174 (1.325 sec/step)\n",
            "I0919 16:42:12.213749 140474388072320 learning.py:507] global step 511: loss = 1.3348 (1.272 sec/step)\n",
            "I0919 16:42:13.500324 140474388072320 learning.py:507] global step 512: loss = 1.4031 (1.285 sec/step)\n",
            "I0919 16:42:14.812990 140474388072320 learning.py:507] global step 513: loss = 1.3938 (1.311 sec/step)\n",
            "I0919 16:42:16.099503 140474388072320 learning.py:507] global step 514: loss = 1.6329 (1.285 sec/step)\n",
            "I0919 16:42:17.387800 140474388072320 learning.py:507] global step 515: loss = 1.3648 (1.287 sec/step)\n",
            "I0919 16:42:18.667399 140474388072320 learning.py:507] global step 516: loss = 1.5546 (1.278 sec/step)\n",
            "I0919 16:42:19.957727 140474388072320 learning.py:507] global step 517: loss = 1.4426 (1.288 sec/step)\n",
            "I0919 16:42:21.259896 140474388072320 learning.py:507] global step 518: loss = 1.7803 (1.301 sec/step)\n",
            "I0919 16:42:22.587208 140474388072320 learning.py:507] global step 519: loss = 1.6157 (1.325 sec/step)\n",
            "I0919 16:42:23.893102 140474388072320 learning.py:507] global step 520: loss = 1.6949 (1.304 sec/step)\n",
            "I0919 16:42:25.218059 140474388072320 learning.py:507] global step 521: loss = 1.5457 (1.323 sec/step)\n",
            "I0919 16:42:26.571053 140474388072320 learning.py:507] global step 522: loss = 1.8329 (1.351 sec/step)\n",
            "I0919 16:42:26.872575 140471305729792 supervisor.py:1099] global_step/sec: 0.750004\n",
            "I0919 16:42:28.776623 140471297337088 supervisor.py:1050] Recording summary at step 523.\n",
            "I0919 16:42:28.789145 140474388072320 learning.py:507] global step 523: loss = 1.4666 (2.216 sec/step)\n",
            "I0919 16:42:30.080647 140474388072320 learning.py:507] global step 524: loss = 1.5376 (1.286 sec/step)\n",
            "I0919 16:42:31.419116 140474388072320 learning.py:507] global step 525: loss = 1.5219 (1.337 sec/step)\n",
            "I0919 16:42:32.703097 140474388072320 learning.py:507] global step 526: loss = 1.5889 (1.282 sec/step)\n",
            "I0919 16:42:34.028305 140474388072320 learning.py:507] global step 527: loss = 1.5555 (1.324 sec/step)\n",
            "I0919 16:42:35.346681 140474388072320 learning.py:507] global step 528: loss = 1.4471 (1.317 sec/step)\n",
            "I0919 16:42:36.653191 140474388072320 learning.py:507] global step 529: loss = 1.5714 (1.304 sec/step)\n",
            "I0919 16:42:37.972733 140474388072320 learning.py:507] global step 530: loss = 1.1875 (1.318 sec/step)\n",
            "I0919 16:42:39.291739 140474388072320 learning.py:507] global step 531: loss = 1.4813 (1.317 sec/step)\n",
            "I0919 16:42:40.632380 140474388072320 learning.py:507] global step 532: loss = 1.3727 (1.339 sec/step)\n",
            "I0919 16:42:41.932729 140474388072320 learning.py:507] global step 533: loss = 1.3231 (1.298 sec/step)\n",
            "I0919 16:42:43.265238 140474388072320 learning.py:507] global step 534: loss = 1.3044 (1.331 sec/step)\n",
            "I0919 16:42:44.563630 140474388072320 learning.py:507] global step 535: loss = 2.3977 (1.297 sec/step)\n",
            "I0919 16:42:45.874428 140474388072320 learning.py:507] global step 536: loss = 1.4642 (1.309 sec/step)\n",
            "I0919 16:42:47.260008 140474388072320 learning.py:507] global step 537: loss = 1.2519 (1.384 sec/step)\n",
            "I0919 16:42:48.579003 140474388072320 learning.py:507] global step 538: loss = 1.4843 (1.317 sec/step)\n",
            "I0919 16:42:49.899684 140474388072320 learning.py:507] global step 539: loss = 1.5320 (1.319 sec/step)\n",
            "I0919 16:42:51.205498 140474388072320 learning.py:507] global step 540: loss = 1.5149 (1.304 sec/step)\n",
            "I0919 16:42:52.535032 140474388072320 learning.py:507] global step 541: loss = 1.4926 (1.327 sec/step)\n",
            "I0919 16:42:53.852309 140474388072320 learning.py:507] global step 542: loss = 1.6321 (1.315 sec/step)\n",
            "I0919 16:42:55.203964 140474388072320 learning.py:507] global step 543: loss = 1.5487 (1.350 sec/step)\n",
            "I0919 16:42:56.522135 140474388072320 learning.py:507] global step 544: loss = 1.6853 (1.316 sec/step)\n",
            "I0919 16:42:57.826375 140474388072320 learning.py:507] global step 545: loss = 1.4050 (1.303 sec/step)\n",
            "I0919 16:42:59.149013 140474388072320 learning.py:507] global step 546: loss = 1.5322 (1.321 sec/step)\n",
            "I0919 16:43:00.446150 140474388072320 learning.py:507] global step 547: loss = 1.8708 (1.295 sec/step)\n",
            "I0919 16:43:01.753051 140474388072320 learning.py:507] global step 548: loss = 1.5964 (1.305 sec/step)\n",
            "I0919 16:43:03.062803 140474388072320 learning.py:507] global step 549: loss = 1.5661 (1.308 sec/step)\n",
            "I0919 16:43:04.360601 140474388072320 learning.py:507] global step 550: loss = 1.4171 (1.296 sec/step)\n",
            "I0919 16:43:05.658447 140474388072320 learning.py:507] global step 551: loss = 1.7007 (1.296 sec/step)\n",
            "I0919 16:43:06.976489 140474388072320 learning.py:507] global step 552: loss = 1.6380 (1.316 sec/step)\n",
            "I0919 16:43:08.294296 140474388072320 learning.py:507] global step 553: loss = 1.3065 (1.316 sec/step)\n",
            "I0919 16:43:09.584990 140474388072320 learning.py:507] global step 554: loss = 1.3115 (1.289 sec/step)\n",
            "I0919 16:43:10.894063 140474388072320 learning.py:507] global step 555: loss = 1.3536 (1.307 sec/step)\n",
            "I0919 16:43:12.225908 140474388072320 learning.py:507] global step 556: loss = 1.5000 (1.330 sec/step)\n",
            "I0919 16:43:13.557199 140474388072320 learning.py:507] global step 557: loss = 1.3310 (1.329 sec/step)\n",
            "I0919 16:43:14.847853 140474388072320 learning.py:507] global step 558: loss = 1.4696 (1.289 sec/step)\n",
            "I0919 16:43:16.189541 140474388072320 learning.py:507] global step 559: loss = 1.3212 (1.340 sec/step)\n",
            "I0919 16:43:17.508258 140474388072320 learning.py:507] global step 560: loss = 1.5218 (1.317 sec/step)\n",
            "I0919 16:43:18.811048 140474388072320 learning.py:507] global step 561: loss = 1.4811 (1.301 sec/step)\n",
            "I0919 16:43:20.107017 140474388072320 learning.py:507] global step 562: loss = 1.4617 (1.294 sec/step)\n",
            "I0919 16:43:21.422439 140474388072320 learning.py:507] global step 563: loss = 1.4843 (1.314 sec/step)\n",
            "I0919 16:43:22.718949 140474388072320 learning.py:507] global step 564: loss = 1.6740 (1.295 sec/step)\n",
            "I0919 16:43:24.034688 140474388072320 learning.py:507] global step 565: loss = 1.4782 (1.313 sec/step)\n",
            "I0919 16:43:25.333144 140474388072320 learning.py:507] global step 566: loss = 1.6891 (1.297 sec/step)\n",
            "I0919 16:43:26.624880 140474388072320 learning.py:507] global step 567: loss = 1.4777 (1.290 sec/step)\n",
            "I0919 16:43:27.919631 140474388072320 learning.py:507] global step 568: loss = 1.7677 (1.293 sec/step)\n",
            "I0919 16:43:29.228290 140474388072320 learning.py:507] global step 569: loss = 1.8653 (1.307 sec/step)\n",
            "I0919 16:43:30.540009 140474388072320 learning.py:507] global step 570: loss = 1.7555 (1.307 sec/step)\n",
            "I0919 16:43:31.841239 140474388072320 learning.py:507] global step 571: loss = 2.1043 (1.299 sec/step)\n",
            "I0919 16:43:33.136219 140474388072320 learning.py:507] global step 572: loss = 1.6297 (1.293 sec/step)\n",
            "I0919 16:43:34.446737 140474388072320 learning.py:507] global step 573: loss = 1.6183 (1.309 sec/step)\n",
            "I0919 16:43:35.718019 140474388072320 learning.py:507] global step 574: loss = 1.3659 (1.269 sec/step)\n",
            "I0919 16:43:37.058070 140474388072320 learning.py:507] global step 575: loss = 1.8453 (1.338 sec/step)\n",
            "I0919 16:43:38.391819 140474388072320 learning.py:507] global step 576: loss = 1.4467 (1.332 sec/step)\n",
            "I0919 16:43:39.677431 140474388072320 learning.py:507] global step 577: loss = 1.2805 (1.284 sec/step)\n",
            "I0919 16:43:41.012809 140474388072320 learning.py:507] global step 578: loss = 1.1615 (1.334 sec/step)\n",
            "I0919 16:43:42.344961 140474388072320 learning.py:507] global step 579: loss = 1.4905 (1.330 sec/step)\n",
            "I0919 16:43:43.666280 140474388072320 learning.py:507] global step 580: loss = 1.3843 (1.320 sec/step)\n",
            "I0919 16:43:44.987581 140474388072320 learning.py:507] global step 581: loss = 1.9357 (1.319 sec/step)\n",
            "I0919 16:43:46.290297 140474388072320 learning.py:507] global step 582: loss = 1.3077 (1.301 sec/step)\n",
            "I0919 16:43:47.581146 140474388072320 learning.py:507] global step 583: loss = 1.3934 (1.289 sec/step)\n",
            "I0919 16:43:48.876833 140474388072320 learning.py:507] global step 584: loss = 1.4551 (1.294 sec/step)\n",
            "I0919 16:43:50.198321 140474388072320 learning.py:507] global step 585: loss = 1.7261 (1.320 sec/step)\n",
            "I0919 16:43:51.532239 140474388072320 learning.py:507] global step 586: loss = 1.9531 (1.332 sec/step)\n",
            "I0919 16:43:52.829381 140474388072320 learning.py:507] global step 587: loss = 1.3715 (1.296 sec/step)\n",
            "I0919 16:43:54.138051 140474388072320 learning.py:507] global step 588: loss = 1.1703 (1.307 sec/step)\n",
            "I0919 16:43:55.452616 140474388072320 learning.py:507] global step 589: loss = 1.6491 (1.312 sec/step)\n",
            "I0919 16:43:56.761696 140474388072320 learning.py:507] global step 590: loss = 1.6209 (1.307 sec/step)\n",
            "I0919 16:43:58.064042 140474388072320 learning.py:507] global step 591: loss = 1.3896 (1.301 sec/step)\n",
            "I0919 16:43:59.411716 140474388072320 learning.py:507] global step 592: loss = 1.6870 (1.346 sec/step)\n",
            "I0919 16:44:00.721276 140474388072320 learning.py:507] global step 593: loss = 1.3450 (1.308 sec/step)\n",
            "I0919 16:44:02.026664 140474388072320 learning.py:507] global step 594: loss = 1.3760 (1.304 sec/step)\n",
            "I0919 16:44:03.302851 140474388072320 learning.py:507] global step 595: loss = 1.8087 (1.274 sec/step)\n",
            "I0919 16:44:04.597919 140474388072320 learning.py:507] global step 596: loss = 1.5421 (1.293 sec/step)\n",
            "I0919 16:44:05.887504 140474388072320 learning.py:507] global step 597: loss = 1.1377 (1.288 sec/step)\n",
            "I0919 16:44:07.209878 140474388072320 learning.py:507] global step 598: loss = 1.5055 (1.321 sec/step)\n",
            "I0919 16:44:08.507841 140474388072320 learning.py:507] global step 599: loss = 1.4234 (1.296 sec/step)\n",
            "I0919 16:44:09.812762 140474388072320 learning.py:507] global step 600: loss = 1.4896 (1.303 sec/step)\n",
            "I0919 16:44:11.174726 140474388072320 learning.py:507] global step 601: loss = 1.2207 (1.360 sec/step)\n",
            "I0919 16:44:12.483308 140474388072320 learning.py:507] global step 602: loss = 1.4225 (1.307 sec/step)\n",
            "I0919 16:44:13.781898 140474388072320 learning.py:507] global step 603: loss = 1.4328 (1.297 sec/step)\n",
            "I0919 16:44:15.075768 140474388072320 learning.py:507] global step 604: loss = 1.6383 (1.292 sec/step)\n",
            "I0919 16:44:16.376717 140474388072320 learning.py:507] global step 605: loss = 1.4584 (1.299 sec/step)\n",
            "I0919 16:44:17.684485 140474388072320 learning.py:507] global step 606: loss = 1.5153 (1.306 sec/step)\n",
            "I0919 16:44:19.006892 140474388072320 learning.py:507] global step 607: loss = 1.4500 (1.321 sec/step)\n",
            "I0919 16:44:20.284657 140474388072320 learning.py:507] global step 608: loss = 2.1095 (1.276 sec/step)\n",
            "I0919 16:44:21.591801 140474388072320 learning.py:507] global step 609: loss = 1.2449 (1.305 sec/step)\n",
            "I0919 16:44:22.909944 140474388072320 learning.py:507] global step 610: loss = 1.9003 (1.317 sec/step)\n",
            "I0919 16:44:24.230495 140474388072320 learning.py:507] global step 611: loss = 1.4708 (1.319 sec/step)\n",
            "I0919 16:44:25.507256 140474388072320 learning.py:507] global step 612: loss = 1.4094 (1.275 sec/step)\n",
            "I0919 16:44:26.794193 140474388072320 learning.py:507] global step 613: loss = 1.3856 (1.285 sec/step)\n",
            "I0919 16:44:26.905689 140471305729792 supervisor.py:1099] global_step/sec: 0.758124\n",
            "I0919 16:44:28.747956 140474388072320 learning.py:507] global step 614: loss = 1.1374 (1.952 sec/step)\n",
            "I0919 16:44:29.175064 140471297337088 supervisor.py:1050] Recording summary at step 614.\n",
            "I0919 16:44:30.196557 140474388072320 learning.py:507] global step 615: loss = 2.0044 (1.447 sec/step)\n",
            "I0919 16:44:31.484008 140474388072320 learning.py:507] global step 616: loss = 1.4384 (1.286 sec/step)\n",
            "I0919 16:44:32.811182 140474388072320 learning.py:507] global step 617: loss = 1.5360 (1.325 sec/step)\n",
            "I0919 16:44:34.132000 140474388072320 learning.py:507] global step 618: loss = 1.4465 (1.319 sec/step)\n",
            "I0919 16:44:35.443135 140474388072320 learning.py:507] global step 619: loss = 1.3518 (1.310 sec/step)\n",
            "I0919 16:44:36.764538 140474388072320 learning.py:507] global step 620: loss = 1.4400 (1.320 sec/step)\n",
            "I0919 16:44:38.099005 140474388072320 learning.py:507] global step 621: loss = 1.2657 (1.333 sec/step)\n",
            "I0919 16:44:39.409774 140474388072320 learning.py:507] global step 622: loss = 1.4402 (1.308 sec/step)\n",
            "I0919 16:44:40.708628 140474388072320 learning.py:507] global step 623: loss = 1.3739 (1.297 sec/step)\n",
            "I0919 16:44:42.002619 140474388072320 learning.py:507] global step 624: loss = 1.3288 (1.292 sec/step)\n",
            "I0919 16:44:43.309341 140474388072320 learning.py:507] global step 625: loss = 1.3249 (1.305 sec/step)\n",
            "I0919 16:44:44.616352 140474388072320 learning.py:507] global step 626: loss = 1.6796 (1.305 sec/step)\n",
            "I0919 16:44:45.926055 140474388072320 learning.py:507] global step 627: loss = 1.4686 (1.308 sec/step)\n",
            "I0919 16:44:47.236690 140474388072320 learning.py:507] global step 628: loss = 1.3669 (1.309 sec/step)\n",
            "I0919 16:44:48.576453 140474388072320 learning.py:507] global step 629: loss = 1.2924 (1.338 sec/step)\n",
            "I0919 16:44:49.909315 140474388072320 learning.py:507] global step 630: loss = 1.7455 (1.331 sec/step)\n",
            "I0919 16:44:51.234056 140474388072320 learning.py:507] global step 631: loss = 1.7989 (1.323 sec/step)\n",
            "I0919 16:44:52.542922 140474388072320 learning.py:507] global step 632: loss = 1.4819 (1.307 sec/step)\n",
            "I0919 16:44:53.843414 140474388072320 learning.py:507] global step 633: loss = 1.2498 (1.299 sec/step)\n",
            "I0919 16:44:55.128592 140474388072320 learning.py:507] global step 634: loss = 1.1138 (1.283 sec/step)\n",
            "I0919 16:44:56.408190 140474388072320 learning.py:507] global step 635: loss = 1.2727 (1.278 sec/step)\n",
            "I0919 16:44:57.700588 140474388072320 learning.py:507] global step 636: loss = 1.1766 (1.291 sec/step)\n",
            "I0919 16:44:58.993267 140474388072320 learning.py:507] global step 637: loss = 1.2925 (1.291 sec/step)\n",
            "I0919 16:45:00.292305 140474388072320 learning.py:507] global step 638: loss = 1.5136 (1.297 sec/step)\n",
            "I0919 16:45:01.609848 140474388072320 learning.py:507] global step 639: loss = 1.2861 (1.316 sec/step)\n",
            "I0919 16:45:02.982996 140474388072320 learning.py:507] global step 640: loss = 1.5927 (1.371 sec/step)\n",
            "I0919 16:45:04.354507 140474388072320 learning.py:507] global step 641: loss = 1.5035 (1.370 sec/step)\n",
            "I0919 16:45:05.647854 140474388072320 learning.py:507] global step 642: loss = 1.6685 (1.292 sec/step)\n",
            "I0919 16:45:06.994698 140474388072320 learning.py:507] global step 643: loss = 1.3906 (1.345 sec/step)\n",
            "I0919 16:45:08.290240 140474388072320 learning.py:507] global step 644: loss = 1.3674 (1.294 sec/step)\n",
            "I0919 16:45:09.616835 140474388072320 learning.py:507] global step 645: loss = 1.6195 (1.325 sec/step)\n",
            "I0919 16:45:10.921097 140474388072320 learning.py:507] global step 646: loss = 1.3459 (1.302 sec/step)\n",
            "I0919 16:45:12.210004 140474388072320 learning.py:507] global step 647: loss = 1.4071 (1.287 sec/step)\n",
            "I0919 16:45:13.536953 140474388072320 learning.py:507] global step 648: loss = 1.4862 (1.325 sec/step)\n",
            "I0919 16:45:14.864640 140474388072320 learning.py:507] global step 649: loss = 1.4576 (1.326 sec/step)\n",
            "I0919 16:45:16.183212 140474388072320 learning.py:507] global step 650: loss = 1.5120 (1.317 sec/step)\n",
            "I0919 16:45:17.531662 140474388072320 learning.py:507] global step 651: loss = 1.5458 (1.347 sec/step)\n",
            "I0919 16:45:18.818018 140474388072320 learning.py:507] global step 652: loss = 1.3759 (1.284 sec/step)\n",
            "I0919 16:45:20.158405 140474388072320 learning.py:507] global step 653: loss = 1.6799 (1.339 sec/step)\n",
            "I0919 16:45:21.520354 140474388072320 learning.py:507] global step 654: loss = 1.4684 (1.360 sec/step)\n",
            "I0919 16:45:22.796711 140474388072320 learning.py:507] global step 655: loss = 1.5430 (1.274 sec/step)\n",
            "I0919 16:45:24.124593 140474388072320 learning.py:507] global step 656: loss = 1.2577 (1.326 sec/step)\n",
            "I0919 16:45:25.410653 140474388072320 learning.py:507] global step 657: loss = 1.3095 (1.284 sec/step)\n",
            "I0919 16:45:26.726140 140474388072320 learning.py:507] global step 658: loss = 1.4916 (1.314 sec/step)\n",
            "I0919 16:45:28.059768 140474388072320 learning.py:507] global step 659: loss = 1.3557 (1.332 sec/step)\n",
            "I0919 16:45:29.413337 140474388072320 learning.py:507] global step 660: loss = 1.5404 (1.352 sec/step)\n",
            "I0919 16:45:30.711507 140474388072320 learning.py:507] global step 661: loss = 1.2499 (1.296 sec/step)\n",
            "I0919 16:45:32.040544 140474388072320 learning.py:507] global step 662: loss = 1.1464 (1.327 sec/step)\n",
            "I0919 16:45:33.353598 140474388072320 learning.py:507] global step 663: loss = 1.4249 (1.311 sec/step)\n",
            "I0919 16:45:34.646652 140474388072320 learning.py:507] global step 664: loss = 1.4640 (1.291 sec/step)\n",
            "I0919 16:45:35.971333 140474388072320 learning.py:507] global step 665: loss = 1.2083 (1.323 sec/step)\n",
            "I0919 16:45:37.286144 140474388072320 learning.py:507] global step 666: loss = 1.3742 (1.313 sec/step)\n",
            "I0919 16:45:38.583071 140474388072320 learning.py:507] global step 667: loss = 1.7800 (1.295 sec/step)\n",
            "I0919 16:45:39.878001 140474388072320 learning.py:507] global step 668: loss = 1.4711 (1.293 sec/step)\n",
            "I0919 16:45:41.217077 140474388072320 learning.py:507] global step 669: loss = 1.8860 (1.337 sec/step)\n",
            "I0919 16:45:42.529033 140474388072320 learning.py:507] global step 670: loss = 1.3410 (1.310 sec/step)\n",
            "I0919 16:45:43.861066 140474388072320 learning.py:507] global step 671: loss = 1.5127 (1.330 sec/step)\n",
            "I0919 16:45:45.168916 140474388072320 learning.py:507] global step 672: loss = 1.2333 (1.306 sec/step)\n",
            "I0919 16:45:46.440421 140474388072320 learning.py:507] global step 673: loss = 1.4600 (1.270 sec/step)\n",
            "I0919 16:45:47.777401 140474388072320 learning.py:507] global step 674: loss = 1.3036 (1.335 sec/step)\n",
            "I0919 16:45:49.081938 140474388072320 learning.py:507] global step 675: loss = 1.3435 (1.303 sec/step)\n",
            "I0919 16:45:50.412024 140474388072320 learning.py:507] global step 676: loss = 1.5565 (1.328 sec/step)\n",
            "I0919 16:45:51.712275 140474388072320 learning.py:507] global step 677: loss = 1.0032 (1.298 sec/step)\n",
            "I0919 16:45:53.053714 140474388072320 learning.py:507] global step 678: loss = 1.5747 (1.339 sec/step)\n",
            "I0919 16:45:54.390151 140474388072320 learning.py:507] global step 679: loss = 1.2605 (1.335 sec/step)\n",
            "I0919 16:45:55.742252 140474388072320 learning.py:507] global step 680: loss = 1.6775 (1.350 sec/step)\n",
            "I0919 16:45:57.097816 140474388072320 learning.py:507] global step 681: loss = 1.4400 (1.354 sec/step)\n",
            "I0919 16:45:58.412384 140474388072320 learning.py:507] global step 682: loss = 1.4613 (1.313 sec/step)\n",
            "I0919 16:45:59.729434 140474388072320 learning.py:507] global step 683: loss = 1.3184 (1.315 sec/step)\n",
            "I0919 16:46:01.033596 140474388072320 learning.py:507] global step 684: loss = 1.2596 (1.303 sec/step)\n",
            "I0919 16:46:02.351753 140474388072320 learning.py:507] global step 685: loss = 1.2492 (1.316 sec/step)\n",
            "I0919 16:46:03.660568 140474388072320 learning.py:507] global step 686: loss = 1.3834 (1.307 sec/step)\n",
            "I0919 16:46:05.003525 140474388072320 learning.py:507] global step 687: loss = 1.2652 (1.341 sec/step)\n",
            "I0919 16:46:06.274471 140474388072320 learning.py:507] global step 688: loss = 1.7097 (1.269 sec/step)\n",
            "I0919 16:46:07.580441 140474388072320 learning.py:507] global step 689: loss = 1.2982 (1.304 sec/step)\n",
            "I0919 16:46:08.856168 140474388072320 learning.py:507] global step 690: loss = 1.5726 (1.274 sec/step)\n",
            "I0919 16:46:10.183614 140474388072320 learning.py:507] global step 691: loss = 1.4678 (1.326 sec/step)\n",
            "I0919 16:46:11.501080 140474388072320 learning.py:507] global step 692: loss = 1.1882 (1.316 sec/step)\n",
            "I0919 16:46:12.784854 140474388072320 learning.py:507] global step 693: loss = 1.6604 (1.282 sec/step)\n",
            "I0919 16:46:14.063344 140474388072320 learning.py:507] global step 694: loss = 1.3785 (1.277 sec/step)\n",
            "I0919 16:46:15.351104 140474388072320 learning.py:507] global step 695: loss = 1.3839 (1.286 sec/step)\n",
            "I0919 16:46:16.675317 140474388072320 learning.py:507] global step 696: loss = 1.6153 (1.322 sec/step)\n",
            "I0919 16:46:18.000758 140474388072320 learning.py:507] global step 697: loss = 1.2843 (1.324 sec/step)\n",
            "I0919 16:46:19.312802 140474388072320 learning.py:507] global step 698: loss = 1.2012 (1.310 sec/step)\n",
            "I0919 16:46:20.643923 140474388072320 learning.py:507] global step 699: loss = 1.4396 (1.329 sec/step)\n",
            "I0919 16:46:21.963448 140474388072320 learning.py:507] global step 700: loss = 1.3161 (1.318 sec/step)\n",
            "I0919 16:46:23.295546 140474388072320 learning.py:507] global step 701: loss = 1.7234 (1.330 sec/step)\n",
            "I0919 16:46:24.618191 140474388072320 learning.py:507] global step 702: loss = 1.3055 (1.321 sec/step)\n",
            "I0919 16:46:25.911989 140474388072320 learning.py:507] global step 703: loss = 1.3120 (1.292 sec/step)\n",
            "I0919 16:46:27.241555 140471305729792 supervisor.py:1099] global_step/sec: 0.747906\n",
            "I0919 16:46:27.507026 140474388072320 learning.py:507] global step 704: loss = 1.5096 (1.427 sec/step)\n",
            "I0919 16:46:28.440179 140471297337088 supervisor.py:1050] Recording summary at step 704.\n",
            "I0919 16:46:29.453418 140474388072320 learning.py:507] global step 705: loss = 1.2425 (1.931 sec/step)\n",
            "I0919 16:46:30.796025 140474388072320 learning.py:507] global step 706: loss = 1.2506 (1.341 sec/step)\n",
            "I0919 16:46:32.092036 140474388072320 learning.py:507] global step 707: loss = 1.6129 (1.294 sec/step)\n",
            "I0919 16:46:33.431197 140474388072320 learning.py:507] global step 708: loss = 1.2462 (1.337 sec/step)\n",
            "I0919 16:46:34.723429 140474388072320 learning.py:507] global step 709: loss = 1.3595 (1.290 sec/step)\n",
            "I0919 16:46:36.057641 140474388072320 learning.py:507] global step 710: loss = 1.2062 (1.332 sec/step)\n",
            "I0919 16:46:37.363703 140474388072320 learning.py:507] global step 711: loss = 1.4453 (1.304 sec/step)\n",
            "I0919 16:46:38.662199 140474388072320 learning.py:507] global step 712: loss = 1.5750 (1.297 sec/step)\n",
            "I0919 16:46:39.970290 140474388072320 learning.py:507] global step 713: loss = 1.4677 (1.307 sec/step)\n",
            "I0919 16:46:41.268444 140474388072320 learning.py:507] global step 714: loss = 1.3312 (1.297 sec/step)\n",
            "I0919 16:46:42.566687 140474388072320 learning.py:507] global step 715: loss = 1.2554 (1.297 sec/step)\n",
            "I0919 16:46:43.899688 140474388072320 learning.py:507] global step 716: loss = 1.3071 (1.331 sec/step)\n",
            "I0919 16:46:45.213603 140474388072320 learning.py:507] global step 717: loss = 1.3269 (1.312 sec/step)\n",
            "I0919 16:46:46.527281 140474388072320 learning.py:507] global step 718: loss = 1.6208 (1.312 sec/step)\n",
            "I0919 16:46:47.836405 140474388072320 learning.py:507] global step 719: loss = 1.9705 (1.307 sec/step)\n",
            "I0919 16:46:49.136652 140474388072320 learning.py:507] global step 720: loss = 1.2619 (1.298 sec/step)\n",
            "I0919 16:46:50.486866 140474388072320 learning.py:507] global step 721: loss = 1.5247 (1.349 sec/step)\n",
            "I0919 16:46:51.800987 140474388072320 learning.py:507] global step 722: loss = 1.5498 (1.312 sec/step)\n",
            "I0919 16:46:53.115176 140474388072320 learning.py:507] global step 723: loss = 1.2381 (1.312 sec/step)\n",
            "I0919 16:46:54.407932 140474388072320 learning.py:507] global step 724: loss = 1.3549 (1.291 sec/step)\n",
            "I0919 16:46:55.712856 140474388072320 learning.py:507] global step 725: loss = 2.0130 (1.303 sec/step)\n",
            "I0919 16:46:57.033225 140474388072320 learning.py:507] global step 726: loss = 1.6060 (1.318 sec/step)\n",
            "I0919 16:46:58.353266 140474388072320 learning.py:507] global step 727: loss = 1.5668 (1.318 sec/step)\n",
            "I0919 16:46:59.712325 140474388072320 learning.py:507] global step 728: loss = 1.2536 (1.357 sec/step)\n",
            "I0919 16:47:01.064616 140474388072320 learning.py:507] global step 729: loss = 1.3119 (1.350 sec/step)\n",
            "I0919 16:47:02.372500 140474388072320 learning.py:507] global step 730: loss = 1.4983 (1.306 sec/step)\n",
            "I0919 16:47:03.687510 140474388072320 learning.py:507] global step 731: loss = 1.1632 (1.313 sec/step)\n",
            "I0919 16:47:04.979346 140474388072320 learning.py:507] global step 732: loss = 1.3549 (1.290 sec/step)\n",
            "I0919 16:47:06.261128 140474388072320 learning.py:507] global step 733: loss = 1.4285 (1.280 sec/step)\n",
            "I0919 16:47:07.568268 140474388072320 learning.py:507] global step 734: loss = 1.4618 (1.305 sec/step)\n",
            "I0919 16:47:08.891623 140474388072320 learning.py:507] global step 735: loss = 1.3948 (1.322 sec/step)\n",
            "I0919 16:47:10.222064 140474388072320 learning.py:507] global step 736: loss = 1.2499 (1.328 sec/step)\n",
            "I0919 16:47:11.581079 140474388072320 learning.py:507] global step 737: loss = 1.1018 (1.357 sec/step)\n",
            "I0919 16:47:12.845501 140474388072320 learning.py:507] global step 738: loss = 1.5168 (1.263 sec/step)\n",
            "I0919 16:47:14.133553 140474388072320 learning.py:507] global step 739: loss = 1.4038 (1.286 sec/step)\n",
            "I0919 16:47:15.446602 140474388072320 learning.py:507] global step 740: loss = 1.4602 (1.311 sec/step)\n",
            "I0919 16:47:16.772187 140474388072320 learning.py:507] global step 741: loss = 1.5556 (1.324 sec/step)\n",
            "I0919 16:47:18.085900 140474388072320 learning.py:507] global step 742: loss = 1.4050 (1.312 sec/step)\n",
            "I0919 16:47:19.397491 140474388072320 learning.py:507] global step 743: loss = 1.4975 (1.310 sec/step)\n",
            "I0919 16:47:20.681607 140474388072320 learning.py:507] global step 744: loss = 1.1307 (1.282 sec/step)\n",
            "I0919 16:47:21.988503 140474388072320 learning.py:507] global step 745: loss = 1.4061 (1.305 sec/step)\n",
            "I0919 16:47:23.302721 140474388072320 learning.py:507] global step 746: loss = 1.3863 (1.312 sec/step)\n",
            "I0919 16:47:24.657644 140474388072320 learning.py:507] global step 747: loss = 1.2082 (1.353 sec/step)\n",
            "I0919 16:47:25.975586 140474388072320 learning.py:507] global step 748: loss = 1.2112 (1.316 sec/step)\n",
            "I0919 16:47:27.286926 140474388072320 learning.py:507] global step 749: loss = 1.2546 (1.309 sec/step)\n",
            "I0919 16:47:28.586163 140474388072320 learning.py:507] global step 750: loss = 1.3225 (1.298 sec/step)\n",
            "I0919 16:47:29.891078 140474388072320 learning.py:507] global step 751: loss = 1.2915 (1.303 sec/step)\n",
            "I0919 16:47:31.246655 140474388072320 learning.py:507] global step 752: loss = 1.2478 (1.354 sec/step)\n",
            "I0919 16:47:32.532977 140474388072320 learning.py:507] global step 753: loss = 1.5773 (1.285 sec/step)\n",
            "I0919 16:47:33.828466 140474388072320 learning.py:507] global step 754: loss = 1.3170 (1.294 sec/step)\n",
            "I0919 16:47:35.133507 140474388072320 learning.py:507] global step 755: loss = 1.2517 (1.303 sec/step)\n",
            "I0919 16:47:36.446571 140474388072320 learning.py:507] global step 756: loss = 1.4393 (1.311 sec/step)\n",
            "I0919 16:47:37.753807 140474388072320 learning.py:507] global step 757: loss = 1.1908 (1.305 sec/step)\n",
            "I0919 16:47:39.060043 140474388072320 learning.py:507] global step 758: loss = 1.4319 (1.304 sec/step)\n",
            "I0919 16:47:40.368174 140474388072320 learning.py:507] global step 759: loss = 1.1779 (1.306 sec/step)\n",
            "I0919 16:47:41.687016 140474388072320 learning.py:507] global step 760: loss = 1.3736 (1.317 sec/step)\n",
            "I0919 16:47:42.988215 140474388072320 learning.py:507] global step 761: loss = 1.3851 (1.299 sec/step)\n",
            "I0919 16:47:44.266670 140474388072320 learning.py:507] global step 762: loss = 1.4011 (1.277 sec/step)\n",
            "I0919 16:47:45.564166 140474388072320 learning.py:507] global step 763: loss = 1.4116 (1.296 sec/step)\n",
            "I0919 16:47:46.857908 140474388072320 learning.py:507] global step 764: loss = 1.5783 (1.292 sec/step)\n",
            "I0919 16:47:48.166424 140474388072320 learning.py:507] global step 765: loss = 1.1274 (1.307 sec/step)\n",
            "I0919 16:47:49.492351 140474388072320 learning.py:507] global step 766: loss = 1.1764 (1.324 sec/step)\n",
            "I0919 16:47:50.798053 140474388072320 learning.py:507] global step 767: loss = 1.3724 (1.304 sec/step)\n",
            "I0919 16:47:52.145590 140474388072320 learning.py:507] global step 768: loss = 1.4264 (1.346 sec/step)\n",
            "I0919 16:47:53.459304 140474388072320 learning.py:507] global step 769: loss = 1.5251 (1.312 sec/step)\n",
            "I0919 16:47:54.751837 140474388072320 learning.py:507] global step 770: loss = 1.3510 (1.291 sec/step)\n",
            "I0919 16:47:56.070431 140474388072320 learning.py:507] global step 771: loss = 1.2477 (1.317 sec/step)\n",
            "I0919 16:47:57.415040 140474388072320 learning.py:507] global step 772: loss = 1.3651 (1.343 sec/step)\n",
            "I0919 16:47:58.738226 140474388072320 learning.py:507] global step 773: loss = 1.2316 (1.321 sec/step)\n",
            "I0919 16:48:00.047337 140474388072320 learning.py:507] global step 774: loss = 1.2784 (1.308 sec/step)\n",
            "I0919 16:48:01.331670 140474388072320 learning.py:507] global step 775: loss = 1.2090 (1.283 sec/step)\n",
            "I0919 16:48:02.679303 140474388072320 learning.py:507] global step 776: loss = 1.1195 (1.346 sec/step)\n",
            "I0919 16:48:03.961272 140474388072320 learning.py:507] global step 777: loss = 1.1258 (1.280 sec/step)\n",
            "I0919 16:48:05.263044 140474388072320 learning.py:507] global step 778: loss = 1.4748 (1.300 sec/step)\n",
            "I0919 16:48:06.630914 140474388072320 learning.py:507] global step 779: loss = 1.2799 (1.366 sec/step)\n",
            "I0919 16:48:07.957875 140474388072320 learning.py:507] global step 780: loss = 1.1273 (1.325 sec/step)\n",
            "I0919 16:48:09.295347 140474388072320 learning.py:507] global step 781: loss = 1.5915 (1.336 sec/step)\n",
            "I0919 16:48:10.597228 140474388072320 learning.py:507] global step 782: loss = 1.0122 (1.300 sec/step)\n",
            "I0919 16:48:11.919409 140474388072320 learning.py:507] global step 783: loss = 1.3573 (1.320 sec/step)\n",
            "I0919 16:48:13.276935 140474388072320 learning.py:507] global step 784: loss = 1.3873 (1.356 sec/step)\n",
            "I0919 16:48:14.631577 140474388072320 learning.py:507] global step 785: loss = 1.1855 (1.353 sec/step)\n",
            "I0919 16:48:15.930661 140474388072320 learning.py:507] global step 786: loss = 1.4486 (1.297 sec/step)\n",
            "I0919 16:48:17.213495 140474388072320 learning.py:507] global step 787: loss = 1.3356 (1.280 sec/step)\n",
            "I0919 16:48:18.491833 140474388072320 learning.py:507] global step 788: loss = 1.3167 (1.275 sec/step)\n",
            "I0919 16:48:19.831459 140474388072320 learning.py:507] global step 789: loss = 1.4889 (1.338 sec/step)\n",
            "I0919 16:48:21.162334 140474388072320 learning.py:507] global step 790: loss = 1.2911 (1.329 sec/step)\n",
            "I0919 16:48:22.476984 140474388072320 learning.py:507] global step 791: loss = 1.4874 (1.313 sec/step)\n",
            "I0919 16:48:23.770281 140474388072320 learning.py:507] global step 792: loss = 1.7585 (1.292 sec/step)\n",
            "I0919 16:48:25.094006 140474388072320 learning.py:507] global step 793: loss = 1.2266 (1.322 sec/step)\n",
            "I0919 16:48:26.379815 140474388072320 learning.py:507] global step 794: loss = 1.2624 (1.284 sec/step)\n",
            "I0919 16:48:27.062670 140471305729792 supervisor.py:1099] global_step/sec: 0.759467\n",
            "I0919 16:48:28.197793 140474388072320 learning.py:507] global step 795: loss = 1.1729 (1.816 sec/step)\n",
            "I0919 16:48:28.724132 140471297337088 supervisor.py:1050] Recording summary at step 795.\n",
            "I0919 16:48:29.847142 140474388072320 learning.py:507] global step 796: loss = 1.2516 (1.553 sec/step)\n",
            "I0919 16:48:31.225317 140474388072320 learning.py:507] global step 797: loss = 1.4095 (1.376 sec/step)\n",
            "I0919 16:48:32.527803 140474388072320 learning.py:507] global step 798: loss = 1.4316 (1.301 sec/step)\n",
            "I0919 16:48:33.839425 140474388072320 learning.py:507] global step 799: loss = 1.2632 (1.310 sec/step)\n",
            "I0919 16:48:35.145517 140474388072320 learning.py:507] global step 800: loss = 1.4697 (1.304 sec/step)\n",
            "I0919 16:48:36.431940 140474388072320 learning.py:507] global step 801: loss = 1.2961 (1.285 sec/step)\n",
            "I0919 16:48:37.733747 140474388072320 learning.py:507] global step 802: loss = 1.3558 (1.300 sec/step)\n",
            "I0919 16:48:39.035139 140474388072320 learning.py:507] global step 803: loss = 1.5035 (1.300 sec/step)\n",
            "I0919 16:48:40.359066 140474388072320 learning.py:507] global step 804: loss = 1.4142 (1.322 sec/step)\n",
            "I0919 16:48:41.675343 140474388072320 learning.py:507] global step 805: loss = 1.1373 (1.314 sec/step)\n",
            "I0919 16:48:42.973628 140474388072320 learning.py:507] global step 806: loss = 1.4606 (1.296 sec/step)\n",
            "I0919 16:48:44.311248 140474388072320 learning.py:507] global step 807: loss = 1.2996 (1.336 sec/step)\n",
            "I0919 16:48:45.653223 140474388072320 learning.py:507] global step 808: loss = 1.5263 (1.340 sec/step)\n",
            "I0919 16:48:46.988449 140474388072320 learning.py:507] global step 809: loss = 1.3478 (1.333 sec/step)\n",
            "I0919 16:48:48.324048 140474388072320 learning.py:507] global step 810: loss = 1.2058 (1.334 sec/step)\n",
            "I0919 16:48:49.624808 140474388072320 learning.py:507] global step 811: loss = 1.6438 (1.299 sec/step)\n",
            "I0919 16:48:50.947072 140474388072320 learning.py:507] global step 812: loss = 1.3035 (1.320 sec/step)\n",
            "I0919 16:48:52.241508 140474388072320 learning.py:507] global step 813: loss = 1.2609 (1.293 sec/step)\n",
            "I0919 16:48:53.525950 140474388072320 learning.py:507] global step 814: loss = 1.3653 (1.283 sec/step)\n",
            "I0919 16:48:54.856916 140474388072320 learning.py:507] global step 815: loss = 1.3445 (1.329 sec/step)\n",
            "I0919 16:48:56.144217 140474388072320 learning.py:507] global step 816: loss = 1.4389 (1.285 sec/step)\n",
            "I0919 16:48:57.446316 140474388072320 learning.py:507] global step 817: loss = 1.1691 (1.300 sec/step)\n",
            "I0919 16:48:58.759551 140474388072320 learning.py:507] global step 818: loss = 1.0734 (1.312 sec/step)\n",
            "I0919 16:49:00.042282 140474388072320 learning.py:507] global step 819: loss = 1.1161 (1.281 sec/step)\n",
            "I0919 16:49:01.345199 140474388072320 learning.py:507] global step 820: loss = 1.2734 (1.301 sec/step)\n",
            "I0919 16:49:02.647994 140474388072320 learning.py:507] global step 821: loss = 1.3252 (1.301 sec/step)\n",
            "I0919 16:49:03.951241 140474388072320 learning.py:507] global step 822: loss = 1.4717 (1.301 sec/step)\n",
            "I0919 16:49:05.260063 140474388072320 learning.py:507] global step 823: loss = 1.6739 (1.307 sec/step)\n",
            "I0919 16:49:06.558582 140474388072320 learning.py:507] global step 824: loss = 1.2996 (1.297 sec/step)\n",
            "I0919 16:49:07.873594 140474388072320 learning.py:507] global step 825: loss = 1.0177 (1.313 sec/step)\n",
            "I0919 16:49:09.193430 140474388072320 learning.py:507] global step 826: loss = 1.4210 (1.318 sec/step)\n",
            "I0919 16:49:10.495747 140474388072320 learning.py:507] global step 827: loss = 1.3224 (1.301 sec/step)\n",
            "I0919 16:49:11.817435 140474388072320 learning.py:507] global step 828: loss = 1.1460 (1.320 sec/step)\n",
            "I0919 16:49:13.171319 140474388072320 learning.py:507] global step 829: loss = 1.2383 (1.352 sec/step)\n",
            "I0919 16:49:14.492151 140474388072320 learning.py:507] global step 830: loss = 1.3166 (1.319 sec/step)\n",
            "I0919 16:49:15.759561 140474388072320 learning.py:507] global step 831: loss = 1.2261 (1.266 sec/step)\n",
            "I0919 16:49:17.057660 140474388072320 learning.py:507] global step 832: loss = 1.5064 (1.297 sec/step)\n",
            "I0919 16:49:18.407447 140474388072320 learning.py:507] global step 833: loss = 1.6315 (1.348 sec/step)\n",
            "I0919 16:49:19.703480 140474388072320 learning.py:507] global step 834: loss = 1.2906 (1.294 sec/step)\n",
            "I0919 16:49:21.002806 140474388072320 learning.py:507] global step 835: loss = 1.2126 (1.297 sec/step)\n",
            "I0919 16:49:22.292701 140474388072320 learning.py:507] global step 836: loss = 1.4879 (1.288 sec/step)\n",
            "I0919 16:49:23.588314 140474388072320 learning.py:507] global step 837: loss = 1.4959 (1.294 sec/step)\n",
            "I0919 16:49:24.955774 140474388072320 learning.py:507] global step 838: loss = 1.1478 (1.366 sec/step)\n",
            "I0919 16:49:26.284061 140474388072320 learning.py:507] global step 839: loss = 1.1440 (1.327 sec/step)\n",
            "I0919 16:49:27.601941 140474388072320 learning.py:507] global step 840: loss = 1.5502 (1.316 sec/step)\n",
            "I0919 16:49:28.906501 140474388072320 learning.py:507] global step 841: loss = 1.2953 (1.303 sec/step)\n",
            "I0919 16:49:30.201253 140474388072320 learning.py:507] global step 842: loss = 1.2607 (1.293 sec/step)\n",
            "I0919 16:49:31.518966 140474388072320 learning.py:507] global step 843: loss = 1.3741 (1.316 sec/step)\n",
            "I0919 16:49:32.797703 140474388072320 learning.py:507] global step 844: loss = 1.3340 (1.277 sec/step)\n",
            "I0919 16:49:34.138751 140474388072320 learning.py:507] global step 845: loss = 1.2551 (1.339 sec/step)\n",
            "I0919 16:49:35.435480 140474388072320 learning.py:507] global step 846: loss = 1.2899 (1.295 sec/step)\n",
            "I0919 16:49:36.723839 140474388072320 learning.py:507] global step 847: loss = 1.6709 (1.287 sec/step)\n",
            "I0919 16:49:38.016917 140474388072320 learning.py:507] global step 848: loss = 1.4062 (1.291 sec/step)\n",
            "I0919 16:49:39.319564 140474388072320 learning.py:507] global step 849: loss = 1.3733 (1.301 sec/step)\n",
            "I0919 16:49:40.614137 140474388072320 learning.py:507] global step 850: loss = 1.4417 (1.293 sec/step)\n",
            "I0919 16:49:41.923036 140474388072320 learning.py:507] global step 851: loss = 1.0599 (1.307 sec/step)\n",
            "I0919 16:49:43.281485 140474388072320 learning.py:507] global step 852: loss = 1.1611 (1.357 sec/step)\n",
            "I0919 16:49:44.606791 140474388072320 learning.py:507] global step 853: loss = 1.2475 (1.323 sec/step)\n",
            "I0919 16:49:45.920423 140474388072320 learning.py:507] global step 854: loss = 1.5813 (1.311 sec/step)\n",
            "I0919 16:49:47.218725 140474388072320 learning.py:507] global step 855: loss = 1.4029 (1.297 sec/step)\n",
            "I0919 16:49:48.550477 140474388072320 learning.py:507] global step 856: loss = 1.3211 (1.330 sec/step)\n",
            "I0919 16:49:49.856489 140474388072320 learning.py:507] global step 857: loss = 1.2251 (1.304 sec/step)\n",
            "I0919 16:49:51.172554 140474388072320 learning.py:507] global step 858: loss = 1.1708 (1.314 sec/step)\n",
            "I0919 16:49:52.500785 140474388072320 learning.py:507] global step 859: loss = 1.3141 (1.326 sec/step)\n",
            "I0919 16:49:53.792388 140474388072320 learning.py:507] global step 860: loss = 1.4049 (1.290 sec/step)\n",
            "I0919 16:49:55.095715 140474388072320 learning.py:507] global step 861: loss = 1.2587 (1.302 sec/step)\n",
            "I0919 16:49:56.389081 140474388072320 learning.py:507] global step 862: loss = 1.2589 (1.291 sec/step)\n",
            "I0919 16:49:57.690700 140474388072320 learning.py:507] global step 863: loss = 1.1851 (1.300 sec/step)\n",
            "I0919 16:49:58.999330 140474388072320 learning.py:507] global step 864: loss = 1.1282 (1.306 sec/step)\n",
            "I0919 16:50:00.291873 140474388072320 learning.py:507] global step 865: loss = 1.1370 (1.291 sec/step)\n",
            "I0919 16:50:01.602325 140474388072320 learning.py:507] global step 866: loss = 0.9903 (1.309 sec/step)\n",
            "I0919 16:50:02.893446 140474388072320 learning.py:507] global step 867: loss = 1.2111 (1.289 sec/step)\n",
            "I0919 16:50:04.219794 140474388072320 learning.py:507] global step 868: loss = 1.2515 (1.325 sec/step)\n",
            "I0919 16:50:05.515333 140474388072320 learning.py:507] global step 869: loss = 1.4331 (1.294 sec/step)\n",
            "I0919 16:50:06.849838 140474388072320 learning.py:507] global step 870: loss = 1.0529 (1.333 sec/step)\n",
            "I0919 16:50:08.155354 140474388072320 learning.py:507] global step 871: loss = 1.2467 (1.304 sec/step)\n",
            "I0919 16:50:09.432324 140474388072320 learning.py:507] global step 872: loss = 1.0448 (1.275 sec/step)\n",
            "I0919 16:50:10.785459 140474388072320 learning.py:507] global step 873: loss = 1.1508 (1.351 sec/step)\n",
            "I0919 16:50:12.109220 140474388072320 learning.py:507] global step 874: loss = 1.2856 (1.322 sec/step)\n",
            "I0919 16:50:13.401191 140474388072320 learning.py:507] global step 875: loss = 1.3301 (1.290 sec/step)\n",
            "I0919 16:50:14.692585 140474388072320 learning.py:507] global step 876: loss = 1.0753 (1.290 sec/step)\n",
            "I0919 16:50:16.001724 140474388072320 learning.py:507] global step 877: loss = 1.1999 (1.307 sec/step)\n",
            "I0919 16:50:17.310998 140474388072320 learning.py:507] global step 878: loss = 1.0999 (1.307 sec/step)\n",
            "I0919 16:50:18.629309 140474388072320 learning.py:507] global step 879: loss = 1.0190 (1.316 sec/step)\n",
            "I0919 16:50:19.909641 140474388072320 learning.py:507] global step 880: loss = 1.4120 (1.279 sec/step)\n",
            "I0919 16:50:21.233052 140474388072320 learning.py:507] global step 881: loss = 1.2626 (1.322 sec/step)\n",
            "I0919 16:50:22.540172 140474388072320 learning.py:507] global step 882: loss = 1.2230 (1.305 sec/step)\n",
            "I0919 16:50:23.835263 140474388072320 learning.py:507] global step 883: loss = 1.4241 (1.293 sec/step)\n",
            "I0919 16:50:25.146342 140474388072320 learning.py:507] global step 884: loss = 1.2313 (1.309 sec/step)\n",
            "I0919 16:50:26.476361 140474388072320 learning.py:507] global step 885: loss = 1.4698 (1.328 sec/step)\n",
            "I0919 16:50:26.873009 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 16:50:28.114687 140474388072320 learning.py:507] global step 886: loss = 1.4492 (1.621 sec/step)\n",
            "I0919 16:50:30.035533 140471297337088 supervisor.py:1050] Recording summary at step 886.\n",
            "I0919 16:50:30.739187 140474388072320 learning.py:507] global step 887: loss = 1.2427 (2.612 sec/step)\n",
            "I0919 16:50:32.625643 140474388072320 learning.py:507] global step 888: loss = 1.4626 (1.857 sec/step)\n",
            "I0919 16:50:34.027862 140474388072320 learning.py:507] global step 889: loss = 1.3222 (1.390 sec/step)\n",
            "I0919 16:50:35.302834 140474388072320 learning.py:507] global step 890: loss = 1.4099 (1.273 sec/step)\n",
            "I0919 16:50:36.611289 140474388072320 learning.py:507] global step 891: loss = 1.4087 (1.307 sec/step)\n",
            "I0919 16:50:37.922835 140474388072320 learning.py:507] global step 892: loss = 1.7747 (1.310 sec/step)\n",
            "I0919 16:50:39.284354 140474388072320 learning.py:507] global step 893: loss = 1.2033 (1.359 sec/step)\n",
            "I0919 16:50:40.600016 140474388072320 learning.py:507] global step 894: loss = 1.3496 (1.314 sec/step)\n",
            "I0919 16:50:41.898027 140474388072320 learning.py:507] global step 895: loss = 1.2215 (1.296 sec/step)\n",
            "I0919 16:50:43.188925 140474388072320 learning.py:507] global step 896: loss = 1.1066 (1.289 sec/step)\n",
            "I0919 16:50:44.474004 140474388072320 learning.py:507] global step 897: loss = 1.3576 (1.283 sec/step)\n",
            "I0919 16:50:45.774801 140474388072320 learning.py:507] global step 898: loss = 1.2365 (1.299 sec/step)\n",
            "I0919 16:50:47.086179 140474388072320 learning.py:507] global step 899: loss = 1.4990 (1.310 sec/step)\n",
            "I0919 16:50:48.397419 140474388072320 learning.py:507] global step 900: loss = 1.1858 (1.309 sec/step)\n",
            "I0919 16:50:49.691365 140474388072320 learning.py:507] global step 901: loss = 1.5531 (1.292 sec/step)\n",
            "I0919 16:50:51.015058 140474388072320 learning.py:507] global step 902: loss = 1.8634 (1.322 sec/step)\n",
            "I0919 16:50:52.320861 140474388072320 learning.py:507] global step 903: loss = 1.3833 (1.304 sec/step)\n",
            "I0919 16:50:53.617930 140474388072320 learning.py:507] global step 904: loss = 1.2259 (1.295 sec/step)\n",
            "I0919 16:50:54.909977 140474388072320 learning.py:507] global step 905: loss = 1.2974 (1.290 sec/step)\n",
            "I0919 16:50:56.236226 140474388072320 learning.py:507] global step 906: loss = 1.2492 (1.324 sec/step)\n",
            "I0919 16:50:57.563025 140474388072320 learning.py:507] global step 907: loss = 1.1159 (1.325 sec/step)\n",
            "I0919 16:50:58.905102 140474388072320 learning.py:507] global step 908: loss = 1.5394 (1.340 sec/step)\n",
            "I0919 16:51:00.220099 140474388072320 learning.py:507] global step 909: loss = 1.1347 (1.313 sec/step)\n",
            "I0919 16:51:01.522336 140474388072320 learning.py:507] global step 910: loss = 1.5540 (1.300 sec/step)\n",
            "I0919 16:51:02.851893 140474388072320 learning.py:507] global step 911: loss = 1.2911 (1.327 sec/step)\n",
            "I0919 16:51:04.140074 140474388072320 learning.py:507] global step 912: loss = 1.2621 (1.286 sec/step)\n",
            "I0919 16:51:05.428419 140474388072320 learning.py:507] global step 913: loss = 1.2203 (1.287 sec/step)\n",
            "I0919 16:51:06.749526 140474388072320 learning.py:507] global step 914: loss = 1.1703 (1.318 sec/step)\n",
            "I0919 16:51:08.075922 140474388072320 learning.py:507] global step 915: loss = 1.3188 (1.324 sec/step)\n",
            "I0919 16:51:09.411211 140474388072320 learning.py:507] global step 916: loss = 1.3321 (1.333 sec/step)\n",
            "I0919 16:51:10.695471 140474388072320 learning.py:507] global step 917: loss = 1.1322 (1.282 sec/step)\n",
            "I0919 16:51:11.975497 140474388072320 learning.py:507] global step 918: loss = 1.2490 (1.278 sec/step)\n",
            "I0919 16:51:13.266344 140474388072320 learning.py:507] global step 919: loss = 1.2233 (1.289 sec/step)\n",
            "I0919 16:51:14.574419 140474388072320 learning.py:507] global step 920: loss = 1.2007 (1.306 sec/step)\n",
            "I0919 16:51:15.846549 140474388072320 learning.py:507] global step 921: loss = 1.1796 (1.270 sec/step)\n",
            "I0919 16:51:17.175296 140474388072320 learning.py:507] global step 922: loss = 1.3281 (1.327 sec/step)\n",
            "I0919 16:51:18.492280 140474388072320 learning.py:507] global step 923: loss = 1.3828 (1.315 sec/step)\n",
            "I0919 16:51:19.776165 140474388072320 learning.py:507] global step 924: loss = 1.4100 (1.282 sec/step)\n",
            "I0919 16:51:21.076934 140474388072320 learning.py:507] global step 925: loss = 1.3498 (1.299 sec/step)\n",
            "I0919 16:51:22.374890 140474388072320 learning.py:507] global step 926: loss = 1.2067 (1.296 sec/step)\n",
            "I0919 16:51:23.679826 140474388072320 learning.py:507] global step 927: loss = 1.3303 (1.302 sec/step)\n",
            "I0919 16:51:24.994649 140474388072320 learning.py:507] global step 928: loss = 1.1193 (1.313 sec/step)\n",
            "I0919 16:51:26.290035 140474388072320 learning.py:507] global step 929: loss = 1.1647 (1.294 sec/step)\n",
            "I0919 16:51:27.587372 140474388072320 learning.py:507] global step 930: loss = 1.2015 (1.296 sec/step)\n",
            "I0919 16:51:28.875637 140474388072320 learning.py:507] global step 931: loss = 1.2907 (1.286 sec/step)\n",
            "I0919 16:51:30.190731 140474388072320 learning.py:507] global step 932: loss = 1.3471 (1.313 sec/step)\n",
            "I0919 16:51:31.471136 140474388072320 learning.py:507] global step 933: loss = 1.2518 (1.279 sec/step)\n",
            "I0919 16:51:32.791791 140474388072320 learning.py:507] global step 934: loss = 1.1642 (1.319 sec/step)\n",
            "I0919 16:51:34.112863 140474388072320 learning.py:507] global step 935: loss = 1.3183 (1.319 sec/step)\n",
            "I0919 16:51:35.411243 140474388072320 learning.py:507] global step 936: loss = 1.3992 (1.296 sec/step)\n",
            "I0919 16:51:36.714287 140474388072320 learning.py:507] global step 937: loss = 1.2053 (1.301 sec/step)\n",
            "I0919 16:51:38.045664 140474388072320 learning.py:507] global step 938: loss = 1.3385 (1.329 sec/step)\n",
            "I0919 16:51:39.345421 140474388072320 learning.py:507] global step 939: loss = 1.0782 (1.298 sec/step)\n",
            "I0919 16:51:40.666052 140474388072320 learning.py:507] global step 940: loss = 1.1586 (1.319 sec/step)\n",
            "I0919 16:51:41.967624 140474388072320 learning.py:507] global step 941: loss = 1.5040 (1.300 sec/step)\n",
            "I0919 16:51:43.270978 140474388072320 learning.py:507] global step 942: loss = 1.2727 (1.301 sec/step)\n",
            "I0919 16:51:44.574017 140474388072320 learning.py:507] global step 943: loss = 1.1290 (1.301 sec/step)\n",
            "I0919 16:51:45.932768 140474388072320 learning.py:507] global step 944: loss = 1.1291 (1.357 sec/step)\n",
            "I0919 16:51:47.280067 140474388072320 learning.py:507] global step 945: loss = 1.2976 (1.346 sec/step)\n",
            "I0919 16:51:48.583385 140474388072320 learning.py:507] global step 946: loss = 1.2031 (1.302 sec/step)\n",
            "I0919 16:51:49.897456 140474388072320 learning.py:507] global step 947: loss = 1.0509 (1.312 sec/step)\n",
            "I0919 16:51:51.225836 140474388072320 learning.py:507] global step 948: loss = 1.3080 (1.327 sec/step)\n",
            "I0919 16:51:52.508528 140474388072320 learning.py:507] global step 949: loss = 0.9694 (1.281 sec/step)\n",
            "I0919 16:51:53.881421 140474388072320 learning.py:507] global step 950: loss = 1.3497 (1.371 sec/step)\n",
            "I0919 16:51:55.153381 140474388072320 learning.py:507] global step 951: loss = 1.3199 (1.270 sec/step)\n",
            "I0919 16:51:56.450165 140474388072320 learning.py:507] global step 952: loss = 1.4637 (1.295 sec/step)\n",
            "I0919 16:51:57.771425 140474388072320 learning.py:507] global step 953: loss = 1.2573 (1.319 sec/step)\n",
            "I0919 16:51:59.110534 140474388072320 learning.py:507] global step 954: loss = 1.1466 (1.337 sec/step)\n",
            "I0919 16:52:00.415452 140474388072320 learning.py:507] global step 955: loss = 1.1524 (1.303 sec/step)\n",
            "I0919 16:52:01.683903 140474388072320 learning.py:507] global step 956: loss = 1.2051 (1.267 sec/step)\n",
            "I0919 16:52:02.993082 140474388072320 learning.py:507] global step 957: loss = 1.4410 (1.307 sec/step)\n",
            "I0919 16:52:04.297923 140474388072320 learning.py:507] global step 958: loss = 1.2725 (1.303 sec/step)\n",
            "I0919 16:52:05.624402 140474388072320 learning.py:507] global step 959: loss = 1.3009 (1.325 sec/step)\n",
            "I0919 16:52:06.963234 140474388072320 learning.py:507] global step 960: loss = 1.1438 (1.337 sec/step)\n",
            "I0919 16:52:08.277787 140474388072320 learning.py:507] global step 961: loss = 1.0109 (1.313 sec/step)\n",
            "I0919 16:52:09.579564 140474388072320 learning.py:507] global step 962: loss = 1.2775 (1.300 sec/step)\n",
            "I0919 16:52:10.914547 140474388072320 learning.py:507] global step 963: loss = 0.9920 (1.333 sec/step)\n",
            "I0919 16:52:12.259818 140474388072320 learning.py:507] global step 964: loss = 1.0443 (1.343 sec/step)\n",
            "I0919 16:52:13.590859 140474388072320 learning.py:507] global step 965: loss = 1.0448 (1.329 sec/step)\n",
            "I0919 16:52:14.889003 140474388072320 learning.py:507] global step 966: loss = 1.3079 (1.297 sec/step)\n",
            "I0919 16:52:16.207065 140474388072320 learning.py:507] global step 967: loss = 1.1385 (1.316 sec/step)\n",
            "I0919 16:52:17.524831 140474388072320 learning.py:507] global step 968: loss = 1.2250 (1.316 sec/step)\n",
            "I0919 16:52:18.832139 140474388072320 learning.py:507] global step 969: loss = 1.5530 (1.305 sec/step)\n",
            "I0919 16:52:20.136440 140474388072320 learning.py:507] global step 970: loss = 1.1710 (1.302 sec/step)\n",
            "I0919 16:52:21.457242 140474388072320 learning.py:507] global step 971: loss = 1.1554 (1.319 sec/step)\n",
            "I0919 16:52:22.789721 140474388072320 learning.py:507] global step 972: loss = 1.0081 (1.331 sec/step)\n",
            "I0919 16:52:24.143363 140474388072320 learning.py:507] global step 973: loss = 1.2862 (1.352 sec/step)\n",
            "I0919 16:52:25.429002 140474388072320 learning.py:507] global step 974: loss = 1.2551 (1.284 sec/step)\n",
            "I0919 16:52:26.748756 140474388072320 learning.py:507] global step 975: loss = 1.1742 (1.318 sec/step)\n",
            "I0919 16:52:28.842313 140474388072320 learning.py:507] global step 976: loss = 1.0581 (2.092 sec/step)\n",
            "I0919 16:52:28.861812 140471297337088 supervisor.py:1050] Recording summary at step 976.\n",
            "I0919 16:52:30.162344 140474388072320 learning.py:507] global step 977: loss = 1.0080 (1.316 sec/step)\n",
            "I0919 16:52:31.503494 140474388072320 learning.py:507] global step 978: loss = 1.3326 (1.340 sec/step)\n",
            "I0919 16:52:32.838926 140474388072320 learning.py:507] global step 979: loss = 1.3004 (1.334 sec/step)\n",
            "I0919 16:52:34.134785 140474388072320 learning.py:507] global step 980: loss = 1.2170 (1.294 sec/step)\n",
            "I0919 16:52:35.427743 140474388072320 learning.py:507] global step 981: loss = 1.1899 (1.291 sec/step)\n",
            "I0919 16:52:36.766048 140474388072320 learning.py:507] global step 982: loss = 1.2389 (1.336 sec/step)\n",
            "I0919 16:52:38.071008 140474388072320 learning.py:507] global step 983: loss = 1.2390 (1.303 sec/step)\n",
            "I0919 16:52:39.368386 140474388072320 learning.py:507] global step 984: loss = 1.1284 (1.295 sec/step)\n",
            "I0919 16:52:40.699285 140474388072320 learning.py:507] global step 985: loss = 1.3935 (1.329 sec/step)\n",
            "I0919 16:52:41.988245 140474388072320 learning.py:507] global step 986: loss = 1.1078 (1.287 sec/step)\n",
            "I0919 16:52:43.296204 140474388072320 learning.py:507] global step 987: loss = 1.2203 (1.306 sec/step)\n",
            "I0919 16:52:44.594279 140474388072320 learning.py:507] global step 988: loss = 1.3315 (1.296 sec/step)\n",
            "I0919 16:52:45.887953 140474388072320 learning.py:507] global step 989: loss = 0.9959 (1.292 sec/step)\n",
            "I0919 16:52:47.179795 140474388072320 learning.py:507] global step 990: loss = 0.9925 (1.290 sec/step)\n",
            "I0919 16:52:48.499249 140474388072320 learning.py:507] global step 991: loss = 0.9994 (1.318 sec/step)\n",
            "I0919 16:52:49.799509 140474388072320 learning.py:507] global step 992: loss = 1.3180 (1.299 sec/step)\n",
            "I0919 16:52:51.120421 140474388072320 learning.py:507] global step 993: loss = 1.4585 (1.319 sec/step)\n",
            "I0919 16:52:52.437031 140474388072320 learning.py:507] global step 994: loss = 1.2822 (1.315 sec/step)\n",
            "I0919 16:52:53.774472 140474388072320 learning.py:507] global step 995: loss = 1.1285 (1.336 sec/step)\n",
            "I0919 16:52:55.082428 140474388072320 learning.py:507] global step 996: loss = 1.5403 (1.306 sec/step)\n",
            "I0919 16:52:56.408375 140474388072320 learning.py:507] global step 997: loss = 1.2217 (1.324 sec/step)\n",
            "I0919 16:52:57.767527 140474388072320 learning.py:507] global step 998: loss = 1.1728 (1.358 sec/step)\n",
            "I0919 16:52:59.086282 140474388072320 learning.py:507] global step 999: loss = 1.2463 (1.317 sec/step)\n",
            "I0919 16:53:00.381248 140474388072320 learning.py:507] global step 1000: loss = 1.0457 (1.293 sec/step)\n",
            "I0919 16:53:01.699527 140474388072320 learning.py:507] global step 1001: loss = 1.0027 (1.317 sec/step)\n",
            "I0919 16:53:02.975595 140474388072320 learning.py:507] global step 1002: loss = 1.1738 (1.274 sec/step)\n",
            "I0919 16:53:04.283025 140474388072320 learning.py:507] global step 1003: loss = 1.5009 (1.306 sec/step)\n",
            "I0919 16:53:05.581022 140474388072320 learning.py:507] global step 1004: loss = 1.2423 (1.296 sec/step)\n",
            "I0919 16:53:06.886348 140474388072320 learning.py:507] global step 1005: loss = 1.2575 (1.303 sec/step)\n",
            "I0919 16:53:08.241875 140474388072320 learning.py:507] global step 1006: loss = 0.9812 (1.354 sec/step)\n",
            "I0919 16:53:09.531196 140474388072320 learning.py:507] global step 1007: loss = 1.4503 (1.284 sec/step)\n",
            "I0919 16:53:10.817873 140474388072320 learning.py:507] global step 1008: loss = 1.2105 (1.285 sec/step)\n",
            "I0919 16:53:12.111187 140474388072320 learning.py:507] global step 1009: loss = 1.1007 (1.291 sec/step)\n",
            "I0919 16:53:13.411375 140474388072320 learning.py:507] global step 1010: loss = 1.0608 (1.298 sec/step)\n",
            "I0919 16:53:14.715464 140474388072320 learning.py:507] global step 1011: loss = 1.4152 (1.303 sec/step)\n",
            "I0919 16:53:16.023652 140474388072320 learning.py:507] global step 1012: loss = 1.1144 (1.306 sec/step)\n",
            "I0919 16:53:17.320724 140474388072320 learning.py:507] global step 1013: loss = 1.1864 (1.296 sec/step)\n",
            "I0919 16:53:18.651166 140474388072320 learning.py:507] global step 1014: loss = 1.3298 (1.329 sec/step)\n",
            "I0919 16:53:19.935200 140474388072320 learning.py:507] global step 1015: loss = 1.2981 (1.282 sec/step)\n",
            "I0919 16:53:21.228306 140474388072320 learning.py:507] global step 1016: loss = 1.1280 (1.291 sec/step)\n",
            "I0919 16:53:22.544507 140474388072320 learning.py:507] global step 1017: loss = 1.2245 (1.314 sec/step)\n",
            "I0919 16:53:23.887228 140474388072320 learning.py:507] global step 1018: loss = 1.5173 (1.341 sec/step)\n",
            "I0919 16:53:25.276097 140474388072320 learning.py:507] global step 1019: loss = 1.2822 (1.387 sec/step)\n",
            "I0919 16:53:26.616985 140474388072320 learning.py:507] global step 1020: loss = 1.4333 (1.339 sec/step)\n",
            "I0919 16:53:27.928196 140474388072320 learning.py:507] global step 1021: loss = 1.3857 (1.309 sec/step)\n",
            "I0919 16:53:29.207356 140474388072320 learning.py:507] global step 1022: loss = 1.1554 (1.278 sec/step)\n",
            "I0919 16:53:30.521178 140474388072320 learning.py:507] global step 1023: loss = 1.1803 (1.312 sec/step)\n",
            "I0919 16:53:31.831067 140474388072320 learning.py:507] global step 1024: loss = 1.1625 (1.308 sec/step)\n",
            "I0919 16:53:33.133702 140474388072320 learning.py:507] global step 1025: loss = 1.3122 (1.301 sec/step)\n",
            "I0919 16:53:34.471880 140474388072320 learning.py:507] global step 1026: loss = 1.0845 (1.337 sec/step)\n",
            "I0919 16:53:35.785157 140474388072320 learning.py:507] global step 1027: loss = 1.2907 (1.311 sec/step)\n",
            "I0919 16:53:37.079946 140474388072320 learning.py:507] global step 1028: loss = 1.3524 (1.293 sec/step)\n",
            "I0919 16:53:38.347019 140474388072320 learning.py:507] global step 1029: loss = 1.1868 (1.265 sec/step)\n",
            "I0919 16:53:39.639621 140474388072320 learning.py:507] global step 1030: loss = 1.4186 (1.291 sec/step)\n",
            "I0919 16:53:40.936585 140474388072320 learning.py:507] global step 1031: loss = 1.2457 (1.295 sec/step)\n",
            "I0919 16:53:42.226243 140474388072320 learning.py:507] global step 1032: loss = 1.2734 (1.288 sec/step)\n",
            "I0919 16:53:43.514194 140474388072320 learning.py:507] global step 1033: loss = 1.4100 (1.286 sec/step)\n",
            "I0919 16:53:44.823497 140474388072320 learning.py:507] global step 1034: loss = 1.0567 (1.308 sec/step)\n",
            "I0919 16:53:46.146898 140474388072320 learning.py:507] global step 1035: loss = 1.1449 (1.322 sec/step)\n",
            "I0919 16:53:47.448537 140474388072320 learning.py:507] global step 1036: loss = 1.2383 (1.300 sec/step)\n",
            "I0919 16:53:48.731405 140474388072320 learning.py:507] global step 1037: loss = 1.1437 (1.281 sec/step)\n",
            "I0919 16:53:49.999943 140474388072320 learning.py:507] global step 1038: loss = 1.2099 (1.267 sec/step)\n",
            "I0919 16:53:51.334202 140474388072320 learning.py:507] global step 1039: loss = 1.2681 (1.333 sec/step)\n",
            "I0919 16:53:52.683454 140474388072320 learning.py:507] global step 1040: loss = 1.1513 (1.347 sec/step)\n",
            "I0919 16:53:54.017158 140474388072320 learning.py:507] global step 1041: loss = 1.1986 (1.332 sec/step)\n",
            "I0919 16:53:55.342328 140474388072320 learning.py:507] global step 1042: loss = 1.0622 (1.323 sec/step)\n",
            "I0919 16:53:56.663847 140474388072320 learning.py:507] global step 1043: loss = 1.2411 (1.320 sec/step)\n",
            "I0919 16:53:57.942342 140474388072320 learning.py:507] global step 1044: loss = 1.5917 (1.277 sec/step)\n",
            "I0919 16:53:59.240328 140474388072320 learning.py:507] global step 1045: loss = 1.6101 (1.296 sec/step)\n",
            "I0919 16:54:00.556000 140474388072320 learning.py:507] global step 1046: loss = 1.2427 (1.314 sec/step)\n",
            "I0919 16:54:01.862432 140474388072320 learning.py:507] global step 1047: loss = 1.3306 (1.305 sec/step)\n",
            "I0919 16:54:03.149068 140474388072320 learning.py:507] global step 1048: loss = 1.1719 (1.285 sec/step)\n",
            "I0919 16:54:04.461924 140474388072320 learning.py:507] global step 1049: loss = 1.0704 (1.311 sec/step)\n",
            "I0919 16:54:05.769012 140474388072320 learning.py:507] global step 1050: loss = 1.1475 (1.306 sec/step)\n",
            "I0919 16:54:07.080755 140474388072320 learning.py:507] global step 1051: loss = 1.5265 (1.310 sec/step)\n",
            "I0919 16:54:08.432156 140474388072320 learning.py:507] global step 1052: loss = 1.2759 (1.349 sec/step)\n",
            "I0919 16:54:09.762921 140474388072320 learning.py:507] global step 1053: loss = 1.2688 (1.329 sec/step)\n",
            "I0919 16:54:11.035993 140474388072320 learning.py:507] global step 1054: loss = 1.2705 (1.271 sec/step)\n",
            "I0919 16:54:12.309883 140474388072320 learning.py:507] global step 1055: loss = 1.0821 (1.272 sec/step)\n",
            "I0919 16:54:13.645458 140474388072320 learning.py:507] global step 1056: loss = 1.2751 (1.334 sec/step)\n",
            "I0919 16:54:14.968691 140474388072320 learning.py:507] global step 1057: loss = 1.5192 (1.321 sec/step)\n",
            "I0919 16:54:16.273993 140474388072320 learning.py:507] global step 1058: loss = 1.3543 (1.303 sec/step)\n",
            "I0919 16:54:17.603935 140474388072320 learning.py:507] global step 1059: loss = 1.0935 (1.328 sec/step)\n",
            "I0919 16:54:18.945702 140474388072320 learning.py:507] global step 1060: loss = 1.2762 (1.340 sec/step)\n",
            "I0919 16:54:20.248884 140474388072320 learning.py:507] global step 1061: loss = 1.5205 (1.301 sec/step)\n",
            "I0919 16:54:21.590178 140474388072320 learning.py:507] global step 1062: loss = 1.2677 (1.339 sec/step)\n",
            "I0919 16:54:22.891046 140474388072320 learning.py:507] global step 1063: loss = 1.1668 (1.299 sec/step)\n",
            "I0919 16:54:24.184500 140474388072320 learning.py:507] global step 1064: loss = 1.1714 (1.292 sec/step)\n",
            "I0919 16:54:25.480683 140474388072320 learning.py:507] global step 1065: loss = 1.0476 (1.295 sec/step)\n",
            "I0919 16:54:26.801320 140474388072320 learning.py:507] global step 1066: loss = 1.0080 (1.319 sec/step)\n",
            "I0919 16:54:28.976371 140474388072320 learning.py:507] global step 1067: loss = 1.1255 (2.173 sec/step)\n",
            "I0919 16:54:28.976993 140471297337088 supervisor.py:1050] Recording summary at step 1067.\n",
            "I0919 16:54:30.282815 140474388072320 learning.py:507] global step 1068: loss = 1.1809 (1.300 sec/step)\n",
            "I0919 16:54:31.558938 140474388072320 learning.py:507] global step 1069: loss = 1.0791 (1.275 sec/step)\n",
            "I0919 16:54:32.841861 140474388072320 learning.py:507] global step 1070: loss = 1.3387 (1.281 sec/step)\n",
            "I0919 16:54:34.138334 140474388072320 learning.py:507] global step 1071: loss = 1.2304 (1.295 sec/step)\n",
            "I0919 16:54:35.438452 140474388072320 learning.py:507] global step 1072: loss = 1.2429 (1.298 sec/step)\n",
            "I0919 16:54:36.741353 140474388072320 learning.py:507] global step 1073: loss = 1.2310 (1.301 sec/step)\n",
            "I0919 16:54:38.084384 140474388072320 learning.py:507] global step 1074: loss = 1.1524 (1.341 sec/step)\n",
            "I0919 16:54:39.384510 140474388072320 learning.py:507] global step 1075: loss = 1.2560 (1.298 sec/step)\n",
            "I0919 16:54:40.698776 140474388072320 learning.py:507] global step 1076: loss = 1.1320 (1.313 sec/step)\n",
            "I0919 16:54:41.983397 140474388072320 learning.py:507] global step 1077: loss = 1.1952 (1.283 sec/step)\n",
            "I0919 16:54:43.295931 140474388072320 learning.py:507] global step 1078: loss = 1.2772 (1.311 sec/step)\n",
            "I0919 16:54:44.591365 140474388072320 learning.py:507] global step 1079: loss = 1.2104 (1.294 sec/step)\n",
            "I0919 16:54:45.947580 140474388072320 learning.py:507] global step 1080: loss = 1.2536 (1.354 sec/step)\n",
            "I0919 16:54:47.243449 140474388072320 learning.py:507] global step 1081: loss = 1.2178 (1.294 sec/step)\n",
            "I0919 16:54:48.538435 140474388072320 learning.py:507] global step 1082: loss = 1.1432 (1.293 sec/step)\n",
            "I0919 16:54:49.866964 140474388072320 learning.py:507] global step 1083: loss = 1.5291 (1.326 sec/step)\n",
            "I0919 16:54:51.155005 140474388072320 learning.py:507] global step 1084: loss = 0.9345 (1.286 sec/step)\n",
            "I0919 16:54:52.450192 140474388072320 learning.py:507] global step 1085: loss = 1.2099 (1.293 sec/step)\n",
            "I0919 16:54:53.775835 140474388072320 learning.py:507] global step 1086: loss = 1.3485 (1.323 sec/step)\n",
            "I0919 16:54:55.087580 140474388072320 learning.py:507] global step 1087: loss = 1.4750 (1.310 sec/step)\n",
            "I0919 16:54:56.361896 140474388072320 learning.py:507] global step 1088: loss = 1.5595 (1.272 sec/step)\n",
            "I0919 16:54:57.688239 140474388072320 learning.py:507] global step 1089: loss = 1.2374 (1.325 sec/step)\n",
            "I0919 16:54:59.023623 140474388072320 learning.py:507] global step 1090: loss = 1.0941 (1.334 sec/step)\n",
            "I0919 16:55:00.418843 140474388072320 learning.py:507] global step 1091: loss = 1.1777 (1.393 sec/step)\n",
            "I0919 16:55:01.752624 140474388072320 learning.py:507] global step 1092: loss = 1.5731 (1.332 sec/step)\n",
            "I0919 16:55:03.061796 140474388072320 learning.py:507] global step 1093: loss = 1.1057 (1.307 sec/step)\n",
            "I0919 16:55:04.338386 140474388072320 learning.py:507] global step 1094: loss = 1.0373 (1.275 sec/step)\n",
            "I0919 16:55:05.644617 140474388072320 learning.py:507] global step 1095: loss = 1.4362 (1.305 sec/step)\n",
            "I0919 16:55:06.940598 140474388072320 learning.py:507] global step 1096: loss = 1.0146 (1.294 sec/step)\n",
            "I0919 16:55:08.250283 140474388072320 learning.py:507] global step 1097: loss = 2.1805 (1.308 sec/step)\n",
            "I0919 16:55:09.556411 140474388072320 learning.py:507] global step 1098: loss = 1.0624 (1.302 sec/step)\n",
            "I0919 16:55:10.885164 140474388072320 learning.py:507] global step 1099: loss = 1.2968 (1.327 sec/step)\n",
            "I0919 16:55:12.184784 140474388072320 learning.py:507] global step 1100: loss = 1.2318 (1.298 sec/step)\n",
            "I0919 16:55:13.488723 140474388072320 learning.py:507] global step 1101: loss = 1.0146 (1.302 sec/step)\n",
            "I0919 16:55:14.795319 140474388072320 learning.py:507] global step 1102: loss = 1.4308 (1.305 sec/step)\n",
            "I0919 16:55:16.080620 140474388072320 learning.py:507] global step 1103: loss = 1.0427 (1.283 sec/step)\n",
            "I0919 16:55:17.369714 140474388072320 learning.py:507] global step 1104: loss = 1.1027 (1.287 sec/step)\n",
            "I0919 16:55:18.704024 140474388072320 learning.py:507] global step 1105: loss = 1.1814 (1.333 sec/step)\n",
            "I0919 16:55:20.005467 140474388072320 learning.py:507] global step 1106: loss = 1.5289 (1.300 sec/step)\n",
            "I0919 16:55:21.300898 140474388072320 learning.py:507] global step 1107: loss = 1.0617 (1.294 sec/step)\n",
            "I0919 16:55:22.592899 140474388072320 learning.py:507] global step 1108: loss = 1.0433 (1.290 sec/step)\n",
            "I0919 16:55:23.905920 140474388072320 learning.py:507] global step 1109: loss = 1.0152 (1.311 sec/step)\n",
            "I0919 16:55:25.233972 140474388072320 learning.py:507] global step 1110: loss = 0.9359 (1.326 sec/step)\n",
            "I0919 16:55:26.561160 140474388072320 learning.py:507] global step 1111: loss = 1.4795 (1.325 sec/step)\n",
            "I0919 16:55:27.865514 140474388072320 learning.py:507] global step 1112: loss = 0.9897 (1.303 sec/step)\n",
            "I0919 16:55:29.161017 140474388072320 learning.py:507] global step 1113: loss = 1.1195 (1.294 sec/step)\n",
            "I0919 16:55:30.428966 140474388072320 learning.py:507] global step 1114: loss = 1.1609 (1.266 sec/step)\n",
            "I0919 16:55:31.731872 140474388072320 learning.py:507] global step 1115: loss = 1.0190 (1.301 sec/step)\n",
            "I0919 16:55:33.022515 140474388072320 learning.py:507] global step 1116: loss = 1.0671 (1.289 sec/step)\n",
            "I0919 16:55:34.333596 140474388072320 learning.py:507] global step 1117: loss = 1.4347 (1.309 sec/step)\n",
            "I0919 16:55:35.659034 140474388072320 learning.py:507] global step 1118: loss = 1.2057 (1.324 sec/step)\n",
            "I0919 16:55:36.991230 140474388072320 learning.py:507] global step 1119: loss = 1.4448 (1.330 sec/step)\n",
            "I0919 16:55:38.317972 140474388072320 learning.py:507] global step 1120: loss = 1.2830 (1.325 sec/step)\n",
            "I0919 16:55:39.629208 140474388072320 learning.py:507] global step 1121: loss = 1.1946 (1.309 sec/step)\n",
            "I0919 16:55:40.995451 140474388072320 learning.py:507] global step 1122: loss = 1.2386 (1.364 sec/step)\n",
            "I0919 16:55:42.319403 140474388072320 learning.py:507] global step 1123: loss = 1.2088 (1.322 sec/step)\n",
            "I0919 16:55:43.635018 140474388072320 learning.py:507] global step 1124: loss = 1.0988 (1.314 sec/step)\n",
            "I0919 16:55:44.934530 140474388072320 learning.py:507] global step 1125: loss = 0.9254 (1.298 sec/step)\n",
            "I0919 16:55:46.244200 140474388072320 learning.py:507] global step 1126: loss = 1.0972 (1.308 sec/step)\n",
            "I0919 16:55:47.529211 140474388072320 learning.py:507] global step 1127: loss = 1.3298 (1.283 sec/step)\n",
            "I0919 16:55:48.849174 140474388072320 learning.py:507] global step 1128: loss = 1.2718 (1.318 sec/step)\n",
            "I0919 16:55:50.168437 140474388072320 learning.py:507] global step 1129: loss = 1.1823 (1.318 sec/step)\n",
            "I0919 16:55:51.434568 140474388072320 learning.py:507] global step 1130: loss = 0.9572 (1.264 sec/step)\n",
            "I0919 16:55:52.741904 140474388072320 learning.py:507] global step 1131: loss = 1.1691 (1.304 sec/step)\n",
            "I0919 16:55:54.013037 140474388072320 learning.py:507] global step 1132: loss = 1.1357 (1.270 sec/step)\n",
            "I0919 16:55:55.312902 140474388072320 learning.py:507] global step 1133: loss = 1.0721 (1.298 sec/step)\n",
            "I0919 16:55:56.589742 140474388072320 learning.py:507] global step 1134: loss = 1.3275 (1.275 sec/step)\n",
            "I0919 16:55:57.896869 140474388072320 learning.py:507] global step 1135: loss = 1.3886 (1.305 sec/step)\n",
            "I0919 16:55:59.194036 140474388072320 learning.py:507] global step 1136: loss = 1.2757 (1.296 sec/step)\n",
            "I0919 16:56:00.513974 140474388072320 learning.py:507] global step 1137: loss = 1.2375 (1.318 sec/step)\n",
            "I0919 16:56:01.814328 140474388072320 learning.py:507] global step 1138: loss = 1.2654 (1.299 sec/step)\n",
            "I0919 16:56:03.108095 140474388072320 learning.py:507] global step 1139: loss = 1.3517 (1.292 sec/step)\n",
            "I0919 16:56:04.402951 140474388072320 learning.py:507] global step 1140: loss = 1.2874 (1.293 sec/step)\n",
            "I0919 16:56:05.733289 140474388072320 learning.py:507] global step 1141: loss = 1.0944 (1.329 sec/step)\n",
            "I0919 16:56:07.034816 140474388072320 learning.py:507] global step 1142: loss = 1.3488 (1.300 sec/step)\n",
            "I0919 16:56:08.318885 140474388072320 learning.py:507] global step 1143: loss = 1.0814 (1.282 sec/step)\n",
            "I0919 16:56:09.655435 140474388072320 learning.py:507] global step 1144: loss = 1.2152 (1.335 sec/step)\n",
            "I0919 16:56:10.973485 140474388072320 learning.py:507] global step 1145: loss = 1.1346 (1.316 sec/step)\n",
            "I0919 16:56:12.349100 140474388072320 learning.py:507] global step 1146: loss = 1.1969 (1.374 sec/step)\n",
            "I0919 16:56:13.662101 140474388072320 learning.py:507] global step 1147: loss = 1.5979 (1.311 sec/step)\n",
            "I0919 16:56:14.963191 140474388072320 learning.py:507] global step 1148: loss = 1.1236 (1.299 sec/step)\n",
            "I0919 16:56:16.251738 140474388072320 learning.py:507] global step 1149: loss = 1.2877 (1.287 sec/step)\n",
            "I0919 16:56:17.568370 140474388072320 learning.py:507] global step 1150: loss = 1.3557 (1.314 sec/step)\n",
            "I0919 16:56:18.903352 140474388072320 learning.py:507] global step 1151: loss = 1.1119 (1.333 sec/step)\n",
            "I0919 16:56:20.227040 140474388072320 learning.py:507] global step 1152: loss = 1.1401 (1.322 sec/step)\n",
            "I0919 16:56:21.565849 140474388072320 learning.py:507] global step 1153: loss = 1.1500 (1.337 sec/step)\n",
            "I0919 16:56:22.889478 140474388072320 learning.py:507] global step 1154: loss = 1.1040 (1.322 sec/step)\n",
            "I0919 16:56:24.202505 140474388072320 learning.py:507] global step 1155: loss = 1.2787 (1.311 sec/step)\n",
            "I0919 16:56:25.502044 140474388072320 learning.py:507] global step 1156: loss = 1.4003 (1.298 sec/step)\n",
            "I0919 16:56:26.801942 140474388072320 learning.py:507] global step 1157: loss = 1.0453 (1.298 sec/step)\n",
            "I0919 16:56:28.937856 140474388072320 learning.py:507] global step 1158: loss = 1.2627 (2.134 sec/step)\n",
            "I0919 16:56:29.028528 140471297337088 supervisor.py:1050] Recording summary at step 1158.\n",
            "I0919 16:56:30.273464 140474388072320 learning.py:507] global step 1159: loss = 1.0434 (1.334 sec/step)\n",
            "I0919 16:56:31.561301 140474388072320 learning.py:507] global step 1160: loss = 1.3476 (1.286 sec/step)\n",
            "I0919 16:56:32.871458 140474388072320 learning.py:507] global step 1161: loss = 1.1482 (1.309 sec/step)\n",
            "I0919 16:56:34.160234 140474388072320 learning.py:507] global step 1162: loss = 1.2781 (1.287 sec/step)\n",
            "I0919 16:56:35.466243 140474388072320 learning.py:507] global step 1163: loss = 1.1047 (1.304 sec/step)\n",
            "I0919 16:56:36.764790 140474388072320 learning.py:507] global step 1164: loss = 1.0056 (1.297 sec/step)\n",
            "I0919 16:56:38.092978 140474388072320 learning.py:507] global step 1165: loss = 1.3114 (1.326 sec/step)\n",
            "I0919 16:56:39.400085 140474388072320 learning.py:507] global step 1166: loss = 1.3849 (1.305 sec/step)\n",
            "I0919 16:56:40.728209 140474388072320 learning.py:507] global step 1167: loss = 1.0655 (1.326 sec/step)\n",
            "I0919 16:56:42.038009 140474388072320 learning.py:507] global step 1168: loss = 1.2745 (1.308 sec/step)\n",
            "I0919 16:56:43.331794 140474388072320 learning.py:507] global step 1169: loss = 1.3454 (1.292 sec/step)\n",
            "I0919 16:56:44.630687 140474388072320 learning.py:507] global step 1170: loss = 1.2792 (1.297 sec/step)\n",
            "I0919 16:56:45.979397 140474388072320 learning.py:507] global step 1171: loss = 1.4366 (1.347 sec/step)\n",
            "I0919 16:56:47.281630 140474388072320 learning.py:507] global step 1172: loss = 0.9631 (1.300 sec/step)\n",
            "I0919 16:56:48.563760 140474388072320 learning.py:507] global step 1173: loss = 1.0804 (1.280 sec/step)\n",
            "I0919 16:56:49.844084 140474388072320 learning.py:507] global step 1174: loss = 1.4604 (1.279 sec/step)\n",
            "I0919 16:56:51.121731 140474388072320 learning.py:507] global step 1175: loss = 1.3938 (1.275 sec/step)\n",
            "I0919 16:56:52.441795 140474388072320 learning.py:507] global step 1176: loss = 1.0660 (1.318 sec/step)\n",
            "I0919 16:56:53.763039 140474388072320 learning.py:507] global step 1177: loss = 0.9813 (1.320 sec/step)\n",
            "I0919 16:56:55.063570 140474388072320 learning.py:507] global step 1178: loss = 1.3421 (1.299 sec/step)\n",
            "I0919 16:56:56.426630 140474388072320 learning.py:507] global step 1179: loss = 1.1151 (1.361 sec/step)\n",
            "I0919 16:56:57.792735 140474388072320 learning.py:507] global step 1180: loss = 1.5798 (1.364 sec/step)\n",
            "I0919 16:56:59.103035 140474388072320 learning.py:507] global step 1181: loss = 1.3125 (1.308 sec/step)\n",
            "I0919 16:57:00.400074 140474388072320 learning.py:507] global step 1182: loss = 1.0587 (1.295 sec/step)\n",
            "I0919 16:57:01.731873 140474388072320 learning.py:507] global step 1183: loss = 1.0084 (1.330 sec/step)\n",
            "I0919 16:57:03.038439 140474388072320 learning.py:507] global step 1184: loss = 0.9631 (1.305 sec/step)\n",
            "I0919 16:57:04.334959 140474388072320 learning.py:507] global step 1185: loss = 1.5887 (1.295 sec/step)\n",
            "I0919 16:57:05.617084 140474388072320 learning.py:507] global step 1186: loss = 1.3026 (1.281 sec/step)\n",
            "I0919 16:57:06.915447 140474388072320 learning.py:507] global step 1187: loss = 1.2890 (1.297 sec/step)\n",
            "I0919 16:57:08.223820 140474388072320 learning.py:507] global step 1188: loss = 0.9130 (1.306 sec/step)\n",
            "I0919 16:57:09.592900 140474388072320 learning.py:507] global step 1189: loss = 1.2492 (1.367 sec/step)\n",
            "I0919 16:57:10.913943 140474388072320 learning.py:507] global step 1190: loss = 1.0543 (1.319 sec/step)\n",
            "I0919 16:57:12.217679 140474388072320 learning.py:507] global step 1191: loss = 0.9730 (1.301 sec/step)\n",
            "I0919 16:57:13.497656 140474388072320 learning.py:507] global step 1192: loss = 1.0997 (1.278 sec/step)\n",
            "I0919 16:57:14.807769 140474388072320 learning.py:507] global step 1193: loss = 1.0002 (1.308 sec/step)\n",
            "I0919 16:57:16.104273 140474388072320 learning.py:507] global step 1194: loss = 1.0509 (1.295 sec/step)\n",
            "I0919 16:57:17.399480 140474388072320 learning.py:507] global step 1195: loss = 0.9436 (1.294 sec/step)\n",
            "I0919 16:57:18.713701 140474388072320 learning.py:507] global step 1196: loss = 1.2076 (1.312 sec/step)\n",
            "I0919 16:57:20.015409 140474388072320 learning.py:507] global step 1197: loss = 1.3765 (1.300 sec/step)\n",
            "I0919 16:57:21.326082 140474388072320 learning.py:507] global step 1198: loss = 1.2027 (1.309 sec/step)\n",
            "I0919 16:57:22.623411 140474388072320 learning.py:507] global step 1199: loss = 1.1831 (1.296 sec/step)\n",
            "I0919 16:57:23.910923 140474388072320 learning.py:507] global step 1200: loss = 1.1067 (1.286 sec/step)\n",
            "I0919 16:57:25.228410 140474388072320 learning.py:507] global step 1201: loss = 1.0727 (1.316 sec/step)\n",
            "I0919 16:57:26.562889 140474388072320 learning.py:507] global step 1202: loss = 1.0407 (1.333 sec/step)\n",
            "I0919 16:57:27.904608 140474388072320 learning.py:507] global step 1203: loss = 1.1804 (1.340 sec/step)\n",
            "I0919 16:57:29.223445 140474388072320 learning.py:507] global step 1204: loss = 1.4357 (1.317 sec/step)\n",
            "I0919 16:57:30.514204 140474388072320 learning.py:507] global step 1205: loss = 1.0115 (1.289 sec/step)\n",
            "I0919 16:57:31.780287 140474388072320 learning.py:507] global step 1206: loss = 1.4384 (1.264 sec/step)\n",
            "I0919 16:57:33.063869 140474388072320 learning.py:507] global step 1207: loss = 1.3304 (1.282 sec/step)\n",
            "I0919 16:57:34.362045 140474388072320 learning.py:507] global step 1208: loss = 1.2020 (1.296 sec/step)\n",
            "I0919 16:57:35.656282 140474388072320 learning.py:507] global step 1209: loss = 1.0881 (1.292 sec/step)\n",
            "I0919 16:57:36.951800 140474388072320 learning.py:507] global step 1210: loss = 1.0990 (1.294 sec/step)\n",
            "I0919 16:57:38.231983 140474388072320 learning.py:507] global step 1211: loss = 1.1732 (1.278 sec/step)\n",
            "I0919 16:57:39.584692 140474388072320 learning.py:507] global step 1212: loss = 1.2156 (1.351 sec/step)\n",
            "I0919 16:57:40.875845 140474388072320 learning.py:507] global step 1213: loss = 1.1746 (1.289 sec/step)\n",
            "I0919 16:57:42.188733 140474388072320 learning.py:507] global step 1214: loss = 0.9585 (1.311 sec/step)\n",
            "I0919 16:57:43.483470 140474388072320 learning.py:507] global step 1215: loss = 0.8741 (1.293 sec/step)\n",
            "I0919 16:57:44.849808 140474388072320 learning.py:507] global step 1216: loss = 1.3120 (1.365 sec/step)\n",
            "I0919 16:57:46.159208 140474388072320 learning.py:507] global step 1217: loss = 1.0943 (1.308 sec/step)\n",
            "I0919 16:57:47.511717 140474388072320 learning.py:507] global step 1218: loss = 1.0541 (1.351 sec/step)\n",
            "I0919 16:57:48.810563 140474388072320 learning.py:507] global step 1219: loss = 1.1026 (1.297 sec/step)\n",
            "I0919 16:57:50.131597 140474388072320 learning.py:507] global step 1220: loss = 1.0998 (1.319 sec/step)\n",
            "I0919 16:57:51.475080 140474388072320 learning.py:507] global step 1221: loss = 1.1350 (1.342 sec/step)\n",
            "I0919 16:57:52.750639 140474388072320 learning.py:507] global step 1222: loss = 1.3016 (1.274 sec/step)\n",
            "I0919 16:57:54.037823 140474388072320 learning.py:507] global step 1223: loss = 1.1352 (1.286 sec/step)\n",
            "I0919 16:57:55.355403 140474388072320 learning.py:507] global step 1224: loss = 1.0701 (1.316 sec/step)\n",
            "I0919 16:57:56.667448 140474388072320 learning.py:507] global step 1225: loss = 1.0978 (1.310 sec/step)\n",
            "I0919 16:57:57.968154 140474388072320 learning.py:507] global step 1226: loss = 1.0739 (1.299 sec/step)\n",
            "I0919 16:57:59.275269 140474388072320 learning.py:507] global step 1227: loss = 1.1092 (1.306 sec/step)\n",
            "I0919 16:58:00.593875 140474388072320 learning.py:507] global step 1228: loss = 1.0211 (1.317 sec/step)\n",
            "I0919 16:58:01.935343 140474388072320 learning.py:507] global step 1229: loss = 0.8859 (1.339 sec/step)\n",
            "I0919 16:58:03.255074 140474388072320 learning.py:507] global step 1230: loss = 1.2263 (1.318 sec/step)\n",
            "I0919 16:58:04.577560 140474388072320 learning.py:507] global step 1231: loss = 1.0440 (1.321 sec/step)\n",
            "I0919 16:58:05.879182 140474388072320 learning.py:507] global step 1232: loss = 0.9163 (1.300 sec/step)\n",
            "I0919 16:58:07.215308 140474388072320 learning.py:507] global step 1233: loss = 0.9750 (1.335 sec/step)\n",
            "I0919 16:58:08.525998 140474388072320 learning.py:507] global step 1234: loss = 1.1625 (1.309 sec/step)\n",
            "I0919 16:58:09.890043 140474388072320 learning.py:507] global step 1235: loss = 0.9365 (1.362 sec/step)\n",
            "I0919 16:58:11.214035 140474388072320 learning.py:507] global step 1236: loss = 0.9730 (1.322 sec/step)\n",
            "I0919 16:58:12.537707 140474388072320 learning.py:507] global step 1237: loss = 0.9724 (1.322 sec/step)\n",
            "I0919 16:58:13.847198 140474388072320 learning.py:507] global step 1238: loss = 1.0428 (1.308 sec/step)\n",
            "I0919 16:58:15.157994 140474388072320 learning.py:507] global step 1239: loss = 0.9565 (1.309 sec/step)\n",
            "I0919 16:58:16.462235 140474388072320 learning.py:507] global step 1240: loss = 1.1753 (1.302 sec/step)\n",
            "I0919 16:58:17.759171 140474388072320 learning.py:507] global step 1241: loss = 1.3002 (1.295 sec/step)\n",
            "I0919 16:58:19.034277 140474388072320 learning.py:507] global step 1242: loss = 1.0307 (1.273 sec/step)\n",
            "I0919 16:58:20.337495 140474388072320 learning.py:507] global step 1243: loss = 1.0645 (1.302 sec/step)\n",
            "I0919 16:58:21.644033 140474388072320 learning.py:507] global step 1244: loss = 1.6757 (1.305 sec/step)\n",
            "I0919 16:58:22.970427 140474388072320 learning.py:507] global step 1245: loss = 1.0820 (1.325 sec/step)\n",
            "I0919 16:58:24.298783 140474388072320 learning.py:507] global step 1246: loss = 1.2907 (1.327 sec/step)\n",
            "I0919 16:58:25.640984 140474388072320 learning.py:507] global step 1247: loss = 1.1561 (1.340 sec/step)\n",
            "I0919 16:58:26.933676 140474388072320 learning.py:507] global step 1248: loss = 0.9845 (1.284 sec/step)\n",
            "I0919 16:58:28.713586 140471297337088 supervisor.py:1050] Recording summary at step 1248.\n",
            "I0919 16:58:29.121074 140474388072320 learning.py:507] global step 1249: loss = 1.1472 (2.186 sec/step)\n",
            "I0919 16:58:30.422255 140474388072320 learning.py:507] global step 1250: loss = 0.9955 (1.299 sec/step)\n",
            "I0919 16:58:31.770056 140474388072320 learning.py:507] global step 1251: loss = 1.0231 (1.346 sec/step)\n",
            "I0919 16:58:33.090430 140474388072320 learning.py:507] global step 1252: loss = 1.0883 (1.318 sec/step)\n",
            "I0919 16:58:34.437572 140474388072320 learning.py:507] global step 1253: loss = 1.3489 (1.346 sec/step)\n",
            "I0919 16:58:35.730895 140474388072320 learning.py:507] global step 1254: loss = 1.0936 (1.292 sec/step)\n",
            "I0919 16:58:37.079543 140474388072320 learning.py:507] global step 1255: loss = 1.1433 (1.347 sec/step)\n",
            "I0919 16:58:38.355796 140474388072320 learning.py:507] global step 1256: loss = 0.9631 (1.274 sec/step)\n",
            "I0919 16:58:39.693120 140474388072320 learning.py:507] global step 1257: loss = 0.9975 (1.336 sec/step)\n",
            "I0919 16:58:41.014436 140474388072320 learning.py:507] global step 1258: loss = 1.0269 (1.320 sec/step)\n",
            "I0919 16:58:42.327733 140474388072320 learning.py:507] global step 1259: loss = 1.0590 (1.311 sec/step)\n",
            "I0919 16:58:43.625472 140474388072320 learning.py:507] global step 1260: loss = 1.0566 (1.296 sec/step)\n",
            "I0919 16:58:44.968460 140474388072320 learning.py:507] global step 1261: loss = 1.1804 (1.341 sec/step)\n",
            "I0919 16:58:46.307400 140474388072320 learning.py:507] global step 1262: loss = 1.2572 (1.337 sec/step)\n",
            "I0919 16:58:47.623604 140474388072320 learning.py:507] global step 1263: loss = 1.2696 (1.315 sec/step)\n",
            "I0919 16:58:48.957384 140474388072320 learning.py:507] global step 1264: loss = 1.6279 (1.332 sec/step)\n",
            "I0919 16:58:50.241096 140474388072320 learning.py:507] global step 1265: loss = 1.3712 (1.282 sec/step)\n",
            "I0919 16:58:51.553565 140474388072320 learning.py:507] global step 1266: loss = 1.0784 (1.311 sec/step)\n",
            "I0919 16:58:52.879053 140474388072320 learning.py:507] global step 1267: loss = 1.2336 (1.324 sec/step)\n",
            "I0919 16:58:54.190017 140474388072320 learning.py:507] global step 1268: loss = 1.0216 (1.309 sec/step)\n",
            "I0919 16:58:55.545731 140474388072320 learning.py:507] global step 1269: loss = 0.9807 (1.354 sec/step)\n",
            "I0919 16:58:56.867260 140474388072320 learning.py:507] global step 1270: loss = 1.0648 (1.320 sec/step)\n",
            "I0919 16:58:58.162358 140474388072320 learning.py:507] global step 1271: loss = 0.9953 (1.293 sec/step)\n",
            "I0919 16:58:59.458833 140474388072320 learning.py:507] global step 1272: loss = 1.0882 (1.295 sec/step)\n",
            "I0919 16:59:00.788350 140474388072320 learning.py:507] global step 1273: loss = 1.1684 (1.328 sec/step)\n",
            "I0919 16:59:02.104033 140474388072320 learning.py:507] global step 1274: loss = 1.3492 (1.314 sec/step)\n",
            "I0919 16:59:03.428018 140474388072320 learning.py:507] global step 1275: loss = 1.0156 (1.322 sec/step)\n",
            "I0919 16:59:04.721565 140474388072320 learning.py:507] global step 1276: loss = 1.1346 (1.292 sec/step)\n",
            "I0919 16:59:06.049170 140474388072320 learning.py:507] global step 1277: loss = 1.2546 (1.324 sec/step)\n",
            "I0919 16:59:07.333914 140474388072320 learning.py:507] global step 1278: loss = 1.3219 (1.283 sec/step)\n",
            "I0919 16:59:08.651768 140474388072320 learning.py:507] global step 1279: loss = 1.0552 (1.316 sec/step)\n",
            "I0919 16:59:10.010992 140474388072320 learning.py:507] global step 1280: loss = 1.1310 (1.357 sec/step)\n",
            "I0919 16:59:11.323462 140474388072320 learning.py:507] global step 1281: loss = 1.2636 (1.311 sec/step)\n",
            "I0919 16:59:12.603234 140474388072320 learning.py:507] global step 1282: loss = 1.1896 (1.278 sec/step)\n",
            "I0919 16:59:13.881659 140474388072320 learning.py:507] global step 1283: loss = 0.9909 (1.277 sec/step)\n",
            "I0919 16:59:15.190934 140474388072320 learning.py:507] global step 1284: loss = 1.3613 (1.307 sec/step)\n",
            "I0919 16:59:16.527699 140474388072320 learning.py:507] global step 1285: loss = 1.0803 (1.335 sec/step)\n",
            "I0919 16:59:17.880265 140474388072320 learning.py:507] global step 1286: loss = 0.9874 (1.351 sec/step)\n",
            "I0919 16:59:19.172805 140474388072320 learning.py:507] global step 1287: loss = 1.0659 (1.291 sec/step)\n",
            "I0919 16:59:20.490433 140474388072320 learning.py:507] global step 1288: loss = 0.9895 (1.316 sec/step)\n",
            "I0919 16:59:21.806704 140474388072320 learning.py:507] global step 1289: loss = 0.9819 (1.314 sec/step)\n",
            "I0919 16:59:23.127398 140474388072320 learning.py:507] global step 1290: loss = 0.9791 (1.318 sec/step)\n",
            "I0919 16:59:24.422544 140474388072320 learning.py:507] global step 1291: loss = 1.3697 (1.293 sec/step)\n",
            "I0919 16:59:25.718681 140474388072320 learning.py:507] global step 1292: loss = 1.1534 (1.294 sec/step)\n",
            "I0919 16:59:26.997303 140474388072320 learning.py:507] global step 1293: loss = 1.2475 (1.277 sec/step)\n",
            "I0919 16:59:28.322309 140474388072320 learning.py:507] global step 1294: loss = 1.0090 (1.323 sec/step)\n",
            "I0919 16:59:29.674753 140474388072320 learning.py:507] global step 1295: loss = 1.4514 (1.351 sec/step)\n",
            "I0919 16:59:30.961091 140474388072320 learning.py:507] global step 1296: loss = 1.0715 (1.285 sec/step)\n",
            "I0919 16:59:32.274746 140474388072320 learning.py:507] global step 1297: loss = 1.2380 (1.312 sec/step)\n",
            "I0919 16:59:33.580368 140474388072320 learning.py:507] global step 1298: loss = 1.0039 (1.304 sec/step)\n",
            "I0919 16:59:34.925816 140474388072320 learning.py:507] global step 1299: loss = 1.0255 (1.344 sec/step)\n",
            "I0919 16:59:36.254291 140474388072320 learning.py:507] global step 1300: loss = 1.1246 (1.327 sec/step)\n",
            "I0919 16:59:37.569431 140474388072320 learning.py:507] global step 1301: loss = 1.2369 (1.313 sec/step)\n",
            "I0919 16:59:38.885319 140474388072320 learning.py:507] global step 1302: loss = 1.2141 (1.314 sec/step)\n",
            "I0919 16:59:40.190249 140474388072320 learning.py:507] global step 1303: loss = 1.0274 (1.303 sec/step)\n",
            "I0919 16:59:41.511162 140474388072320 learning.py:507] global step 1304: loss = 1.6953 (1.319 sec/step)\n",
            "I0919 16:59:42.814191 140474388072320 learning.py:507] global step 1305: loss = 1.0943 (1.301 sec/step)\n",
            "I0919 16:59:44.132791 140474388072320 learning.py:507] global step 1306: loss = 0.8930 (1.317 sec/step)\n",
            "I0919 16:59:45.468429 140474388072320 learning.py:507] global step 1307: loss = 1.1732 (1.334 sec/step)\n",
            "I0919 16:59:46.821994 140474388072320 learning.py:507] global step 1308: loss = 1.2257 (1.351 sec/step)\n",
            "I0919 16:59:48.129410 140474388072320 learning.py:507] global step 1309: loss = 1.2217 (1.306 sec/step)\n",
            "I0919 16:59:49.431786 140474388072320 learning.py:507] global step 1310: loss = 1.1679 (1.301 sec/step)\n",
            "I0919 16:59:50.764067 140474388072320 learning.py:507] global step 1311: loss = 1.0651 (1.331 sec/step)\n",
            "I0919 16:59:52.044617 140474388072320 learning.py:507] global step 1312: loss = 1.0180 (1.279 sec/step)\n",
            "I0919 16:59:53.320730 140474388072320 learning.py:507] global step 1313: loss = 0.9946 (1.274 sec/step)\n",
            "I0919 16:59:54.616355 140474388072320 learning.py:507] global step 1314: loss = 1.1242 (1.294 sec/step)\n",
            "I0919 16:59:55.922755 140474388072320 learning.py:507] global step 1315: loss = 0.9996 (1.305 sec/step)\n",
            "I0919 16:59:57.248533 140474388072320 learning.py:507] global step 1316: loss = 1.3432 (1.324 sec/step)\n",
            "I0919 16:59:58.562444 140474388072320 learning.py:507] global step 1317: loss = 1.0289 (1.312 sec/step)\n",
            "I0919 16:59:59.870692 140474388072320 learning.py:507] global step 1318: loss = 1.0272 (1.306 sec/step)\n",
            "I0919 17:00:01.171412 140474388072320 learning.py:507] global step 1319: loss = 1.1793 (1.299 sec/step)\n",
            "I0919 17:00:02.495130 140474388072320 learning.py:507] global step 1320: loss = 1.0370 (1.322 sec/step)\n",
            "I0919 17:00:03.814867 140474388072320 learning.py:507] global step 1321: loss = 1.0056 (1.318 sec/step)\n",
            "I0919 17:00:05.169100 140474388072320 learning.py:507] global step 1322: loss = 0.9639 (1.352 sec/step)\n",
            "I0919 17:00:06.502506 140474388072320 learning.py:507] global step 1323: loss = 1.1793 (1.331 sec/step)\n",
            "I0919 17:00:07.846318 140474388072320 learning.py:507] global step 1324: loss = 1.3748 (1.342 sec/step)\n",
            "I0919 17:00:09.128520 140474388072320 learning.py:507] global step 1325: loss = 1.1261 (1.280 sec/step)\n",
            "I0919 17:00:10.421892 140474388072320 learning.py:507] global step 1326: loss = 1.1243 (1.292 sec/step)\n",
            "I0919 17:00:11.728645 140474388072320 learning.py:507] global step 1327: loss = 0.9955 (1.305 sec/step)\n",
            "I0919 17:00:13.021219 140474388072320 learning.py:507] global step 1328: loss = 1.2889 (1.291 sec/step)\n",
            "I0919 17:00:14.292741 140474388072320 learning.py:507] global step 1329: loss = 0.8892 (1.270 sec/step)\n",
            "I0919 17:00:15.600856 140474388072320 learning.py:507] global step 1330: loss = 1.0720 (1.306 sec/step)\n",
            "I0919 17:00:16.910355 140474388072320 learning.py:507] global step 1331: loss = 1.0083 (1.308 sec/step)\n",
            "I0919 17:00:18.219024 140474388072320 learning.py:507] global step 1332: loss = 0.9841 (1.307 sec/step)\n",
            "I0919 17:00:19.500401 140474388072320 learning.py:507] global step 1333: loss = 1.3436 (1.280 sec/step)\n",
            "I0919 17:00:20.792519 140474388072320 learning.py:507] global step 1334: loss = 0.9573 (1.291 sec/step)\n",
            "I0919 17:00:22.155299 140474388072320 learning.py:507] global step 1335: loss = 1.0800 (1.360 sec/step)\n",
            "I0919 17:00:23.484899 140474388072320 learning.py:507] global step 1336: loss = 0.9242 (1.328 sec/step)\n",
            "I0919 17:00:24.852248 140474388072320 learning.py:507] global step 1337: loss = 1.0709 (1.366 sec/step)\n",
            "I0919 17:00:26.161698 140474388072320 learning.py:507] global step 1338: loss = 0.9499 (1.308 sec/step)\n",
            "I0919 17:00:26.873548 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 17:00:28.629007 140474388072320 learning.py:507] global step 1339: loss = 0.9292 (2.465 sec/step)\n",
            "I0919 17:00:29.007708 140471297337088 supervisor.py:1050] Recording summary at step 1339.\n",
            "I0919 17:00:30.413697 140474388072320 learning.py:507] global step 1340: loss = 1.4129 (1.777 sec/step)\n",
            "I0919 17:00:32.324578 140474388072320 learning.py:507] global step 1341: loss = 1.1291 (1.900 sec/step)\n",
            "I0919 17:00:33.621323 140474388072320 learning.py:507] global step 1342: loss = 1.2607 (1.295 sec/step)\n",
            "I0919 17:00:34.932781 140474388072320 learning.py:507] global step 1343: loss = 1.0964 (1.310 sec/step)\n",
            "I0919 17:00:36.244978 140474388072320 learning.py:507] global step 1344: loss = 1.1561 (1.310 sec/step)\n",
            "I0919 17:00:37.544014 140474388072320 learning.py:507] global step 1345: loss = 0.9974 (1.297 sec/step)\n",
            "I0919 17:00:38.865022 140474388072320 learning.py:507] global step 1346: loss = 0.8951 (1.319 sec/step)\n",
            "I0919 17:00:40.199282 140474388072320 learning.py:507] global step 1347: loss = 1.0680 (1.332 sec/step)\n",
            "I0919 17:00:41.529393 140474388072320 learning.py:507] global step 1348: loss = 0.9712 (1.328 sec/step)\n",
            "I0919 17:00:42.838352 140474388072320 learning.py:507] global step 1349: loss = 1.2445 (1.307 sec/step)\n",
            "I0919 17:00:44.128967 140474388072320 learning.py:507] global step 1350: loss = 1.1281 (1.289 sec/step)\n",
            "I0919 17:00:45.448903 140474388072320 learning.py:507] global step 1351: loss = 0.9887 (1.318 sec/step)\n",
            "I0919 17:00:46.750221 140474388072320 learning.py:507] global step 1352: loss = 1.1290 (1.300 sec/step)\n",
            "I0919 17:00:48.031814 140474388072320 learning.py:507] global step 1353: loss = 1.0053 (1.280 sec/step)\n",
            "I0919 17:00:49.329315 140474388072320 learning.py:507] global step 1354: loss = 1.1952 (1.296 sec/step)\n",
            "I0919 17:00:50.654385 140474388072320 learning.py:507] global step 1355: loss = 1.0038 (1.323 sec/step)\n",
            "I0919 17:00:51.983510 140474388072320 learning.py:507] global step 1356: loss = 1.2672 (1.327 sec/step)\n",
            "I0919 17:00:53.289927 140474388072320 learning.py:507] global step 1357: loss = 1.0781 (1.304 sec/step)\n",
            "I0919 17:00:54.594434 140474388072320 learning.py:507] global step 1358: loss = 0.9591 (1.303 sec/step)\n",
            "I0919 17:00:55.941968 140474388072320 learning.py:507] global step 1359: loss = 0.9169 (1.346 sec/step)\n",
            "I0919 17:00:57.240969 140474388072320 learning.py:507] global step 1360: loss = 1.1648 (1.297 sec/step)\n",
            "I0919 17:00:58.523408 140474388072320 learning.py:507] global step 1361: loss = 1.4841 (1.281 sec/step)\n",
            "I0919 17:00:59.830048 140474388072320 learning.py:507] global step 1362: loss = 1.2275 (1.305 sec/step)\n",
            "I0919 17:01:01.135476 140474388072320 learning.py:507] global step 1363: loss = 0.9100 (1.303 sec/step)\n",
            "I0919 17:01:02.414482 140474388072320 learning.py:507] global step 1364: loss = 1.6954 (1.277 sec/step)\n",
            "I0919 17:01:03.788033 140474388072320 learning.py:507] global step 1365: loss = 1.0945 (1.372 sec/step)\n",
            "I0919 17:01:05.076181 140474388072320 learning.py:507] global step 1366: loss = 0.9554 (1.287 sec/step)\n",
            "I0919 17:01:06.376152 140474388072320 learning.py:507] global step 1367: loss = 1.0649 (1.298 sec/step)\n",
            "I0919 17:01:07.660263 140474388072320 learning.py:507] global step 1368: loss = 1.4326 (1.283 sec/step)\n",
            "I0919 17:01:08.972033 140474388072320 learning.py:507] global step 1369: loss = 0.8859 (1.310 sec/step)\n",
            "I0919 17:01:10.265518 140474388072320 learning.py:507] global step 1370: loss = 1.0363 (1.292 sec/step)\n",
            "I0919 17:01:11.557442 140474388072320 learning.py:507] global step 1371: loss = 1.3449 (1.290 sec/step)\n",
            "I0919 17:01:12.854130 140474388072320 learning.py:507] global step 1372: loss = 1.0907 (1.295 sec/step)\n",
            "I0919 17:01:14.158030 140474388072320 learning.py:507] global step 1373: loss = 1.0061 (1.302 sec/step)\n",
            "I0919 17:01:15.455789 140474388072320 learning.py:507] global step 1374: loss = 1.6871 (1.296 sec/step)\n",
            "I0919 17:01:16.772609 140474388072320 learning.py:507] global step 1375: loss = 1.0670 (1.315 sec/step)\n",
            "I0919 17:01:18.120297 140474388072320 learning.py:507] global step 1376: loss = 1.0546 (1.346 sec/step)\n",
            "I0919 17:01:19.419481 140474388072320 learning.py:507] global step 1377: loss = 1.0571 (1.297 sec/step)\n",
            "I0919 17:01:20.765228 140474388072320 learning.py:507] global step 1378: loss = 0.9285 (1.344 sec/step)\n",
            "I0919 17:01:22.091726 140474388072320 learning.py:507] global step 1379: loss = 0.9779 (1.325 sec/step)\n",
            "I0919 17:01:23.426506 140474388072320 learning.py:507] global step 1380: loss = 1.2258 (1.333 sec/step)\n",
            "I0919 17:01:24.749953 140474388072320 learning.py:507] global step 1381: loss = 1.1173 (1.322 sec/step)\n",
            "I0919 17:01:26.043473 140474388072320 learning.py:507] global step 1382: loss = 0.9194 (1.291 sec/step)\n",
            "I0919 17:01:27.337796 140474388072320 learning.py:507] global step 1383: loss = 1.4181 (1.293 sec/step)\n",
            "I0919 17:01:28.680959 140474388072320 learning.py:507] global step 1384: loss = 1.6423 (1.341 sec/step)\n",
            "I0919 17:01:29.991332 140474388072320 learning.py:507] global step 1385: loss = 1.0451 (1.308 sec/step)\n",
            "I0919 17:01:31.325146 140474388072320 learning.py:507] global step 1386: loss = 1.1509 (1.332 sec/step)\n",
            "I0919 17:01:32.628554 140474388072320 learning.py:507] global step 1387: loss = 1.0146 (1.302 sec/step)\n",
            "I0919 17:01:33.948865 140474388072320 learning.py:507] global step 1388: loss = 1.1461 (1.319 sec/step)\n",
            "I0919 17:01:35.241472 140474388072320 learning.py:507] global step 1389: loss = 1.0216 (1.291 sec/step)\n",
            "I0919 17:01:36.567713 140474388072320 learning.py:507] global step 1390: loss = 1.2633 (1.325 sec/step)\n",
            "I0919 17:01:37.888278 140474388072320 learning.py:507] global step 1391: loss = 1.0494 (1.319 sec/step)\n",
            "I0919 17:01:39.189016 140474388072320 learning.py:507] global step 1392: loss = 1.0201 (1.299 sec/step)\n",
            "I0919 17:01:40.461934 140474388072320 learning.py:507] global step 1393: loss = 1.0667 (1.271 sec/step)\n",
            "I0919 17:01:41.822065 140474388072320 learning.py:507] global step 1394: loss = 1.2050 (1.358 sec/step)\n",
            "I0919 17:01:43.121921 140474388072320 learning.py:507] global step 1395: loss = 1.0762 (1.298 sec/step)\n",
            "I0919 17:01:44.440411 140474388072320 learning.py:507] global step 1396: loss = 1.2679 (1.317 sec/step)\n",
            "I0919 17:01:45.743592 140474388072320 learning.py:507] global step 1397: loss = 1.2501 (1.302 sec/step)\n",
            "I0919 17:01:47.051170 140474388072320 learning.py:507] global step 1398: loss = 1.4823 (1.306 sec/step)\n",
            "I0919 17:01:48.344629 140474388072320 learning.py:507] global step 1399: loss = 1.0666 (1.291 sec/step)\n",
            "I0919 17:01:49.624809 140474388072320 learning.py:507] global step 1400: loss = 0.9853 (1.279 sec/step)\n",
            "I0919 17:01:50.926286 140474388072320 learning.py:507] global step 1401: loss = 1.4076 (1.300 sec/step)\n",
            "I0919 17:01:52.255181 140474388072320 learning.py:507] global step 1402: loss = 1.1469 (1.327 sec/step)\n",
            "I0919 17:01:53.623034 140474388072320 learning.py:507] global step 1403: loss = 1.3264 (1.366 sec/step)\n",
            "I0919 17:01:54.942078 140474388072320 learning.py:507] global step 1404: loss = 1.4789 (1.317 sec/step)\n",
            "I0919 17:01:56.279401 140474388072320 learning.py:507] global step 1405: loss = 0.9215 (1.336 sec/step)\n",
            "I0919 17:01:57.599761 140474388072320 learning.py:507] global step 1406: loss = 0.9598 (1.319 sec/step)\n",
            "I0919 17:01:58.892867 140474388072320 learning.py:507] global step 1407: loss = 1.0005 (1.291 sec/step)\n",
            "I0919 17:02:00.185307 140474388072320 learning.py:507] global step 1408: loss = 1.2710 (1.291 sec/step)\n",
            "I0919 17:02:01.489588 140474388072320 learning.py:507] global step 1409: loss = 1.0007 (1.303 sec/step)\n",
            "I0919 17:02:02.777482 140474388072320 learning.py:507] global step 1410: loss = 1.0604 (1.286 sec/step)\n",
            "I0919 17:02:04.144608 140474388072320 learning.py:507] global step 1411: loss = 1.0366 (1.366 sec/step)\n",
            "I0919 17:02:05.473253 140474388072320 learning.py:507] global step 1412: loss = 1.0567 (1.327 sec/step)\n",
            "I0919 17:02:06.761358 140474388072320 learning.py:507] global step 1413: loss = 0.8515 (1.286 sec/step)\n",
            "I0919 17:02:08.116555 140474388072320 learning.py:507] global step 1414: loss = 0.9123 (1.354 sec/step)\n",
            "I0919 17:02:09.461635 140474388072320 learning.py:507] global step 1415: loss = 1.0894 (1.343 sec/step)\n",
            "I0919 17:02:10.786802 140474388072320 learning.py:507] global step 1416: loss = 1.0558 (1.324 sec/step)\n",
            "I0919 17:02:12.082027 140474388072320 learning.py:507] global step 1417: loss = 1.0050 (1.294 sec/step)\n",
            "I0919 17:02:13.401824 140474388072320 learning.py:507] global step 1418: loss = 1.1404 (1.318 sec/step)\n",
            "I0919 17:02:14.766669 140474388072320 learning.py:507] global step 1419: loss = 1.1693 (1.363 sec/step)\n",
            "I0919 17:02:16.064521 140474388072320 learning.py:507] global step 1420: loss = 1.1301 (1.296 sec/step)\n",
            "I0919 17:02:17.383277 140474388072320 learning.py:507] global step 1421: loss = 0.9267 (1.317 sec/step)\n",
            "I0919 17:02:18.687356 140474388072320 learning.py:507] global step 1422: loss = 0.9276 (1.302 sec/step)\n",
            "I0919 17:02:19.996448 140474388072320 learning.py:507] global step 1423: loss = 1.0066 (1.307 sec/step)\n",
            "I0919 17:02:21.350481 140474388072320 learning.py:507] global step 1424: loss = 1.3941 (1.352 sec/step)\n",
            "I0919 17:02:22.637681 140474388072320 learning.py:507] global step 1425: loss = 1.1151 (1.285 sec/step)\n",
            "I0919 17:02:23.952162 140474388072320 learning.py:507] global step 1426: loss = 1.0398 (1.313 sec/step)\n",
            "I0919 17:02:25.276895 140474388072320 learning.py:507] global step 1427: loss = 1.2944 (1.323 sec/step)\n",
            "I0919 17:02:26.575062 140474388072320 learning.py:507] global step 1428: loss = 0.8850 (1.296 sec/step)\n",
            "I0919 17:02:28.831743 140474388072320 learning.py:507] global step 1429: loss = 1.0501 (2.254 sec/step)\n",
            "I0919 17:02:29.142743 140471297337088 supervisor.py:1050] Recording summary at step 1429.\n",
            "I0919 17:02:30.162930 140474388072320 learning.py:507] global step 1430: loss = 1.1508 (1.329 sec/step)\n",
            "I0919 17:02:31.468312 140474388072320 learning.py:507] global step 1431: loss = 0.9493 (1.304 sec/step)\n",
            "I0919 17:02:32.777158 140474388072320 learning.py:507] global step 1432: loss = 1.0031 (1.307 sec/step)\n",
            "I0919 17:02:34.072954 140474388072320 learning.py:507] global step 1433: loss = 0.9655 (1.294 sec/step)\n",
            "I0919 17:02:35.400326 140474388072320 learning.py:507] global step 1434: loss = 0.9403 (1.326 sec/step)\n",
            "I0919 17:02:36.696873 140474388072320 learning.py:507] global step 1435: loss = 0.9122 (1.295 sec/step)\n",
            "I0919 17:02:38.021497 140474388072320 learning.py:507] global step 1436: loss = 1.0428 (1.323 sec/step)\n",
            "I0919 17:02:39.336237 140474388072320 learning.py:507] global step 1437: loss = 1.1781 (1.313 sec/step)\n",
            "I0919 17:02:40.634340 140474388072320 learning.py:507] global step 1438: loss = 1.1605 (1.296 sec/step)\n",
            "I0919 17:02:41.984658 140474388072320 learning.py:507] global step 1439: loss = 0.9829 (1.348 sec/step)\n",
            "I0919 17:02:43.302979 140474388072320 learning.py:507] global step 1440: loss = 1.3035 (1.316 sec/step)\n",
            "I0919 17:02:44.602967 140474388072320 learning.py:507] global step 1441: loss = 1.2230 (1.298 sec/step)\n",
            "I0919 17:02:45.906222 140474388072320 learning.py:507] global step 1442: loss = 1.0133 (1.301 sec/step)\n",
            "I0919 17:02:47.202606 140474388072320 learning.py:507] global step 1443: loss = 1.0575 (1.294 sec/step)\n",
            "I0919 17:02:48.477684 140474388072320 learning.py:507] global step 1444: loss = 1.1097 (1.273 sec/step)\n",
            "I0919 17:02:49.784056 140474388072320 learning.py:507] global step 1445: loss = 1.4360 (1.305 sec/step)\n",
            "I0919 17:02:51.119996 140474388072320 learning.py:507] global step 1446: loss = 1.0938 (1.334 sec/step)\n",
            "I0919 17:02:52.416184 140474388072320 learning.py:507] global step 1447: loss = 0.8712 (1.295 sec/step)\n",
            "I0919 17:02:53.725831 140474388072320 learning.py:507] global step 1448: loss = 1.1163 (1.308 sec/step)\n",
            "I0919 17:02:55.018990 140474388072320 learning.py:507] global step 1449: loss = 0.9534 (1.291 sec/step)\n",
            "I0919 17:02:56.339840 140474388072320 learning.py:507] global step 1450: loss = 1.4492 (1.319 sec/step)\n",
            "I0919 17:02:57.657907 140474388072320 learning.py:507] global step 1451: loss = 0.9098 (1.316 sec/step)\n",
            "I0919 17:02:58.960441 140474388072320 learning.py:507] global step 1452: loss = 1.0431 (1.301 sec/step)\n",
            "I0919 17:03:00.264384 140474388072320 learning.py:507] global step 1453: loss = 1.2664 (1.302 sec/step)\n",
            "I0919 17:03:01.553345 140474388072320 learning.py:507] global step 1454: loss = 0.9768 (1.287 sec/step)\n",
            "I0919 17:03:02.881493 140474388072320 learning.py:507] global step 1455: loss = 1.0565 (1.326 sec/step)\n",
            "I0919 17:03:04.236838 140474388072320 learning.py:507] global step 1456: loss = 0.9932 (1.353 sec/step)\n",
            "I0919 17:03:05.585777 140474388072320 learning.py:507] global step 1457: loss = 1.0413 (1.347 sec/step)\n",
            "I0919 17:03:06.909929 140474388072320 learning.py:507] global step 1458: loss = 0.8957 (1.322 sec/step)\n",
            "I0919 17:03:08.233649 140474388072320 learning.py:507] global step 1459: loss = 1.1666 (1.322 sec/step)\n",
            "I0919 17:03:09.535615 140474388072320 learning.py:507] global step 1460: loss = 1.1008 (1.300 sec/step)\n",
            "I0919 17:03:10.870723 140474388072320 learning.py:507] global step 1461: loss = 1.0190 (1.333 sec/step)\n",
            "I0919 17:03:12.154643 140474388072320 learning.py:507] global step 1462: loss = 1.1270 (1.282 sec/step)\n",
            "I0919 17:03:13.443831 140474388072320 learning.py:507] global step 1463: loss = 1.0793 (1.288 sec/step)\n",
            "I0919 17:03:14.770643 140474388072320 learning.py:507] global step 1464: loss = 1.2273 (1.325 sec/step)\n",
            "I0919 17:03:16.055597 140474388072320 learning.py:507] global step 1465: loss = 1.0676 (1.283 sec/step)\n",
            "I0919 17:03:17.348313 140474388072320 learning.py:507] global step 1466: loss = 1.2833 (1.291 sec/step)\n",
            "I0919 17:03:18.630763 140474388072320 learning.py:507] global step 1467: loss = 0.9973 (1.281 sec/step)\n",
            "I0919 17:03:19.972896 140474388072320 learning.py:507] global step 1468: loss = 1.1811 (1.340 sec/step)\n",
            "I0919 17:03:21.297674 140474388072320 learning.py:507] global step 1469: loss = 1.5922 (1.323 sec/step)\n",
            "I0919 17:03:22.657223 140474388072320 learning.py:507] global step 1470: loss = 1.1691 (1.358 sec/step)\n",
            "I0919 17:03:23.986454 140474388072320 learning.py:507] global step 1471: loss = 0.9489 (1.328 sec/step)\n",
            "I0919 17:03:25.274939 140474388072320 learning.py:507] global step 1472: loss = 1.0451 (1.287 sec/step)\n",
            "I0919 17:03:26.581248 140474388072320 learning.py:507] global step 1473: loss = 1.0832 (1.305 sec/step)\n",
            "I0919 17:03:27.877039 140474388072320 learning.py:507] global step 1474: loss = 1.1849 (1.294 sec/step)\n",
            "I0919 17:03:29.147185 140474388072320 learning.py:507] global step 1475: loss = 1.1719 (1.269 sec/step)\n",
            "I0919 17:03:30.434520 140474388072320 learning.py:507] global step 1476: loss = 0.9899 (1.286 sec/step)\n",
            "I0919 17:03:31.762176 140474388072320 learning.py:507] global step 1477: loss = 1.0336 (1.325 sec/step)\n",
            "I0919 17:03:33.066363 140474388072320 learning.py:507] global step 1478: loss = 1.3119 (1.302 sec/step)\n",
            "I0919 17:03:34.399179 140474388072320 learning.py:507] global step 1479: loss = 1.1498 (1.331 sec/step)\n",
            "I0919 17:03:35.700897 140474388072320 learning.py:507] global step 1480: loss = 0.9034 (1.300 sec/step)\n",
            "I0919 17:03:36.997219 140474388072320 learning.py:507] global step 1481: loss = 1.3475 (1.294 sec/step)\n",
            "I0919 17:03:38.321053 140474388072320 learning.py:507] global step 1482: loss = 1.0620 (1.322 sec/step)\n",
            "I0919 17:03:39.633959 140474388072320 learning.py:507] global step 1483: loss = 1.3419 (1.311 sec/step)\n",
            "I0919 17:03:40.954945 140474388072320 learning.py:507] global step 1484: loss = 1.1764 (1.319 sec/step)\n",
            "I0919 17:03:42.258172 140474388072320 learning.py:507] global step 1485: loss = 0.8208 (1.301 sec/step)\n",
            "I0919 17:03:43.585190 140474388072320 learning.py:507] global step 1486: loss = 0.9658 (1.325 sec/step)\n",
            "I0919 17:03:44.884141 140474388072320 learning.py:507] global step 1487: loss = 1.2463 (1.297 sec/step)\n",
            "I0919 17:03:46.214168 140474388072320 learning.py:507] global step 1488: loss = 1.3617 (1.328 sec/step)\n",
            "I0919 17:03:47.553459 140474388072320 learning.py:507] global step 1489: loss = 1.5171 (1.338 sec/step)\n",
            "I0919 17:03:48.892025 140474388072320 learning.py:507] global step 1490: loss = 1.5765 (1.337 sec/step)\n",
            "I0919 17:03:50.182568 140474388072320 learning.py:507] global step 1491: loss = 0.9699 (1.289 sec/step)\n",
            "I0919 17:03:51.553136 140474388072320 learning.py:507] global step 1492: loss = 1.0085 (1.369 sec/step)\n",
            "I0919 17:03:52.849343 140474388072320 learning.py:507] global step 1493: loss = 1.2271 (1.295 sec/step)\n",
            "I0919 17:03:54.159610 140474388072320 learning.py:507] global step 1494: loss = 1.0465 (1.309 sec/step)\n",
            "I0919 17:03:55.459180 140474388072320 learning.py:507] global step 1495: loss = 1.0543 (1.298 sec/step)\n",
            "I0919 17:03:56.802320 140474388072320 learning.py:507] global step 1496: loss = 1.2197 (1.342 sec/step)\n",
            "I0919 17:03:58.122020 140474388072320 learning.py:507] global step 1497: loss = 1.1430 (1.318 sec/step)\n",
            "I0919 17:03:59.455761 140474388072320 learning.py:507] global step 1498: loss = 1.4805 (1.332 sec/step)\n",
            "I0919 17:04:00.763506 140474388072320 learning.py:507] global step 1499: loss = 1.0992 (1.306 sec/step)\n",
            "I0919 17:04:02.085169 140474388072320 learning.py:507] global step 1500: loss = 1.1441 (1.319 sec/step)\n",
            "I0919 17:04:03.404520 140474388072320 learning.py:507] global step 1501: loss = 1.0049 (1.318 sec/step)\n",
            "I0919 17:04:04.731904 140474388072320 learning.py:507] global step 1502: loss = 1.1616 (1.326 sec/step)\n",
            "I0919 17:04:06.047722 140474388072320 learning.py:507] global step 1503: loss = 1.0046 (1.314 sec/step)\n",
            "I0919 17:04:07.344813 140474388072320 learning.py:507] global step 1504: loss = 1.2464 (1.295 sec/step)\n",
            "I0919 17:04:08.701091 140474388072320 learning.py:507] global step 1505: loss = 0.9424 (1.355 sec/step)\n",
            "I0919 17:04:10.025938 140474388072320 learning.py:507] global step 1506: loss = 1.3669 (1.323 sec/step)\n",
            "I0919 17:04:11.302364 140474388072320 learning.py:507] global step 1507: loss = 0.8721 (1.275 sec/step)\n",
            "I0919 17:04:12.632668 140474388072320 learning.py:507] global step 1508: loss = 1.2486 (1.329 sec/step)\n",
            "I0919 17:04:13.932708 140474388072320 learning.py:507] global step 1509: loss = 0.9713 (1.298 sec/step)\n",
            "I0919 17:04:15.274723 140474388072320 learning.py:507] global step 1510: loss = 1.0766 (1.340 sec/step)\n",
            "I0919 17:04:16.586672 140474388072320 learning.py:507] global step 1511: loss = 0.9485 (1.310 sec/step)\n",
            "I0919 17:04:17.926570 140474388072320 learning.py:507] global step 1512: loss = 0.9654 (1.338 sec/step)\n",
            "I0919 17:04:19.225744 140474388072320 learning.py:507] global step 1513: loss = 1.0970 (1.297 sec/step)\n",
            "I0919 17:04:20.542859 140474388072320 learning.py:507] global step 1514: loss = 0.9423 (1.315 sec/step)\n",
            "I0919 17:04:21.893287 140474388072320 learning.py:507] global step 1515: loss = 1.1222 (1.349 sec/step)\n",
            "I0919 17:04:23.218102 140474388072320 learning.py:507] global step 1516: loss = 1.1585 (1.323 sec/step)\n",
            "I0919 17:04:24.506992 140474388072320 learning.py:507] global step 1517: loss = 1.0572 (1.287 sec/step)\n",
            "I0919 17:04:25.827632 140474388072320 learning.py:507] global step 1518: loss = 0.9852 (1.319 sec/step)\n",
            "I0919 17:04:27.157104 140474388072320 learning.py:507] global step 1519: loss = 1.1983 (1.328 sec/step)\n",
            "I0919 17:04:28.912301 140471297337088 supervisor.py:1050] Recording summary at step 1519.\n",
            "I0919 17:04:29.366487 140474388072320 learning.py:507] global step 1520: loss = 0.9334 (2.207 sec/step)\n",
            "I0919 17:04:30.654072 140474388072320 learning.py:507] global step 1521: loss = 1.1780 (1.286 sec/step)\n",
            "I0919 17:04:31.966257 140474388072320 learning.py:507] global step 1522: loss = 1.1137 (1.311 sec/step)\n",
            "I0919 17:04:33.296760 140474388072320 learning.py:507] global step 1523: loss = 1.2864 (1.328 sec/step)\n",
            "I0919 17:04:34.574733 140474388072320 learning.py:507] global step 1524: loss = 1.1157 (1.276 sec/step)\n",
            "I0919 17:04:35.868882 140474388072320 learning.py:507] global step 1525: loss = 1.0838 (1.292 sec/step)\n",
            "I0919 17:04:37.176846 140474388072320 learning.py:507] global step 1526: loss = 1.2016 (1.306 sec/step)\n",
            "I0919 17:04:38.464673 140474388072320 learning.py:507] global step 1527: loss = 1.0976 (1.286 sec/step)\n",
            "I0919 17:04:39.754416 140474388072320 learning.py:507] global step 1528: loss = 0.9737 (1.288 sec/step)\n",
            "I0919 17:04:41.038516 140474388072320 learning.py:507] global step 1529: loss = 1.0626 (1.282 sec/step)\n",
            "I0919 17:04:42.315970 140474388072320 learning.py:507] global step 1530: loss = 1.0196 (1.276 sec/step)\n",
            "I0919 17:04:43.643841 140474388072320 learning.py:507] global step 1531: loss = 1.0728 (1.326 sec/step)\n",
            "I0919 17:04:44.987234 140474388072320 learning.py:507] global step 1532: loss = 0.8443 (1.342 sec/step)\n",
            "I0919 17:04:46.286386 140474388072320 learning.py:507] global step 1533: loss = 1.0496 (1.298 sec/step)\n",
            "I0919 17:04:47.618213 140474388072320 learning.py:507] global step 1534: loss = 1.2234 (1.330 sec/step)\n",
            "I0919 17:04:48.899243 140474388072320 learning.py:507] global step 1535: loss = 1.1956 (1.279 sec/step)\n",
            "I0919 17:04:50.220459 140474388072320 learning.py:507] global step 1536: loss = 0.8778 (1.320 sec/step)\n",
            "I0919 17:04:51.545440 140474388072320 learning.py:507] global step 1537: loss = 1.0612 (1.323 sec/step)\n",
            "I0919 17:04:52.851421 140474388072320 learning.py:507] global step 1538: loss = 1.0511 (1.304 sec/step)\n",
            "I0919 17:04:54.164838 140474388072320 learning.py:507] global step 1539: loss = 1.1571 (1.312 sec/step)\n",
            "I0919 17:04:55.459870 140474388072320 learning.py:507] global step 1540: loss = 1.1879 (1.293 sec/step)\n",
            "I0919 17:04:56.755575 140474388072320 learning.py:507] global step 1541: loss = 0.8807 (1.292 sec/step)\n",
            "I0919 17:04:58.046885 140474388072320 learning.py:507] global step 1542: loss = 1.1212 (1.289 sec/step)\n",
            "I0919 17:04:59.321263 140474388072320 learning.py:507] global step 1543: loss = 1.0837 (1.273 sec/step)\n",
            "I0919 17:05:00.641668 140474388072320 learning.py:507] global step 1544: loss = 0.9364 (1.318 sec/step)\n",
            "I0919 17:05:01.946202 140474388072320 learning.py:507] global step 1545: loss = 1.3020 (1.303 sec/step)\n",
            "I0919 17:05:03.222716 140474388072320 learning.py:507] global step 1546: loss = 0.8945 (1.275 sec/step)\n",
            "I0919 17:05:04.556258 140474388072320 learning.py:507] global step 1547: loss = 1.1938 (1.332 sec/step)\n",
            "I0919 17:05:05.869316 140474388072320 learning.py:507] global step 1548: loss = 1.6554 (1.311 sec/step)\n",
            "I0919 17:05:07.178299 140474388072320 learning.py:507] global step 1549: loss = 1.0506 (1.307 sec/step)\n",
            "I0919 17:05:08.518660 140474388072320 learning.py:507] global step 1550: loss = 1.0204 (1.339 sec/step)\n",
            "I0919 17:05:09.818536 140474388072320 learning.py:507] global step 1551: loss = 1.0898 (1.298 sec/step)\n",
            "I0919 17:05:11.146285 140474388072320 learning.py:507] global step 1552: loss = 1.4101 (1.326 sec/step)\n",
            "I0919 17:05:12.446502 140474388072320 learning.py:507] global step 1553: loss = 1.1812 (1.299 sec/step)\n",
            "I0919 17:05:13.776885 140474388072320 learning.py:507] global step 1554: loss = 1.0545 (1.328 sec/step)\n",
            "I0919 17:05:15.158983 140474388072320 learning.py:507] global step 1555: loss = 1.0953 (1.380 sec/step)\n",
            "I0919 17:05:16.483447 140474388072320 learning.py:507] global step 1556: loss = 1.1896 (1.323 sec/step)\n",
            "I0919 17:05:17.797415 140474388072320 learning.py:507] global step 1557: loss = 0.9163 (1.312 sec/step)\n",
            "I0919 17:05:19.109014 140474388072320 learning.py:507] global step 1558: loss = 1.1560 (1.310 sec/step)\n",
            "I0919 17:05:20.435420 140474388072320 learning.py:507] global step 1559: loss = 0.9510 (1.325 sec/step)\n",
            "I0919 17:05:21.782958 140474388072320 learning.py:507] global step 1560: loss = 1.1849 (1.346 sec/step)\n",
            "I0919 17:05:23.132154 140474388072320 learning.py:507] global step 1561: loss = 1.8595 (1.347 sec/step)\n",
            "I0919 17:05:24.442994 140474388072320 learning.py:507] global step 1562: loss = 1.2669 (1.309 sec/step)\n",
            "I0919 17:05:25.757700 140474388072320 learning.py:507] global step 1563: loss = 0.9042 (1.313 sec/step)\n",
            "I0919 17:05:27.099406 140474388072320 learning.py:507] global step 1564: loss = 0.8272 (1.340 sec/step)\n",
            "I0919 17:05:28.397588 140474388072320 learning.py:507] global step 1565: loss = 1.1835 (1.296 sec/step)\n",
            "I0919 17:05:29.706042 140474388072320 learning.py:507] global step 1566: loss = 1.4157 (1.307 sec/step)\n",
            "I0919 17:05:30.998282 140474388072320 learning.py:507] global step 1567: loss = 1.1012 (1.290 sec/step)\n",
            "I0919 17:05:32.319539 140474388072320 learning.py:507] global step 1568: loss = 1.2274 (1.319 sec/step)\n",
            "I0919 17:05:33.649274 140474388072320 learning.py:507] global step 1569: loss = 1.1646 (1.328 sec/step)\n",
            "I0919 17:05:34.956588 140474388072320 learning.py:507] global step 1570: loss = 1.2100 (1.306 sec/step)\n",
            "I0919 17:05:36.281539 140474388072320 learning.py:507] global step 1571: loss = 1.0966 (1.323 sec/step)\n",
            "I0919 17:05:37.589421 140474388072320 learning.py:507] global step 1572: loss = 1.0964 (1.306 sec/step)\n",
            "I0919 17:05:38.906196 140474388072320 learning.py:507] global step 1573: loss = 0.9575 (1.315 sec/step)\n",
            "I0919 17:05:40.239717 140474388072320 learning.py:507] global step 1574: loss = 0.9740 (1.332 sec/step)\n",
            "I0919 17:05:41.569694 140474388072320 learning.py:507] global step 1575: loss = 1.0266 (1.328 sec/step)\n",
            "I0919 17:05:42.908356 140474388072320 learning.py:507] global step 1576: loss = 1.0001 (1.337 sec/step)\n",
            "I0919 17:05:44.223495 140474388072320 learning.py:507] global step 1577: loss = 1.0037 (1.313 sec/step)\n",
            "I0919 17:05:45.571034 140474388072320 learning.py:507] global step 1578: loss = 1.0395 (1.346 sec/step)\n",
            "I0919 17:05:46.949055 140474388072320 learning.py:507] global step 1579: loss = 0.9456 (1.376 sec/step)\n",
            "I0919 17:05:48.266194 140474388072320 learning.py:507] global step 1580: loss = 1.2064 (1.315 sec/step)\n",
            "I0919 17:05:49.594393 140474388072320 learning.py:507] global step 1581: loss = 1.0903 (1.326 sec/step)\n",
            "I0919 17:05:50.932044 140474388072320 learning.py:507] global step 1582: loss = 1.0345 (1.336 sec/step)\n",
            "I0919 17:05:52.285413 140474388072320 learning.py:507] global step 1583: loss = 0.8959 (1.351 sec/step)\n",
            "I0919 17:05:53.653244 140474388072320 learning.py:507] global step 1584: loss = 1.3595 (1.366 sec/step)\n",
            "I0919 17:05:54.972429 140474388072320 learning.py:507] global step 1585: loss = 1.2063 (1.317 sec/step)\n",
            "I0919 17:05:56.282243 140474388072320 learning.py:507] global step 1586: loss = 0.9920 (1.308 sec/step)\n",
            "I0919 17:05:57.623456 140474388072320 learning.py:507] global step 1587: loss = 1.5887 (1.339 sec/step)\n",
            "I0919 17:05:58.974607 140474388072320 learning.py:507] global step 1588: loss = 1.0258 (1.349 sec/step)\n",
            "I0919 17:06:00.314500 140474388072320 learning.py:507] global step 1589: loss = 1.2097 (1.338 sec/step)\n",
            "I0919 17:06:01.623577 140474388072320 learning.py:507] global step 1590: loss = 1.1463 (1.307 sec/step)\n",
            "I0919 17:06:02.938509 140474388072320 learning.py:507] global step 1591: loss = 1.1402 (1.313 sec/step)\n",
            "I0919 17:06:04.264465 140474388072320 learning.py:507] global step 1592: loss = 1.1663 (1.324 sec/step)\n",
            "I0919 17:06:05.581254 140474388072320 learning.py:507] global step 1593: loss = 1.0828 (1.315 sec/step)\n",
            "I0919 17:06:06.896248 140474388072320 learning.py:507] global step 1594: loss = 1.0261 (1.313 sec/step)\n",
            "I0919 17:06:08.251550 140474388072320 learning.py:507] global step 1595: loss = 1.0430 (1.353 sec/step)\n",
            "I0919 17:06:09.586760 140474388072320 learning.py:507] global step 1596: loss = 1.1848 (1.333 sec/step)\n",
            "I0919 17:06:10.885921 140474388072320 learning.py:507] global step 1597: loss = 1.1790 (1.297 sec/step)\n",
            "I0919 17:06:12.196455 140474388072320 learning.py:507] global step 1598: loss = 1.1927 (1.309 sec/step)\n",
            "I0919 17:06:13.530390 140474388072320 learning.py:507] global step 1599: loss = 0.9716 (1.332 sec/step)\n",
            "I0919 17:06:14.839690 140474388072320 learning.py:507] global step 1600: loss = 0.9710 (1.307 sec/step)\n",
            "I0919 17:06:16.153659 140474388072320 learning.py:507] global step 1601: loss = 1.0123 (1.312 sec/step)\n",
            "I0919 17:06:17.471497 140474388072320 learning.py:507] global step 1602: loss = 1.2412 (1.316 sec/step)\n",
            "I0919 17:06:18.809499 140474388072320 learning.py:507] global step 1603: loss = 1.1846 (1.336 sec/step)\n",
            "I0919 17:06:20.124608 140474388072320 learning.py:507] global step 1604: loss = 1.0361 (1.313 sec/step)\n",
            "I0919 17:06:21.449364 140474388072320 learning.py:507] global step 1605: loss = 0.8939 (1.323 sec/step)\n",
            "I0919 17:06:22.757077 140474388072320 learning.py:507] global step 1606: loss = 1.1900 (1.306 sec/step)\n",
            "I0919 17:06:24.079849 140474388072320 learning.py:507] global step 1607: loss = 1.0720 (1.321 sec/step)\n",
            "I0919 17:06:25.373806 140474388072320 learning.py:507] global step 1608: loss = 0.9076 (1.292 sec/step)\n",
            "I0919 17:06:26.749650 140474388072320 learning.py:507] global step 1609: loss = 1.4082 (1.374 sec/step)\n",
            "I0919 17:06:28.856684 140474388072320 learning.py:507] global step 1610: loss = 0.9958 (2.059 sec/step)\n",
            "I0919 17:06:29.154144 140471297337088 supervisor.py:1050] Recording summary at step 1610.\n",
            "I0919 17:06:30.251899 140474388072320 learning.py:507] global step 1611: loss = 1.2180 (1.393 sec/step)\n",
            "I0919 17:06:31.579030 140474388072320 learning.py:507] global step 1612: loss = 0.9571 (1.325 sec/step)\n",
            "I0919 17:06:32.901464 140474388072320 learning.py:507] global step 1613: loss = 0.9521 (1.320 sec/step)\n",
            "I0919 17:06:34.239244 140474388072320 learning.py:507] global step 1614: loss = 1.1928 (1.336 sec/step)\n",
            "I0919 17:06:35.514532 140474388072320 learning.py:507] global step 1615: loss = 1.1205 (1.273 sec/step)\n",
            "I0919 17:06:36.836846 140474388072320 learning.py:507] global step 1616: loss = 1.0702 (1.320 sec/step)\n",
            "I0919 17:06:38.153774 140474388072320 learning.py:507] global step 1617: loss = 1.1518 (1.315 sec/step)\n",
            "I0919 17:06:39.464541 140474388072320 learning.py:507] global step 1618: loss = 1.5442 (1.309 sec/step)\n",
            "I0919 17:06:40.764398 140474388072320 learning.py:507] global step 1619: loss = 0.9720 (1.298 sec/step)\n",
            "I0919 17:06:42.097302 140474388072320 learning.py:507] global step 1620: loss = 1.0168 (1.331 sec/step)\n",
            "I0919 17:06:43.407140 140474388072320 learning.py:507] global step 1621: loss = 0.9367 (1.308 sec/step)\n",
            "I0919 17:06:44.688061 140474388072320 learning.py:507] global step 1622: loss = 1.0689 (1.279 sec/step)\n",
            "I0919 17:06:45.985534 140474388072320 learning.py:507] global step 1623: loss = 0.9202 (1.296 sec/step)\n",
            "I0919 17:06:47.283427 140474388072320 learning.py:507] global step 1624: loss = 1.1615 (1.296 sec/step)\n",
            "I0919 17:06:48.604398 140474388072320 learning.py:507] global step 1625: loss = 0.8398 (1.319 sec/step)\n",
            "I0919 17:06:49.896311 140474388072320 learning.py:507] global step 1626: loss = 1.0648 (1.290 sec/step)\n",
            "I0919 17:06:51.199078 140474388072320 learning.py:507] global step 1627: loss = 1.1885 (1.301 sec/step)\n",
            "I0919 17:06:52.506548 140474388072320 learning.py:507] global step 1628: loss = 1.1807 (1.305 sec/step)\n",
            "I0919 17:06:53.827951 140474388072320 learning.py:507] global step 1629: loss = 0.8377 (1.318 sec/step)\n",
            "I0919 17:06:55.126023 140474388072320 learning.py:507] global step 1630: loss = 1.1818 (1.296 sec/step)\n",
            "I0919 17:06:56.462230 140474388072320 learning.py:507] global step 1631: loss = 0.9476 (1.334 sec/step)\n",
            "I0919 17:06:57.832026 140474388072320 learning.py:507] global step 1632: loss = 1.1275 (1.368 sec/step)\n",
            "I0919 17:06:59.157993 140474388072320 learning.py:507] global step 1633: loss = 1.0969 (1.324 sec/step)\n",
            "I0919 17:07:00.471179 140474388072320 learning.py:507] global step 1634: loss = 1.1163 (1.311 sec/step)\n",
            "I0919 17:07:01.798707 140474388072320 learning.py:507] global step 1635: loss = 1.1907 (1.326 sec/step)\n",
            "I0919 17:07:03.138675 140474388072320 learning.py:507] global step 1636: loss = 1.0575 (1.338 sec/step)\n",
            "I0919 17:07:04.470254 140474388072320 learning.py:507] global step 1637: loss = 1.5057 (1.330 sec/step)\n",
            "I0919 17:07:05.816195 140474388072320 learning.py:507] global step 1638: loss = 1.0554 (1.344 sec/step)\n",
            "I0919 17:07:07.170850 140474388072320 learning.py:507] global step 1639: loss = 0.9461 (1.353 sec/step)\n",
            "I0919 17:07:08.565325 140474388072320 learning.py:507] global step 1640: loss = 0.8846 (1.393 sec/step)\n",
            "I0919 17:07:09.892630 140474388072320 learning.py:507] global step 1641: loss = 1.1550 (1.326 sec/step)\n",
            "I0919 17:07:11.210284 140474388072320 learning.py:507] global step 1642: loss = 1.2157 (1.316 sec/step)\n",
            "I0919 17:07:12.530893 140474388072320 learning.py:507] global step 1643: loss = 1.0316 (1.319 sec/step)\n",
            "I0919 17:07:13.842715 140474388072320 learning.py:507] global step 1644: loss = 1.0100 (1.310 sec/step)\n",
            "I0919 17:07:15.188863 140474388072320 learning.py:507] global step 1645: loss = 1.0091 (1.344 sec/step)\n",
            "I0919 17:07:16.523394 140474388072320 learning.py:507] global step 1646: loss = 1.2866 (1.332 sec/step)\n",
            "I0919 17:07:17.858612 140474388072320 learning.py:507] global step 1647: loss = 1.0298 (1.333 sec/step)\n",
            "I0919 17:07:19.179388 140474388072320 learning.py:507] global step 1648: loss = 1.0350 (1.319 sec/step)\n",
            "I0919 17:07:20.479722 140474388072320 learning.py:507] global step 1649: loss = 0.9631 (1.299 sec/step)\n",
            "I0919 17:07:21.816816 140474388072320 learning.py:507] global step 1650: loss = 1.2251 (1.335 sec/step)\n",
            "I0919 17:07:23.170547 140474388072320 learning.py:507] global step 1651: loss = 0.8836 (1.351 sec/step)\n",
            "I0919 17:07:24.477204 140474388072320 learning.py:507] global step 1652: loss = 1.4352 (1.304 sec/step)\n",
            "I0919 17:07:25.797416 140474388072320 learning.py:507] global step 1653: loss = 1.0776 (1.319 sec/step)\n",
            "I0919 17:07:27.108473 140474388072320 learning.py:507] global step 1654: loss = 0.9168 (1.309 sec/step)\n",
            "I0919 17:07:28.475338 140474388072320 learning.py:507] global step 1655: loss = 0.9049 (1.365 sec/step)\n",
            "I0919 17:07:29.817176 140474388072320 learning.py:507] global step 1656: loss = 0.8365 (1.340 sec/step)\n",
            "I0919 17:07:31.120403 140474388072320 learning.py:507] global step 1657: loss = 0.9065 (1.301 sec/step)\n",
            "I0919 17:07:32.419018 140474388072320 learning.py:507] global step 1658: loss = 0.9915 (1.297 sec/step)\n",
            "I0919 17:07:33.754242 140474388072320 learning.py:507] global step 1659: loss = 1.0922 (1.334 sec/step)\n",
            "I0919 17:07:35.066884 140474388072320 learning.py:507] global step 1660: loss = 0.9762 (1.311 sec/step)\n",
            "I0919 17:07:36.434411 140474388072320 learning.py:507] global step 1661: loss = 1.1345 (1.366 sec/step)\n",
            "I0919 17:07:37.745948 140474388072320 learning.py:507] global step 1662: loss = 0.8261 (1.310 sec/step)\n",
            "I0919 17:07:39.074233 140474388072320 learning.py:507] global step 1663: loss = 0.8873 (1.327 sec/step)\n",
            "I0919 17:07:40.419063 140474388072320 learning.py:507] global step 1664: loss = 1.1156 (1.343 sec/step)\n",
            "I0919 17:07:41.741323 140474388072320 learning.py:507] global step 1665: loss = 0.7545 (1.320 sec/step)\n",
            "I0919 17:07:43.080646 140474388072320 learning.py:507] global step 1666: loss = 1.1202 (1.338 sec/step)\n",
            "I0919 17:07:44.395160 140474388072320 learning.py:507] global step 1667: loss = 1.3300 (1.313 sec/step)\n",
            "I0919 17:07:45.709631 140474388072320 learning.py:507] global step 1668: loss = 1.1648 (1.313 sec/step)\n",
            "I0919 17:07:47.021379 140474388072320 learning.py:507] global step 1669: loss = 1.1469 (1.310 sec/step)\n",
            "I0919 17:07:48.346055 140474388072320 learning.py:507] global step 1670: loss = 1.0823 (1.323 sec/step)\n",
            "I0919 17:07:49.618158 140474388072320 learning.py:507] global step 1671: loss = 1.2437 (1.270 sec/step)\n",
            "I0919 17:07:50.938247 140474388072320 learning.py:507] global step 1672: loss = 1.0751 (1.318 sec/step)\n",
            "I0919 17:07:52.243453 140474388072320 learning.py:507] global step 1673: loss = 0.8538 (1.304 sec/step)\n",
            "I0919 17:07:53.544347 140474388072320 learning.py:507] global step 1674: loss = 0.9253 (1.299 sec/step)\n",
            "I0919 17:07:54.847942 140474388072320 learning.py:507] global step 1675: loss = 0.8796 (1.302 sec/step)\n",
            "I0919 17:07:56.152159 140474388072320 learning.py:507] global step 1676: loss = 0.8068 (1.302 sec/step)\n",
            "I0919 17:07:57.486252 140474388072320 learning.py:507] global step 1677: loss = 0.9631 (1.332 sec/step)\n",
            "I0919 17:07:58.811101 140474388072320 learning.py:507] global step 1678: loss = 0.9565 (1.323 sec/step)\n",
            "I0919 17:08:00.109208 140474388072320 learning.py:507] global step 1679: loss = 1.0340 (1.296 sec/step)\n",
            "I0919 17:08:01.408560 140474388072320 learning.py:507] global step 1680: loss = 0.8871 (1.298 sec/step)\n",
            "I0919 17:08:02.729347 140474388072320 learning.py:507] global step 1681: loss = 1.0652 (1.319 sec/step)\n",
            "I0919 17:08:04.023046 140474388072320 learning.py:507] global step 1682: loss = 0.9448 (1.292 sec/step)\n",
            "I0919 17:08:05.304207 140474388072320 learning.py:507] global step 1683: loss = 1.0447 (1.279 sec/step)\n",
            "I0919 17:08:06.589968 140474388072320 learning.py:507] global step 1684: loss = 1.0119 (1.284 sec/step)\n",
            "I0919 17:08:07.927649 140474388072320 learning.py:507] global step 1685: loss = 1.0997 (1.336 sec/step)\n",
            "I0919 17:08:09.287370 140474388072320 learning.py:507] global step 1686: loss = 1.0702 (1.358 sec/step)\n",
            "I0919 17:08:10.612947 140474388072320 learning.py:507] global step 1687: loss = 1.1231 (1.324 sec/step)\n",
            "I0919 17:08:11.898706 140474388072320 learning.py:507] global step 1688: loss = 0.8910 (1.284 sec/step)\n",
            "I0919 17:08:13.199371 140474388072320 learning.py:507] global step 1689: loss = 0.9392 (1.299 sec/step)\n",
            "I0919 17:08:14.502393 140474388072320 learning.py:507] global step 1690: loss = 1.0867 (1.301 sec/step)\n",
            "I0919 17:08:15.813232 140474388072320 learning.py:507] global step 1691: loss = 0.8958 (1.309 sec/step)\n",
            "I0919 17:08:17.138230 140474388072320 learning.py:507] global step 1692: loss = 0.9461 (1.323 sec/step)\n",
            "I0919 17:08:18.451378 140474388072320 learning.py:507] global step 1693: loss = 1.0760 (1.312 sec/step)\n",
            "I0919 17:08:19.824611 140474388072320 learning.py:507] global step 1694: loss = 1.0090 (1.372 sec/step)\n",
            "I0919 17:08:21.135582 140474388072320 learning.py:507] global step 1695: loss = 1.5028 (1.309 sec/step)\n",
            "I0919 17:08:22.454869 140474388072320 learning.py:507] global step 1696: loss = 0.9618 (1.318 sec/step)\n",
            "I0919 17:08:23.738389 140474388072320 learning.py:507] global step 1697: loss = 1.1321 (1.282 sec/step)\n",
            "I0919 17:08:25.062099 140474388072320 learning.py:507] global step 1698: loss = 1.1933 (1.322 sec/step)\n",
            "I0919 17:08:26.382848 140474388072320 learning.py:507] global step 1699: loss = 0.9808 (1.318 sec/step)\n",
            "I0919 17:08:27.986601 140474388072320 learning.py:507] global step 1700: loss = 1.0288 (1.570 sec/step)\n",
            "I0919 17:08:29.351855 140471297337088 supervisor.py:1050] Recording summary at step 1700.\n",
            "I0919 17:08:29.914967 140474388072320 learning.py:507] global step 1701: loss = 1.1214 (1.926 sec/step)\n",
            "I0919 17:08:31.210768 140474388072320 learning.py:507] global step 1702: loss = 1.2607 (1.294 sec/step)\n",
            "I0919 17:08:32.512042 140474388072320 learning.py:507] global step 1703: loss = 1.0210 (1.300 sec/step)\n",
            "I0919 17:08:33.879271 140474388072320 learning.py:507] global step 1704: loss = 0.8846 (1.366 sec/step)\n",
            "I0919 17:08:35.205214 140474388072320 learning.py:507] global step 1705: loss = 0.9687 (1.324 sec/step)\n",
            "I0919 17:08:36.495850 140474388072320 learning.py:507] global step 1706: loss = 1.2280 (1.289 sec/step)\n",
            "I0919 17:08:37.848841 140474388072320 learning.py:507] global step 1707: loss = 1.0237 (1.351 sec/step)\n",
            "I0919 17:08:39.153552 140474388072320 learning.py:507] global step 1708: loss = 0.8028 (1.303 sec/step)\n",
            "I0919 17:08:40.496926 140474388072320 learning.py:507] global step 1709: loss = 1.0309 (1.342 sec/step)\n",
            "I0919 17:08:41.835005 140474388072320 learning.py:507] global step 1710: loss = 1.0053 (1.337 sec/step)\n",
            "I0919 17:08:43.159981 140474388072320 learning.py:507] global step 1711: loss = 1.0777 (1.323 sec/step)\n",
            "I0919 17:08:44.479844 140474388072320 learning.py:507] global step 1712: loss = 0.9818 (1.318 sec/step)\n",
            "I0919 17:08:45.780867 140474388072320 learning.py:507] global step 1713: loss = 1.1377 (1.299 sec/step)\n",
            "I0919 17:08:47.091753 140474388072320 learning.py:507] global step 1714: loss = 1.0381 (1.309 sec/step)\n",
            "I0919 17:08:48.402343 140474388072320 learning.py:507] global step 1715: loss = 1.0782 (1.309 sec/step)\n",
            "I0919 17:08:49.719830 140474388072320 learning.py:507] global step 1716: loss = 1.1528 (1.316 sec/step)\n",
            "I0919 17:08:51.009521 140474388072320 learning.py:507] global step 1717: loss = 1.0755 (1.288 sec/step)\n",
            "I0919 17:08:52.320964 140474388072320 learning.py:507] global step 1718: loss = 0.9780 (1.310 sec/step)\n",
            "I0919 17:08:53.662605 140474388072320 learning.py:507] global step 1719: loss = 1.4870 (1.340 sec/step)\n",
            "I0919 17:08:54.975266 140474388072320 learning.py:507] global step 1720: loss = 1.0820 (1.311 sec/step)\n",
            "I0919 17:08:56.286237 140474388072320 learning.py:507] global step 1721: loss = 1.0801 (1.309 sec/step)\n",
            "I0919 17:08:57.594676 140474388072320 learning.py:507] global step 1722: loss = 1.0887 (1.307 sec/step)\n",
            "I0919 17:08:58.899996 140474388072320 learning.py:507] global step 1723: loss = 0.8456 (1.304 sec/step)\n",
            "I0919 17:09:00.203903 140474388072320 learning.py:507] global step 1724: loss = 1.1270 (1.302 sec/step)\n",
            "I0919 17:09:01.549350 140474388072320 learning.py:507] global step 1725: loss = 1.0008 (1.344 sec/step)\n",
            "I0919 17:09:02.867635 140474388072320 learning.py:507] global step 1726: loss = 1.0816 (1.316 sec/step)\n",
            "I0919 17:09:04.203150 140474388072320 learning.py:507] global step 1727: loss = 1.0441 (1.334 sec/step)\n",
            "I0919 17:09:05.512974 140474388072320 learning.py:507] global step 1728: loss = 0.8793 (1.308 sec/step)\n",
            "I0919 17:09:06.807535 140474388072320 learning.py:507] global step 1729: loss = 0.9772 (1.293 sec/step)\n",
            "I0919 17:09:08.174750 140474388072320 learning.py:507] global step 1730: loss = 1.0264 (1.365 sec/step)\n",
            "I0919 17:09:09.517923 140474388072320 learning.py:507] global step 1731: loss = 1.1044 (1.341 sec/step)\n",
            "I0919 17:09:10.844180 140474388072320 learning.py:507] global step 1732: loss = 1.0116 (1.324 sec/step)\n",
            "I0919 17:09:12.138919 140474388072320 learning.py:507] global step 1733: loss = 0.8655 (1.293 sec/step)\n",
            "I0919 17:09:13.432246 140474388072320 learning.py:507] global step 1734: loss = 1.0463 (1.292 sec/step)\n",
            "I0919 17:09:14.758183 140474388072320 learning.py:507] global step 1735: loss = 0.9762 (1.324 sec/step)\n",
            "I0919 17:09:16.095050 140474388072320 learning.py:507] global step 1736: loss = 1.2209 (1.335 sec/step)\n",
            "I0919 17:09:17.389451 140474388072320 learning.py:507] global step 1737: loss = 0.9769 (1.293 sec/step)\n",
            "I0919 17:09:18.684484 140474388072320 learning.py:507] global step 1738: loss = 1.0888 (1.293 sec/step)\n",
            "I0919 17:09:19.988943 140474388072320 learning.py:507] global step 1739: loss = 1.0225 (1.302 sec/step)\n",
            "I0919 17:09:21.298439 140474388072320 learning.py:507] global step 1740: loss = 0.7220 (1.308 sec/step)\n",
            "I0919 17:09:22.578310 140474388072320 learning.py:507] global step 1741: loss = 1.0820 (1.278 sec/step)\n",
            "I0919 17:09:23.898369 140474388072320 learning.py:507] global step 1742: loss = 0.9877 (1.318 sec/step)\n",
            "I0919 17:09:25.198616 140474388072320 learning.py:507] global step 1743: loss = 0.9915 (1.299 sec/step)\n",
            "I0919 17:09:26.494330 140474388072320 learning.py:507] global step 1744: loss = 0.9005 (1.294 sec/step)\n",
            "I0919 17:09:27.799956 140474388072320 learning.py:507] global step 1745: loss = 0.9536 (1.304 sec/step)\n",
            "I0919 17:09:29.096369 140474388072320 learning.py:507] global step 1746: loss = 1.0363 (1.295 sec/step)\n",
            "I0919 17:09:30.387260 140474388072320 learning.py:507] global step 1747: loss = 0.8037 (1.289 sec/step)\n",
            "I0919 17:09:31.670275 140474388072320 learning.py:507] global step 1748: loss = 1.2599 (1.281 sec/step)\n",
            "I0919 17:09:32.962332 140474388072320 learning.py:507] global step 1749: loss = 0.9704 (1.291 sec/step)\n",
            "I0919 17:09:34.283837 140474388072320 learning.py:507] global step 1750: loss = 1.2143 (1.320 sec/step)\n",
            "I0919 17:09:35.586707 140474388072320 learning.py:507] global step 1751: loss = 1.2795 (1.301 sec/step)\n",
            "I0919 17:09:36.912631 140474388072320 learning.py:507] global step 1752: loss = 1.3627 (1.324 sec/step)\n",
            "I0919 17:09:38.235208 140474388072320 learning.py:507] global step 1753: loss = 1.1224 (1.321 sec/step)\n",
            "I0919 17:09:39.554027 140474388072320 learning.py:507] global step 1754: loss = 1.1545 (1.317 sec/step)\n",
            "I0919 17:09:40.838143 140474388072320 learning.py:507] global step 1755: loss = 1.1331 (1.282 sec/step)\n",
            "I0919 17:09:42.153074 140474388072320 learning.py:507] global step 1756: loss = 1.1191 (1.313 sec/step)\n",
            "I0919 17:09:43.475619 140474388072320 learning.py:507] global step 1757: loss = 0.9545 (1.321 sec/step)\n",
            "I0919 17:09:44.757788 140474388072320 learning.py:507] global step 1758: loss = 1.1532 (1.281 sec/step)\n",
            "I0919 17:09:46.071060 140474388072320 learning.py:507] global step 1759: loss = 1.1497 (1.310 sec/step)\n",
            "I0919 17:09:47.419445 140474388072320 learning.py:507] global step 1760: loss = 0.9779 (1.346 sec/step)\n",
            "I0919 17:09:48.724570 140474388072320 learning.py:507] global step 1761: loss = 1.0516 (1.303 sec/step)\n",
            "I0919 17:09:50.011622 140474388072320 learning.py:507] global step 1762: loss = 1.0845 (1.285 sec/step)\n",
            "I0919 17:09:51.296860 140474388072320 learning.py:507] global step 1763: loss = 1.0099 (1.284 sec/step)\n",
            "I0919 17:09:52.596030 140474388072320 learning.py:507] global step 1764: loss = 1.3436 (1.297 sec/step)\n",
            "I0919 17:09:53.925808 140474388072320 learning.py:507] global step 1765: loss = 1.0659 (1.328 sec/step)\n",
            "I0919 17:09:55.224386 140474388072320 learning.py:507] global step 1766: loss = 0.8633 (1.297 sec/step)\n",
            "I0919 17:09:56.528393 140474388072320 learning.py:507] global step 1767: loss = 1.0017 (1.302 sec/step)\n",
            "I0919 17:09:57.863862 140474388072320 learning.py:507] global step 1768: loss = 0.8491 (1.334 sec/step)\n",
            "I0919 17:09:59.142894 140474388072320 learning.py:507] global step 1769: loss = 0.8806 (1.277 sec/step)\n",
            "I0919 17:10:00.441477 140474388072320 learning.py:507] global step 1770: loss = 1.1429 (1.297 sec/step)\n",
            "I0919 17:10:01.740401 140474388072320 learning.py:507] global step 1771: loss = 0.9560 (1.297 sec/step)\n",
            "I0919 17:10:03.048621 140474388072320 learning.py:507] global step 1772: loss = 0.9804 (1.307 sec/step)\n",
            "I0919 17:10:04.365274 140474388072320 learning.py:507] global step 1773: loss = 0.8318 (1.315 sec/step)\n",
            "I0919 17:10:05.690320 140474388072320 learning.py:507] global step 1774: loss = 1.1409 (1.323 sec/step)\n",
            "I0919 17:10:07.012255 140474388072320 learning.py:507] global step 1775: loss = 1.3972 (1.320 sec/step)\n",
            "I0919 17:10:08.365864 140474388072320 learning.py:507] global step 1776: loss = 1.5656 (1.352 sec/step)\n",
            "I0919 17:10:09.743252 140474388072320 learning.py:507] global step 1777: loss = 1.1666 (1.376 sec/step)\n",
            "I0919 17:10:11.055232 140474388072320 learning.py:507] global step 1778: loss = 1.0772 (1.310 sec/step)\n",
            "I0919 17:10:12.360517 140474388072320 learning.py:507] global step 1779: loss = 1.1231 (1.304 sec/step)\n",
            "I0919 17:10:13.679419 140474388072320 learning.py:507] global step 1780: loss = 1.0621 (1.317 sec/step)\n",
            "I0919 17:10:15.013741 140474388072320 learning.py:507] global step 1781: loss = 1.0151 (1.333 sec/step)\n",
            "I0919 17:10:16.317961 140474388072320 learning.py:507] global step 1782: loss = 1.0117 (1.303 sec/step)\n",
            "I0919 17:10:17.653713 140474388072320 learning.py:507] global step 1783: loss = 0.9036 (1.334 sec/step)\n",
            "I0919 17:10:18.963095 140474388072320 learning.py:507] global step 1784: loss = 1.0684 (1.308 sec/step)\n",
            "I0919 17:10:20.276573 140474388072320 learning.py:507] global step 1785: loss = 1.0205 (1.312 sec/step)\n",
            "I0919 17:10:21.566281 140474388072320 learning.py:507] global step 1786: loss = 1.1696 (1.288 sec/step)\n",
            "I0919 17:10:22.862930 140474388072320 learning.py:507] global step 1787: loss = 1.2438 (1.295 sec/step)\n",
            "I0919 17:10:24.178478 140474388072320 learning.py:507] global step 1788: loss = 1.2117 (1.314 sec/step)\n",
            "I0919 17:10:25.493211 140474388072320 learning.py:507] global step 1789: loss = 1.3293 (1.313 sec/step)\n",
            "I0919 17:10:26.776642 140474388072320 learning.py:507] global step 1790: loss = 1.0779 (1.282 sec/step)\n",
            "I0919 17:10:26.872874 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 17:10:29.480395 140474388072320 learning.py:507] global step 1791: loss = 0.8125 (2.700 sec/step)\n",
            "I0919 17:10:30.079552 140471297337088 supervisor.py:1050] Recording summary at step 1791.\n",
            "I0919 17:10:31.351367 140474388072320 learning.py:507] global step 1792: loss = 0.9654 (1.806 sec/step)\n",
            "I0919 17:10:33.175925 140474388072320 learning.py:507] global step 1793: loss = 0.8368 (1.798 sec/step)\n",
            "I0919 17:10:34.542545 140474388072320 learning.py:507] global step 1794: loss = 1.3562 (1.312 sec/step)\n",
            "I0919 17:10:35.838947 140474388072320 learning.py:507] global step 1795: loss = 0.9564 (1.295 sec/step)\n",
            "I0919 17:10:37.159394 140474388072320 learning.py:507] global step 1796: loss = 1.1427 (1.319 sec/step)\n",
            "I0919 17:10:38.493343 140474388072320 learning.py:507] global step 1797: loss = 0.9197 (1.332 sec/step)\n",
            "I0919 17:10:39.857024 140474388072320 learning.py:507] global step 1798: loss = 0.9190 (1.362 sec/step)\n",
            "I0919 17:10:41.166779 140474388072320 learning.py:507] global step 1799: loss = 1.0925 (1.308 sec/step)\n",
            "I0919 17:10:42.450633 140474388072320 learning.py:507] global step 1800: loss = 1.0074 (1.282 sec/step)\n",
            "I0919 17:10:43.750181 140474388072320 learning.py:507] global step 1801: loss = 0.9451 (1.298 sec/step)\n",
            "I0919 17:10:45.038520 140474388072320 learning.py:507] global step 1802: loss = 1.1587 (1.287 sec/step)\n",
            "I0919 17:10:46.336278 140474388072320 learning.py:507] global step 1803: loss = 0.8947 (1.296 sec/step)\n",
            "I0919 17:10:47.610969 140474388072320 learning.py:507] global step 1804: loss = 1.0980 (1.273 sec/step)\n",
            "I0919 17:10:48.909579 140474388072320 learning.py:507] global step 1805: loss = 1.0209 (1.297 sec/step)\n",
            "I0919 17:10:50.231031 140474388072320 learning.py:507] global step 1806: loss = 1.1527 (1.320 sec/step)\n",
            "I0919 17:10:51.522819 140474388072320 learning.py:507] global step 1807: loss = 1.1260 (1.290 sec/step)\n",
            "I0919 17:10:52.807703 140474388072320 learning.py:507] global step 1808: loss = 0.9858 (1.283 sec/step)\n",
            "I0919 17:10:54.103420 140474388072320 learning.py:507] global step 1809: loss = 0.8595 (1.294 sec/step)\n",
            "I0919 17:10:55.401624 140474388072320 learning.py:507] global step 1810: loss = 0.6745 (1.296 sec/step)\n",
            "I0919 17:10:56.703563 140474388072320 learning.py:507] global step 1811: loss = 0.9272 (1.300 sec/step)\n",
            "I0919 17:10:57.996768 140474388072320 learning.py:507] global step 1812: loss = 1.3641 (1.291 sec/step)\n",
            "I0919 17:10:59.327358 140474388072320 learning.py:507] global step 1813: loss = 1.0043 (1.329 sec/step)\n",
            "I0919 17:11:00.603843 140474388072320 learning.py:507] global step 1814: loss = 0.9440 (1.275 sec/step)\n",
            "I0919 17:11:01.972060 140474388072320 learning.py:507] global step 1815: loss = 1.1700 (1.367 sec/step)\n",
            "I0919 17:11:03.281553 140474388072320 learning.py:507] global step 1816: loss = 0.8504 (1.308 sec/step)\n",
            "I0919 17:11:04.564770 140474388072320 learning.py:507] global step 1817: loss = 0.8887 (1.281 sec/step)\n",
            "I0919 17:11:05.853547 140474388072320 learning.py:507] global step 1818: loss = 0.8013 (1.287 sec/step)\n",
            "I0919 17:11:07.180469 140474388072320 learning.py:507] global step 1819: loss = 0.9649 (1.325 sec/step)\n",
            "I0919 17:11:08.492961 140474388072320 learning.py:507] global step 1820: loss = 0.8548 (1.311 sec/step)\n",
            "I0919 17:11:09.853557 140474388072320 learning.py:507] global step 1821: loss = 1.1146 (1.359 sec/step)\n",
            "I0919 17:11:11.170530 140474388072320 learning.py:507] global step 1822: loss = 0.8732 (1.315 sec/step)\n",
            "I0919 17:11:12.504945 140474388072320 learning.py:507] global step 1823: loss = 0.9435 (1.332 sec/step)\n",
            "I0919 17:11:13.832261 140474388072320 learning.py:507] global step 1824: loss = 1.1428 (1.326 sec/step)\n",
            "I0919 17:11:15.130247 140474388072320 learning.py:507] global step 1825: loss = 1.1291 (1.296 sec/step)\n",
            "I0919 17:11:16.417869 140474388072320 learning.py:507] global step 1826: loss = 1.1358 (1.286 sec/step)\n",
            "I0919 17:11:17.725718 140474388072320 learning.py:507] global step 1827: loss = 0.9253 (1.306 sec/step)\n",
            "I0919 17:11:19.070465 140474388072320 learning.py:507] global step 1828: loss = 1.1298 (1.343 sec/step)\n",
            "I0919 17:11:20.359991 140474388072320 learning.py:507] global step 1829: loss = 1.1103 (1.288 sec/step)\n",
            "I0919 17:11:21.633295 140474388072320 learning.py:507] global step 1830: loss = 0.9496 (1.272 sec/step)\n",
            "I0919 17:11:22.952954 140474388072320 learning.py:507] global step 1831: loss = 0.9592 (1.318 sec/step)\n",
            "I0919 17:11:24.318188 140474388072320 learning.py:507] global step 1832: loss = 0.9112 (1.363 sec/step)\n",
            "I0919 17:11:25.640239 140474388072320 learning.py:507] global step 1833: loss = 1.0713 (1.320 sec/step)\n",
            "I0919 17:11:26.965795 140474388072320 learning.py:507] global step 1834: loss = 1.0667 (1.324 sec/step)\n",
            "I0919 17:11:28.313435 140474388072320 learning.py:507] global step 1835: loss = 0.9830 (1.346 sec/step)\n",
            "I0919 17:11:29.594575 140474388072320 learning.py:507] global step 1836: loss = 1.0291 (1.280 sec/step)\n",
            "I0919 17:11:30.901252 140474388072320 learning.py:507] global step 1837: loss = 1.2438 (1.305 sec/step)\n",
            "I0919 17:11:32.209069 140474388072320 learning.py:507] global step 1838: loss = 0.9649 (1.306 sec/step)\n",
            "I0919 17:11:33.502923 140474388072320 learning.py:507] global step 1839: loss = 1.0701 (1.291 sec/step)\n",
            "I0919 17:11:34.813282 140474388072320 learning.py:507] global step 1840: loss = 0.8329 (1.309 sec/step)\n",
            "I0919 17:11:36.131871 140474388072320 learning.py:507] global step 1841: loss = 1.1306 (1.317 sec/step)\n",
            "I0919 17:11:37.424236 140474388072320 learning.py:507] global step 1842: loss = 1.1361 (1.291 sec/step)\n",
            "I0919 17:11:38.732280 140474388072320 learning.py:507] global step 1843: loss = 0.9106 (1.306 sec/step)\n",
            "I0919 17:11:40.047874 140474388072320 learning.py:507] global step 1844: loss = 0.9953 (1.314 sec/step)\n",
            "I0919 17:11:41.349186 140474388072320 learning.py:507] global step 1845: loss = 0.9436 (1.299 sec/step)\n",
            "I0919 17:11:42.678884 140474388072320 learning.py:507] global step 1846: loss = 1.4590 (1.328 sec/step)\n",
            "I0919 17:11:44.014651 140474388072320 learning.py:507] global step 1847: loss = 1.4249 (1.334 sec/step)\n",
            "I0919 17:11:45.345953 140474388072320 learning.py:507] global step 1848: loss = 0.9418 (1.329 sec/step)\n",
            "I0919 17:11:46.692306 140474388072320 learning.py:507] global step 1849: loss = 1.0566 (1.344 sec/step)\n",
            "I0919 17:11:48.023217 140474388072320 learning.py:507] global step 1850: loss = 0.7852 (1.329 sec/step)\n",
            "I0919 17:11:49.360778 140474388072320 learning.py:507] global step 1851: loss = 0.8683 (1.336 sec/step)\n",
            "I0919 17:11:50.720583 140474388072320 learning.py:507] global step 1852: loss = 1.2086 (1.358 sec/step)\n",
            "I0919 17:11:52.015934 140474388072320 learning.py:507] global step 1853: loss = 1.1449 (1.293 sec/step)\n",
            "I0919 17:11:53.321901 140474388072320 learning.py:507] global step 1854: loss = 1.0188 (1.304 sec/step)\n",
            "I0919 17:11:54.630527 140474388072320 learning.py:507] global step 1855: loss = 1.0615 (1.307 sec/step)\n",
            "I0919 17:11:55.919966 140474388072320 learning.py:507] global step 1856: loss = 0.9492 (1.288 sec/step)\n",
            "I0919 17:11:57.223005 140474388072320 learning.py:507] global step 1857: loss = 0.9349 (1.301 sec/step)\n",
            "I0919 17:11:58.515682 140474388072320 learning.py:507] global step 1858: loss = 0.8021 (1.291 sec/step)\n",
            "I0919 17:11:59.858906 140474388072320 learning.py:507] global step 1859: loss = 1.1356 (1.342 sec/step)\n",
            "I0919 17:12:01.191495 140474388072320 learning.py:507] global step 1860: loss = 1.2824 (1.331 sec/step)\n",
            "I0919 17:12:02.490021 140474388072320 learning.py:507] global step 1861: loss = 0.9100 (1.297 sec/step)\n",
            "I0919 17:12:03.810904 140474388072320 learning.py:507] global step 1862: loss = 1.4286 (1.319 sec/step)\n",
            "I0919 17:12:05.134279 140474388072320 learning.py:507] global step 1863: loss = 0.9875 (1.321 sec/step)\n",
            "I0919 17:12:06.439274 140474388072320 learning.py:507] global step 1864: loss = 0.9839 (1.304 sec/step)\n",
            "I0919 17:12:07.739553 140474388072320 learning.py:507] global step 1865: loss = 0.8464 (1.298 sec/step)\n",
            "I0919 17:12:09.074906 140474388072320 learning.py:507] global step 1866: loss = 1.2237 (1.334 sec/step)\n",
            "I0919 17:12:10.412427 140474388072320 learning.py:507] global step 1867: loss = 0.9438 (1.336 sec/step)\n",
            "I0919 17:12:11.768596 140474388072320 learning.py:507] global step 1868: loss = 1.1250 (1.354 sec/step)\n",
            "I0919 17:12:13.097552 140474388072320 learning.py:507] global step 1869: loss = 1.0530 (1.327 sec/step)\n",
            "I0919 17:12:14.388151 140474388072320 learning.py:507] global step 1870: loss = 0.9183 (1.289 sec/step)\n",
            "I0919 17:12:15.719552 140474388072320 learning.py:507] global step 1871: loss = 0.8750 (1.330 sec/step)\n",
            "I0919 17:12:17.009778 140474388072320 learning.py:507] global step 1872: loss = 0.9748 (1.289 sec/step)\n",
            "I0919 17:12:18.337557 140474388072320 learning.py:507] global step 1873: loss = 0.9020 (1.326 sec/step)\n",
            "I0919 17:12:19.676619 140474388072320 learning.py:507] global step 1874: loss = 1.1625 (1.337 sec/step)\n",
            "I0919 17:12:20.978487 140474388072320 learning.py:507] global step 1875: loss = 0.9289 (1.300 sec/step)\n",
            "I0919 17:12:22.274480 140474388072320 learning.py:507] global step 1876: loss = 0.8292 (1.294 sec/step)\n",
            "I0919 17:12:23.550410 140474388072320 learning.py:507] global step 1877: loss = 1.1397 (1.274 sec/step)\n",
            "I0919 17:12:24.840327 140474388072320 learning.py:507] global step 1878: loss = 0.9142 (1.288 sec/step)\n",
            "I0919 17:12:26.154038 140474388072320 learning.py:507] global step 1879: loss = 0.8659 (1.312 sec/step)\n",
            "I0919 17:12:27.488612 140474388072320 learning.py:507] global step 1880: loss = 1.2545 (1.332 sec/step)\n",
            "I0919 17:12:29.253328 140471297337088 supervisor.py:1050] Recording summary at step 1880.\n",
            "I0919 17:12:29.728236 140474388072320 learning.py:507] global step 1881: loss = 1.2294 (2.238 sec/step)\n",
            "I0919 17:12:31.065351 140474388072320 learning.py:507] global step 1882: loss = 0.9941 (1.335 sec/step)\n",
            "I0919 17:12:32.349353 140474388072320 learning.py:507] global step 1883: loss = 1.0489 (1.282 sec/step)\n",
            "I0919 17:12:33.625719 140474388072320 learning.py:507] global step 1884: loss = 1.0221 (1.274 sec/step)\n",
            "I0919 17:12:34.924935 140474388072320 learning.py:507] global step 1885: loss = 1.0023 (1.297 sec/step)\n",
            "I0919 17:12:36.231595 140474388072320 learning.py:507] global step 1886: loss = 1.2138 (1.305 sec/step)\n",
            "I0919 17:12:37.557498 140474388072320 learning.py:507] global step 1887: loss = 0.9459 (1.324 sec/step)\n",
            "I0919 17:12:38.927617 140474388072320 learning.py:507] global step 1888: loss = 0.8847 (1.369 sec/step)\n",
            "I0919 17:12:40.245273 140474388072320 learning.py:507] global step 1889: loss = 0.9310 (1.316 sec/step)\n",
            "I0919 17:12:41.553280 140474388072320 learning.py:507] global step 1890: loss = 1.1817 (1.306 sec/step)\n",
            "I0919 17:12:42.832566 140474388072320 learning.py:507] global step 1891: loss = 0.8816 (1.277 sec/step)\n",
            "I0919 17:12:44.171432 140474388072320 learning.py:507] global step 1892: loss = 0.9937 (1.337 sec/step)\n",
            "I0919 17:12:45.461559 140474388072320 learning.py:507] global step 1893: loss = 1.1486 (1.288 sec/step)\n",
            "I0919 17:12:46.769098 140474388072320 learning.py:507] global step 1894: loss = 1.2405 (1.306 sec/step)\n",
            "I0919 17:12:48.083350 140474388072320 learning.py:507] global step 1895: loss = 0.9404 (1.312 sec/step)\n",
            "I0919 17:12:49.378036 140474388072320 learning.py:507] global step 1896: loss = 1.1277 (1.293 sec/step)\n",
            "I0919 17:12:50.684871 140474388072320 learning.py:507] global step 1897: loss = 1.2307 (1.305 sec/step)\n",
            "I0919 17:12:52.009979 140474388072320 learning.py:507] global step 1898: loss = 1.1189 (1.323 sec/step)\n",
            "I0919 17:12:53.297070 140474388072320 learning.py:507] global step 1899: loss = 0.9029 (1.285 sec/step)\n",
            "I0919 17:12:54.598537 140474388072320 learning.py:507] global step 1900: loss = 1.2459 (1.300 sec/step)\n",
            "I0919 17:12:55.896645 140474388072320 learning.py:507] global step 1901: loss = 1.2297 (1.296 sec/step)\n",
            "I0919 17:12:57.185065 140474388072320 learning.py:507] global step 1902: loss = 1.0827 (1.287 sec/step)\n",
            "I0919 17:12:58.503475 140474388072320 learning.py:507] global step 1903: loss = 0.9188 (1.317 sec/step)\n",
            "I0919 17:12:59.785773 140474388072320 learning.py:507] global step 1904: loss = 0.9817 (1.280 sec/step)\n",
            "I0919 17:13:01.073365 140474388072320 learning.py:507] global step 1905: loss = 1.2010 (1.286 sec/step)\n",
            "I0919 17:13:02.354140 140474388072320 learning.py:507] global step 1906: loss = 1.0003 (1.279 sec/step)\n",
            "I0919 17:13:03.673774 140474388072320 learning.py:507] global step 1907: loss = 1.5253 (1.318 sec/step)\n",
            "I0919 17:13:05.018722 140474388072320 learning.py:507] global step 1908: loss = 1.0611 (1.343 sec/step)\n",
            "I0919 17:13:06.324125 140474388072320 learning.py:507] global step 1909: loss = 0.9429 (1.303 sec/step)\n",
            "I0919 17:13:07.670228 140474388072320 learning.py:507] global step 1910: loss = 0.8901 (1.344 sec/step)\n",
            "I0919 17:13:09.023323 140474388072320 learning.py:507] global step 1911: loss = 0.9555 (1.352 sec/step)\n",
            "I0919 17:13:10.326328 140474388072320 learning.py:507] global step 1912: loss = 1.0922 (1.301 sec/step)\n",
            "I0919 17:13:11.636492 140474388072320 learning.py:507] global step 1913: loss = 0.9502 (1.308 sec/step)\n",
            "I0919 17:13:12.923882 140474388072320 learning.py:507] global step 1914: loss = 1.1276 (1.286 sec/step)\n",
            "I0919 17:13:14.231465 140474388072320 learning.py:507] global step 1915: loss = 0.8937 (1.306 sec/step)\n",
            "I0919 17:13:15.538769 140474388072320 learning.py:507] global step 1916: loss = 0.9307 (1.306 sec/step)\n",
            "I0919 17:13:16.860557 140474388072320 learning.py:507] global step 1917: loss = 1.1835 (1.320 sec/step)\n",
            "I0919 17:13:18.179272 140474388072320 learning.py:507] global step 1918: loss = 1.0800 (1.317 sec/step)\n",
            "I0919 17:13:19.478863 140474388072320 learning.py:507] global step 1919: loss = 1.3809 (1.298 sec/step)\n",
            "I0919 17:13:20.775266 140474388072320 learning.py:507] global step 1920: loss = 1.5617 (1.295 sec/step)\n",
            "I0919 17:13:22.079819 140474388072320 learning.py:507] global step 1921: loss = 0.8662 (1.303 sec/step)\n",
            "I0919 17:13:23.392868 140474388072320 learning.py:507] global step 1922: loss = 1.4751 (1.311 sec/step)\n",
            "I0919 17:13:24.725891 140474388072320 learning.py:507] global step 1923: loss = 0.9703 (1.331 sec/step)\n",
            "I0919 17:13:26.022428 140474388072320 learning.py:507] global step 1924: loss = 1.2015 (1.295 sec/step)\n",
            "I0919 17:13:27.330529 140474388072320 learning.py:507] global step 1925: loss = 1.2109 (1.306 sec/step)\n",
            "I0919 17:13:28.638204 140474388072320 learning.py:507] global step 1926: loss = 1.0121 (1.306 sec/step)\n",
            "I0919 17:13:29.961024 140474388072320 learning.py:507] global step 1927: loss = 1.0345 (1.321 sec/step)\n",
            "I0919 17:13:31.260476 140474388072320 learning.py:507] global step 1928: loss = 1.1372 (1.298 sec/step)\n",
            "I0919 17:13:32.585227 140474388072320 learning.py:507] global step 1929: loss = 0.9634 (1.323 sec/step)\n",
            "I0919 17:13:33.885337 140474388072320 learning.py:507] global step 1930: loss = 0.9083 (1.298 sec/step)\n",
            "I0919 17:13:35.209182 140474388072320 learning.py:507] global step 1931: loss = 0.9811 (1.322 sec/step)\n",
            "I0919 17:13:36.539246 140474388072320 learning.py:507] global step 1932: loss = 0.9664 (1.328 sec/step)\n",
            "I0919 17:13:37.876359 140474388072320 learning.py:507] global step 1933: loss = 0.8846 (1.336 sec/step)\n",
            "I0919 17:13:39.149429 140474388072320 learning.py:507] global step 1934: loss = 0.8240 (1.272 sec/step)\n",
            "I0919 17:13:40.470493 140474388072320 learning.py:507] global step 1935: loss = 1.1244 (1.319 sec/step)\n",
            "I0919 17:13:41.794473 140474388072320 learning.py:507] global step 1936: loss = 1.1521 (1.322 sec/step)\n",
            "I0919 17:13:43.066313 140474388072320 learning.py:507] global step 1937: loss = 1.2261 (1.270 sec/step)\n",
            "I0919 17:13:44.398278 140474388072320 learning.py:507] global step 1938: loss = 1.1043 (1.330 sec/step)\n",
            "I0919 17:13:45.704417 140474388072320 learning.py:507] global step 1939: loss = 1.0540 (1.304 sec/step)\n",
            "I0919 17:13:47.092998 140474388072320 learning.py:507] global step 1940: loss = 0.9734 (1.386 sec/step)\n",
            "I0919 17:13:48.422476 140474388072320 learning.py:507] global step 1941: loss = 0.8572 (1.328 sec/step)\n",
            "I0919 17:13:49.708156 140474388072320 learning.py:507] global step 1942: loss = 1.5857 (1.284 sec/step)\n",
            "I0919 17:13:51.025125 140474388072320 learning.py:507] global step 1943: loss = 1.1364 (1.315 sec/step)\n",
            "I0919 17:13:52.325838 140474388072320 learning.py:507] global step 1944: loss = 1.2942 (1.299 sec/step)\n",
            "I0919 17:13:53.615922 140474388072320 learning.py:507] global step 1945: loss = 0.8707 (1.288 sec/step)\n",
            "I0919 17:13:54.901546 140474388072320 learning.py:507] global step 1946: loss = 1.1284 (1.284 sec/step)\n",
            "I0919 17:13:56.197561 140474388072320 learning.py:507] global step 1947: loss = 1.2707 (1.294 sec/step)\n",
            "I0919 17:13:57.494854 140474388072320 learning.py:507] global step 1948: loss = 1.0780 (1.296 sec/step)\n",
            "I0919 17:13:58.807238 140474388072320 learning.py:507] global step 1949: loss = 1.5574 (1.311 sec/step)\n",
            "I0919 17:14:00.123014 140474388072320 learning.py:507] global step 1950: loss = 1.0481 (1.314 sec/step)\n",
            "I0919 17:14:01.442755 140474388072320 learning.py:507] global step 1951: loss = 0.7767 (1.318 sec/step)\n",
            "I0919 17:14:02.777816 140474388072320 learning.py:507] global step 1952: loss = 1.0256 (1.332 sec/step)\n",
            "I0919 17:14:04.044395 140474388072320 learning.py:507] global step 1953: loss = 0.9171 (1.265 sec/step)\n",
            "I0919 17:14:05.373271 140474388072320 learning.py:507] global step 1954: loss = 0.8306 (1.327 sec/step)\n",
            "I0919 17:14:06.703006 140474388072320 learning.py:507] global step 1955: loss = 1.0263 (1.328 sec/step)\n",
            "I0919 17:14:08.014759 140474388072320 learning.py:507] global step 1956: loss = 1.2158 (1.309 sec/step)\n",
            "I0919 17:14:09.297430 140474388072320 learning.py:507] global step 1957: loss = 1.0093 (1.281 sec/step)\n",
            "I0919 17:14:10.618849 140474388072320 learning.py:507] global step 1958: loss = 0.8232 (1.320 sec/step)\n",
            "I0919 17:14:11.958745 140474388072320 learning.py:507] global step 1959: loss = 0.7280 (1.338 sec/step)\n",
            "I0919 17:14:13.235074 140474388072320 learning.py:507] global step 1960: loss = 0.9431 (1.275 sec/step)\n",
            "I0919 17:14:14.506544 140474388072320 learning.py:507] global step 1961: loss = 1.1089 (1.270 sec/step)\n",
            "I0919 17:14:15.802514 140474388072320 learning.py:507] global step 1962: loss = 1.1406 (1.294 sec/step)\n",
            "I0919 17:14:17.111577 140474388072320 learning.py:507] global step 1963: loss = 1.1832 (1.307 sec/step)\n",
            "I0919 17:14:18.417583 140474388072320 learning.py:507] global step 1964: loss = 0.8151 (1.304 sec/step)\n",
            "I0919 17:14:19.712361 140474388072320 learning.py:507] global step 1965: loss = 0.9228 (1.293 sec/step)\n",
            "I0919 17:14:21.030306 140474388072320 learning.py:507] global step 1966: loss = 1.0512 (1.316 sec/step)\n",
            "I0919 17:14:22.324433 140474388072320 learning.py:507] global step 1967: loss = 0.9800 (1.293 sec/step)\n",
            "I0919 17:14:23.623204 140474388072320 learning.py:507] global step 1968: loss = 0.9196 (1.297 sec/step)\n",
            "I0919 17:14:24.929985 140474388072320 learning.py:507] global step 1969: loss = 1.1848 (1.305 sec/step)\n",
            "I0919 17:14:26.247179 140474388072320 learning.py:507] global step 1970: loss = 1.4378 (1.316 sec/step)\n",
            "I0919 17:14:27.562923 140474388072320 learning.py:507] global step 1971: loss = 1.0047 (1.314 sec/step)\n",
            "I0919 17:14:29.306828 140471297337088 supervisor.py:1050] Recording summary at step 1971.\n",
            "I0919 17:14:29.728478 140474388072320 learning.py:507] global step 1972: loss = 0.8661 (2.164 sec/step)\n",
            "I0919 17:14:31.074177 140474388072320 learning.py:507] global step 1973: loss = 1.0930 (1.344 sec/step)\n",
            "I0919 17:14:32.385818 140474388072320 learning.py:507] global step 1974: loss = 0.8135 (1.310 sec/step)\n",
            "I0919 17:14:33.684103 140474388072320 learning.py:507] global step 1975: loss = 0.9739 (1.297 sec/step)\n",
            "I0919 17:14:34.989212 140474388072320 learning.py:507] global step 1976: loss = 1.1464 (1.304 sec/step)\n",
            "I0919 17:14:36.291523 140474388072320 learning.py:507] global step 1977: loss = 1.0851 (1.300 sec/step)\n",
            "I0919 17:14:37.615231 140474388072320 learning.py:507] global step 1978: loss = 0.9896 (1.322 sec/step)\n",
            "I0919 17:14:38.914777 140474388072320 learning.py:507] global step 1979: loss = 1.0841 (1.298 sec/step)\n",
            "I0919 17:14:40.230803 140474388072320 learning.py:507] global step 1980: loss = 0.9331 (1.314 sec/step)\n",
            "I0919 17:14:41.595942 140474388072320 learning.py:507] global step 1981: loss = 1.2142 (1.363 sec/step)\n",
            "I0919 17:14:42.921994 140474388072320 learning.py:507] global step 1982: loss = 1.1833 (1.324 sec/step)\n",
            "I0919 17:14:44.215532 140474388072320 learning.py:507] global step 1983: loss = 1.1458 (1.292 sec/step)\n",
            "I0919 17:14:45.563716 140474388072320 learning.py:507] global step 1984: loss = 1.1913 (1.347 sec/step)\n",
            "I0919 17:14:46.904907 140474388072320 learning.py:507] global step 1985: loss = 0.8668 (1.339 sec/step)\n",
            "I0919 17:14:48.207854 140474388072320 learning.py:507] global step 1986: loss = 0.7684 (1.301 sec/step)\n",
            "I0919 17:14:49.492729 140474388072320 learning.py:507] global step 1987: loss = 1.0956 (1.283 sec/step)\n",
            "I0919 17:14:50.824708 140474388072320 learning.py:507] global step 1988: loss = 0.7517 (1.330 sec/step)\n",
            "I0919 17:14:52.162422 140474388072320 learning.py:507] global step 1989: loss = 1.0510 (1.336 sec/step)\n",
            "I0919 17:14:53.465826 140474388072320 learning.py:507] global step 1990: loss = 0.9989 (1.301 sec/step)\n",
            "I0919 17:14:54.787410 140474388072320 learning.py:507] global step 1991: loss = 1.1760 (1.320 sec/step)\n",
            "I0919 17:14:56.121931 140474388072320 learning.py:507] global step 1992: loss = 0.9100 (1.333 sec/step)\n",
            "I0919 17:14:57.457762 140474388072320 learning.py:507] global step 1993: loss = 0.7936 (1.334 sec/step)\n",
            "I0919 17:14:58.767677 140474388072320 learning.py:507] global step 1994: loss = 0.8353 (1.308 sec/step)\n",
            "I0919 17:15:00.089299 140474388072320 learning.py:507] global step 1995: loss = 1.0142 (1.320 sec/step)\n",
            "I0919 17:15:01.390158 140474388072320 learning.py:507] global step 1996: loss = 1.1377 (1.299 sec/step)\n",
            "I0919 17:15:02.684354 140474388072320 learning.py:507] global step 1997: loss = 1.1180 (1.292 sec/step)\n",
            "I0919 17:15:03.967467 140474388072320 learning.py:507] global step 1998: loss = 1.0354 (1.281 sec/step)\n",
            "I0919 17:15:05.278609 140474388072320 learning.py:507] global step 1999: loss = 0.8739 (1.310 sec/step)\n",
            "I0919 17:15:06.606692 140474388072320 learning.py:507] global step 2000: loss = 1.1310 (1.326 sec/step)\n",
            "I0919 17:15:07.913218 140474388072320 learning.py:507] global step 2001: loss = 1.2421 (1.305 sec/step)\n",
            "I0919 17:15:09.194228 140474388072320 learning.py:507] global step 2002: loss = 1.1124 (1.279 sec/step)\n",
            "I0919 17:15:10.473833 140474388072320 learning.py:507] global step 2003: loss = 1.3860 (1.278 sec/step)\n",
            "I0919 17:15:11.782603 140474388072320 learning.py:507] global step 2004: loss = 0.9490 (1.307 sec/step)\n",
            "I0919 17:15:13.076282 140474388072320 learning.py:507] global step 2005: loss = 0.9705 (1.292 sec/step)\n",
            "I0919 17:15:14.413444 140474388072320 learning.py:507] global step 2006: loss = 0.9929 (1.335 sec/step)\n",
            "I0919 17:15:15.718899 140474388072320 learning.py:507] global step 2007: loss = 1.5535 (1.304 sec/step)\n",
            "I0919 17:15:17.012556 140474388072320 learning.py:507] global step 2008: loss = 0.9724 (1.292 sec/step)\n",
            "I0919 17:15:18.358061 140474388072320 learning.py:507] global step 2009: loss = 1.0482 (1.344 sec/step)\n",
            "I0919 17:15:19.643734 140474388072320 learning.py:507] global step 2010: loss = 1.1293 (1.284 sec/step)\n",
            "I0919 17:15:20.960455 140474388072320 learning.py:507] global step 2011: loss = 0.9267 (1.315 sec/step)\n",
            "I0919 17:15:22.267837 140474388072320 learning.py:507] global step 2012: loss = 0.7583 (1.305 sec/step)\n",
            "I0919 17:15:23.566890 140474388072320 learning.py:507] global step 2013: loss = 0.9995 (1.298 sec/step)\n",
            "I0919 17:15:24.873533 140474388072320 learning.py:507] global step 2014: loss = 1.1145 (1.305 sec/step)\n",
            "I0919 17:15:26.191687 140474388072320 learning.py:507] global step 2015: loss = 1.4390 (1.316 sec/step)\n",
            "I0919 17:15:27.482726 140474388072320 learning.py:507] global step 2016: loss = 1.0238 (1.289 sec/step)\n",
            "I0919 17:15:28.800214 140474388072320 learning.py:507] global step 2017: loss = 0.8217 (1.316 sec/step)\n",
            "I0919 17:15:30.126692 140474388072320 learning.py:507] global step 2018: loss = 0.9392 (1.325 sec/step)\n",
            "I0919 17:15:31.442951 140474388072320 learning.py:507] global step 2019: loss = 0.9157 (1.315 sec/step)\n",
            "I0919 17:15:32.757411 140474388072320 learning.py:507] global step 2020: loss = 1.2146 (1.313 sec/step)\n",
            "I0919 17:15:34.054953 140474388072320 learning.py:507] global step 2021: loss = 1.1851 (1.296 sec/step)\n",
            "I0919 17:15:35.362767 140474388072320 learning.py:507] global step 2022: loss = 1.1281 (1.306 sec/step)\n",
            "I0919 17:15:36.688388 140474388072320 learning.py:507] global step 2023: loss = 1.0628 (1.324 sec/step)\n",
            "I0919 17:15:38.015269 140474388072320 learning.py:507] global step 2024: loss = 1.1056 (1.325 sec/step)\n",
            "I0919 17:15:39.295480 140474388072320 learning.py:507] global step 2025: loss = 0.9911 (1.278 sec/step)\n",
            "I0919 17:15:40.631038 140474388072320 learning.py:507] global step 2026: loss = 0.8874 (1.334 sec/step)\n",
            "I0919 17:15:41.958424 140474388072320 learning.py:507] global step 2027: loss = 1.2063 (1.325 sec/step)\n",
            "I0919 17:15:43.271101 140474388072320 learning.py:507] global step 2028: loss = 1.0292 (1.311 sec/step)\n",
            "I0919 17:15:44.561563 140474388072320 learning.py:507] global step 2029: loss = 0.9373 (1.289 sec/step)\n",
            "I0919 17:15:45.848392 140474388072320 learning.py:507] global step 2030: loss = 0.8690 (1.285 sec/step)\n",
            "I0919 17:15:47.158226 140474388072320 learning.py:507] global step 2031: loss = 1.0667 (1.308 sec/step)\n",
            "I0919 17:15:48.467926 140474388072320 learning.py:507] global step 2032: loss = 1.0469 (1.308 sec/step)\n",
            "I0919 17:15:49.754018 140474388072320 learning.py:507] global step 2033: loss = 1.2857 (1.285 sec/step)\n",
            "I0919 17:15:51.040709 140474388072320 learning.py:507] global step 2034: loss = 1.0841 (1.285 sec/step)\n",
            "I0919 17:15:52.338666 140474388072320 learning.py:507] global step 2035: loss = 1.0154 (1.296 sec/step)\n",
            "I0919 17:15:53.640392 140474388072320 learning.py:507] global step 2036: loss = 1.1902 (1.300 sec/step)\n",
            "I0919 17:15:54.930483 140474388072320 learning.py:507] global step 2037: loss = 1.2413 (1.288 sec/step)\n",
            "I0919 17:15:56.215475 140474388072320 learning.py:507] global step 2038: loss = 0.9208 (1.283 sec/step)\n",
            "I0919 17:15:57.524465 140474388072320 learning.py:507] global step 2039: loss = 1.1217 (1.307 sec/step)\n",
            "I0919 17:15:58.826988 140474388072320 learning.py:507] global step 2040: loss = 1.0108 (1.301 sec/step)\n",
            "I0919 17:16:00.139209 140474388072320 learning.py:507] global step 2041: loss = 1.1271 (1.310 sec/step)\n",
            "I0919 17:16:01.449762 140474388072320 learning.py:507] global step 2042: loss = 0.8174 (1.309 sec/step)\n",
            "I0919 17:16:02.779711 140474388072320 learning.py:507] global step 2043: loss = 0.9369 (1.328 sec/step)\n",
            "I0919 17:16:04.083344 140474388072320 learning.py:507] global step 2044: loss = 0.9359 (1.302 sec/step)\n",
            "I0919 17:16:05.435674 140474388072320 learning.py:507] global step 2045: loss = 0.8072 (1.350 sec/step)\n",
            "I0919 17:16:06.737312 140474388072320 learning.py:507] global step 2046: loss = 1.1429 (1.300 sec/step)\n",
            "I0919 17:16:08.020839 140474388072320 learning.py:507] global step 2047: loss = 1.1674 (1.282 sec/step)\n",
            "I0919 17:16:09.338126 140474388072320 learning.py:507] global step 2048: loss = 1.0002 (1.315 sec/step)\n",
            "I0919 17:16:10.636424 140474388072320 learning.py:507] global step 2049: loss = 1.4196 (1.297 sec/step)\n",
            "I0919 17:16:11.953547 140474388072320 learning.py:507] global step 2050: loss = 0.9582 (1.316 sec/step)\n",
            "I0919 17:16:13.286869 140474388072320 learning.py:507] global step 2051: loss = 1.0443 (1.328 sec/step)\n",
            "I0919 17:16:14.605278 140474388072320 learning.py:507] global step 2052: loss = 0.8640 (1.317 sec/step)\n",
            "I0919 17:16:15.878187 140474388072320 learning.py:507] global step 2053: loss = 1.0455 (1.271 sec/step)\n",
            "I0919 17:16:17.205318 140474388072320 learning.py:507] global step 2054: loss = 0.9271 (1.325 sec/step)\n",
            "I0919 17:16:18.502095 140474388072320 learning.py:507] global step 2055: loss = 1.0390 (1.295 sec/step)\n",
            "I0919 17:16:19.800659 140474388072320 learning.py:507] global step 2056: loss = 1.0673 (1.297 sec/step)\n",
            "I0919 17:16:21.110148 140474388072320 learning.py:507] global step 2057: loss = 0.9760 (1.308 sec/step)\n",
            "I0919 17:16:22.454804 140474388072320 learning.py:507] global step 2058: loss = 1.0971 (1.343 sec/step)\n",
            "I0919 17:16:23.763530 140474388072320 learning.py:507] global step 2059: loss = 0.7975 (1.307 sec/step)\n",
            "I0919 17:16:25.085994 140474388072320 learning.py:507] global step 2060: loss = 0.9340 (1.321 sec/step)\n",
            "I0919 17:16:26.401285 140474388072320 learning.py:507] global step 2061: loss = 0.9904 (1.314 sec/step)\n",
            "I0919 17:16:27.680454 140474388072320 learning.py:507] global step 2062: loss = 1.2102 (1.276 sec/step)\n",
            "I0919 17:16:29.925007 140471297337088 supervisor.py:1050] Recording summary at step 2063.\n",
            "I0919 17:16:29.938240 140474388072320 learning.py:507] global step 2063: loss = 1.0223 (2.251 sec/step)\n",
            "I0919 17:16:31.270227 140474388072320 learning.py:507] global step 2064: loss = 1.0210 (1.331 sec/step)\n",
            "I0919 17:16:32.618026 140474388072320 learning.py:507] global step 2065: loss = 1.0955 (1.346 sec/step)\n",
            "I0919 17:16:33.915745 140474388072320 learning.py:507] global step 2066: loss = 1.2861 (1.296 sec/step)\n",
            "I0919 17:16:35.205306 140474388072320 learning.py:507] global step 2067: loss = 0.9182 (1.288 sec/step)\n",
            "I0919 17:16:36.521339 140474388072320 learning.py:507] global step 2068: loss = 1.0481 (1.314 sec/step)\n",
            "I0919 17:16:37.837307 140474388072320 learning.py:507] global step 2069: loss = 1.2511 (1.314 sec/step)\n",
            "I0919 17:16:39.153966 140474388072320 learning.py:507] global step 2070: loss = 0.8233 (1.315 sec/step)\n",
            "I0919 17:16:40.454483 140474388072320 learning.py:507] global step 2071: loss = 1.0576 (1.299 sec/step)\n",
            "I0919 17:16:41.781014 140474388072320 learning.py:507] global step 2072: loss = 1.1306 (1.324 sec/step)\n",
            "I0919 17:16:43.131846 140474388072320 learning.py:507] global step 2073: loss = 1.2118 (1.349 sec/step)\n",
            "I0919 17:16:44.445146 140474388072320 learning.py:507] global step 2074: loss = 0.8947 (1.312 sec/step)\n",
            "I0919 17:16:45.779021 140474388072320 learning.py:507] global step 2075: loss = 0.9390 (1.332 sec/step)\n",
            "I0919 17:16:47.088263 140474388072320 learning.py:507] global step 2076: loss = 0.9950 (1.308 sec/step)\n",
            "I0919 17:16:48.417619 140474388072320 learning.py:507] global step 2077: loss = 0.9137 (1.328 sec/step)\n",
            "I0919 17:16:49.765386 140474388072320 learning.py:507] global step 2078: loss = 1.0468 (1.346 sec/step)\n",
            "I0919 17:16:51.049392 140474388072320 learning.py:507] global step 2079: loss = 0.9940 (1.282 sec/step)\n",
            "I0919 17:16:52.359544 140474388072320 learning.py:507] global step 2080: loss = 0.8658 (1.308 sec/step)\n",
            "I0919 17:16:53.662014 140474388072320 learning.py:507] global step 2081: loss = 0.8864 (1.301 sec/step)\n",
            "I0919 17:16:54.994468 140474388072320 learning.py:507] global step 2082: loss = 1.2495 (1.331 sec/step)\n",
            "I0919 17:16:56.355093 140474388072320 learning.py:507] global step 2083: loss = 1.0066 (1.359 sec/step)\n",
            "I0919 17:16:57.638526 140474388072320 learning.py:507] global step 2084: loss = 0.8098 (1.281 sec/step)\n",
            "I0919 17:16:58.988256 140474388072320 learning.py:507] global step 2085: loss = 0.8643 (1.348 sec/step)\n",
            "I0919 17:17:00.295828 140474388072320 learning.py:507] global step 2086: loss = 1.0431 (1.306 sec/step)\n",
            "I0919 17:17:01.617496 140474388072320 learning.py:507] global step 2087: loss = 0.9667 (1.319 sec/step)\n",
            "I0919 17:17:02.906965 140474388072320 learning.py:507] global step 2088: loss = 0.9654 (1.287 sec/step)\n",
            "I0919 17:17:04.207056 140474388072320 learning.py:507] global step 2089: loss = 0.8765 (1.298 sec/step)\n",
            "I0919 17:17:05.490093 140474388072320 learning.py:507] global step 2090: loss = 1.1214 (1.281 sec/step)\n",
            "I0919 17:17:06.831211 140474388072320 learning.py:507] global step 2091: loss = 0.8114 (1.340 sec/step)\n",
            "I0919 17:17:08.119225 140474388072320 learning.py:507] global step 2092: loss = 1.0807 (1.286 sec/step)\n",
            "I0919 17:17:09.474890 140474388072320 learning.py:507] global step 2093: loss = 0.9375 (1.354 sec/step)\n",
            "I0919 17:17:10.766633 140474388072320 learning.py:507] global step 2094: loss = 0.8407 (1.290 sec/step)\n",
            "I0919 17:17:12.064596 140474388072320 learning.py:507] global step 2095: loss = 1.2165 (1.296 sec/step)\n",
            "I0919 17:17:13.364900 140474388072320 learning.py:507] global step 2096: loss = 1.1521 (1.299 sec/step)\n",
            "I0919 17:17:14.675246 140474388072320 learning.py:507] global step 2097: loss = 0.9267 (1.308 sec/step)\n",
            "I0919 17:17:15.966237 140474388072320 learning.py:507] global step 2098: loss = 1.1439 (1.289 sec/step)\n",
            "I0919 17:17:17.273188 140474388072320 learning.py:507] global step 2099: loss = 0.8867 (1.305 sec/step)\n",
            "I0919 17:17:18.570991 140474388072320 learning.py:507] global step 2100: loss = 1.3432 (1.296 sec/step)\n",
            "I0919 17:17:19.934534 140474388072320 learning.py:507] global step 2101: loss = 1.2864 (1.362 sec/step)\n",
            "I0919 17:17:21.225989 140474388072320 learning.py:507] global step 2102: loss = 1.0188 (1.290 sec/step)\n",
            "I0919 17:17:22.588594 140474388072320 learning.py:507] global step 2103: loss = 1.2478 (1.361 sec/step)\n",
            "I0919 17:17:23.892391 140474388072320 learning.py:507] global step 2104: loss = 0.9115 (1.302 sec/step)\n",
            "I0919 17:17:25.207252 140474388072320 learning.py:507] global step 2105: loss = 0.9952 (1.313 sec/step)\n",
            "I0919 17:17:26.497175 140474388072320 learning.py:507] global step 2106: loss = 0.8960 (1.288 sec/step)\n",
            "I0919 17:17:27.812370 140474388072320 learning.py:507] global step 2107: loss = 0.9923 (1.313 sec/step)\n",
            "I0919 17:17:29.141274 140474388072320 learning.py:507] global step 2108: loss = 1.1264 (1.327 sec/step)\n",
            "I0919 17:17:30.423213 140474388072320 learning.py:507] global step 2109: loss = 0.9536 (1.280 sec/step)\n",
            "I0919 17:17:31.700795 140474388072320 learning.py:507] global step 2110: loss = 0.9155 (1.276 sec/step)\n",
            "I0919 17:17:32.995731 140474388072320 learning.py:507] global step 2111: loss = 0.8222 (1.293 sec/step)\n",
            "I0919 17:17:34.308213 140474388072320 learning.py:507] global step 2112: loss = 0.8094 (1.311 sec/step)\n",
            "I0919 17:17:35.667279 140474388072320 learning.py:507] global step 2113: loss = 1.0572 (1.357 sec/step)\n",
            "I0919 17:17:36.979721 140474388072320 learning.py:507] global step 2114: loss = 1.1231 (1.311 sec/step)\n",
            "I0919 17:17:38.305604 140474388072320 learning.py:507] global step 2115: loss = 1.1430 (1.324 sec/step)\n",
            "I0919 17:17:39.644541 140474388072320 learning.py:507] global step 2116: loss = 1.2433 (1.337 sec/step)\n",
            "I0919 17:17:40.921597 140474388072320 learning.py:507] global step 2117: loss = 0.9745 (1.275 sec/step)\n",
            "I0919 17:17:42.246011 140474388072320 learning.py:507] global step 2118: loss = 0.8084 (1.323 sec/step)\n",
            "I0919 17:17:43.563577 140474388072320 learning.py:507] global step 2119: loss = 1.0204 (1.316 sec/step)\n",
            "I0919 17:17:44.864285 140474388072320 learning.py:507] global step 2120: loss = 0.9507 (1.299 sec/step)\n",
            "I0919 17:17:46.169449 140474388072320 learning.py:507] global step 2121: loss = 0.8303 (1.303 sec/step)\n",
            "I0919 17:17:47.441584 140474388072320 learning.py:507] global step 2122: loss = 1.0534 (1.270 sec/step)\n",
            "I0919 17:17:48.740484 140474388072320 learning.py:507] global step 2123: loss = 1.1561 (1.297 sec/step)\n",
            "I0919 17:17:50.039526 140474388072320 learning.py:507] global step 2124: loss = 0.9163 (1.297 sec/step)\n",
            "I0919 17:17:51.326035 140474388072320 learning.py:507] global step 2125: loss = 1.0395 (1.285 sec/step)\n",
            "I0919 17:17:52.663217 140474388072320 learning.py:507] global step 2126: loss = 0.8718 (1.333 sec/step)\n",
            "I0919 17:17:53.975517 140474388072320 learning.py:507] global step 2127: loss = 0.8374 (1.308 sec/step)\n",
            "I0919 17:17:55.301661 140474388072320 learning.py:507] global step 2128: loss = 1.1016 (1.325 sec/step)\n",
            "I0919 17:17:56.593002 140474388072320 learning.py:507] global step 2129: loss = 1.0324 (1.290 sec/step)\n",
            "I0919 17:17:57.879930 140474388072320 learning.py:507] global step 2130: loss = 0.9230 (1.285 sec/step)\n",
            "I0919 17:17:59.164412 140474388072320 learning.py:507] global step 2131: loss = 1.0856 (1.283 sec/step)\n",
            "I0919 17:18:00.473642 140474388072320 learning.py:507] global step 2132: loss = 0.8745 (1.307 sec/step)\n",
            "I0919 17:18:01.777029 140474388072320 learning.py:507] global step 2133: loss = 0.9468 (1.302 sec/step)\n",
            "I0919 17:18:03.082864 140474388072320 learning.py:507] global step 2134: loss = 1.0304 (1.304 sec/step)\n",
            "I0919 17:18:04.421886 140474388072320 learning.py:507] global step 2135: loss = 0.9769 (1.337 sec/step)\n",
            "I0919 17:18:05.709852 140474388072320 learning.py:507] global step 2136: loss = 1.0042 (1.286 sec/step)\n",
            "I0919 17:18:07.021727 140474388072320 learning.py:507] global step 2137: loss = 0.9544 (1.310 sec/step)\n",
            "I0919 17:18:08.316790 140474388072320 learning.py:507] global step 2138: loss = 0.9180 (1.293 sec/step)\n",
            "I0919 17:18:09.601304 140474388072320 learning.py:507] global step 2139: loss = 1.0125 (1.283 sec/step)\n",
            "I0919 17:18:10.883769 140474388072320 learning.py:507] global step 2140: loss = 1.1247 (1.281 sec/step)\n",
            "I0919 17:18:12.199517 140474388072320 learning.py:507] global step 2141: loss = 0.9006 (1.313 sec/step)\n",
            "I0919 17:18:13.524709 140474388072320 learning.py:507] global step 2142: loss = 1.2456 (1.323 sec/step)\n",
            "I0919 17:18:14.839730 140474388072320 learning.py:507] global step 2143: loss = 1.0151 (1.313 sec/step)\n",
            "I0919 17:18:16.173439 140474388072320 learning.py:507] global step 2144: loss = 0.9738 (1.332 sec/step)\n",
            "I0919 17:18:17.472589 140474388072320 learning.py:507] global step 2145: loss = 1.1951 (1.297 sec/step)\n",
            "I0919 17:18:18.794297 140474388072320 learning.py:507] global step 2146: loss = 0.8676 (1.320 sec/step)\n",
            "I0919 17:18:20.063282 140474388072320 learning.py:507] global step 2147: loss = 0.6916 (1.267 sec/step)\n",
            "I0919 17:18:21.354839 140474388072320 learning.py:507] global step 2148: loss = 0.8298 (1.290 sec/step)\n",
            "I0919 17:18:22.669348 140474388072320 learning.py:507] global step 2149: loss = 0.9255 (1.313 sec/step)\n",
            "I0919 17:18:23.991063 140474388072320 learning.py:507] global step 2150: loss = 1.1538 (1.320 sec/step)\n",
            "I0919 17:18:25.260806 140474388072320 learning.py:507] global step 2151: loss = 1.0638 (1.268 sec/step)\n",
            "I0919 17:18:26.591743 140474388072320 learning.py:507] global step 2152: loss = 1.1231 (1.329 sec/step)\n",
            "I0919 17:18:28.423719 140471297337088 supervisor.py:1050] Recording summary at step 2152.\n",
            "I0919 17:18:28.846097 140474388072320 learning.py:507] global step 2153: loss = 0.8516 (2.253 sec/step)\n",
            "I0919 17:18:30.137025 140474388072320 learning.py:507] global step 2154: loss = 0.9431 (1.289 sec/step)\n",
            "I0919 17:18:31.405060 140474388072320 learning.py:507] global step 2155: loss = 0.9488 (1.266 sec/step)\n",
            "I0919 17:18:32.701452 140474388072320 learning.py:507] global step 2156: loss = 1.2629 (1.295 sec/step)\n",
            "I0919 17:18:34.024081 140474388072320 learning.py:507] global step 2157: loss = 0.9037 (1.321 sec/step)\n",
            "I0919 17:18:35.368757 140474388072320 learning.py:507] global step 2158: loss = 0.9451 (1.343 sec/step)\n",
            "I0919 17:18:36.658890 140474388072320 learning.py:507] global step 2159: loss = 0.9223 (1.289 sec/step)\n",
            "I0919 17:18:37.987504 140474388072320 learning.py:507] global step 2160: loss = 1.2408 (1.327 sec/step)\n",
            "I0919 17:18:39.318990 140474388072320 learning.py:507] global step 2161: loss = 1.2562 (1.330 sec/step)\n",
            "I0919 17:18:40.631072 140474388072320 learning.py:507] global step 2162: loss = 1.1053 (1.310 sec/step)\n",
            "I0919 17:18:41.946321 140474388072320 learning.py:507] global step 2163: loss = 1.0053 (1.314 sec/step)\n",
            "I0919 17:18:43.230386 140474388072320 learning.py:507] global step 2164: loss = 1.6941 (1.282 sec/step)\n",
            "I0919 17:18:44.513916 140474388072320 learning.py:507] global step 2165: loss = 0.7819 (1.282 sec/step)\n",
            "I0919 17:18:45.823050 140474388072320 learning.py:507] global step 2166: loss = 0.8545 (1.307 sec/step)\n",
            "I0919 17:18:47.122889 140474388072320 learning.py:507] global step 2167: loss = 1.0511 (1.298 sec/step)\n",
            "I0919 17:18:48.449363 140474388072320 learning.py:507] global step 2168: loss = 1.0303 (1.325 sec/step)\n",
            "I0919 17:18:49.738092 140474388072320 learning.py:507] global step 2169: loss = 0.9694 (1.287 sec/step)\n",
            "I0919 17:18:51.041583 140474388072320 learning.py:507] global step 2170: loss = 0.8309 (1.301 sec/step)\n",
            "I0919 17:18:52.387345 140474388072320 learning.py:507] global step 2171: loss = 0.9364 (1.344 sec/step)\n",
            "I0919 17:18:53.708910 140474388072320 learning.py:507] global step 2172: loss = 0.9286 (1.319 sec/step)\n",
            "I0919 17:18:55.012773 140474388072320 learning.py:507] global step 2173: loss = 0.8393 (1.302 sec/step)\n",
            "I0919 17:18:56.322504 140474388072320 learning.py:507] global step 2174: loss = 0.8519 (1.308 sec/step)\n",
            "I0919 17:18:57.656528 140474388072320 learning.py:507] global step 2175: loss = 0.9383 (1.332 sec/step)\n",
            "I0919 17:18:58.965099 140474388072320 learning.py:507] global step 2176: loss = 0.8710 (1.307 sec/step)\n",
            "I0919 17:19:00.300266 140474388072320 learning.py:507] global step 2177: loss = 1.0035 (1.333 sec/step)\n",
            "I0919 17:19:01.585199 140474388072320 learning.py:507] global step 2178: loss = 0.8987 (1.283 sec/step)\n",
            "I0919 17:19:02.865961 140474388072320 learning.py:507] global step 2179: loss = 0.9336 (1.279 sec/step)\n",
            "I0919 17:19:04.164549 140474388072320 learning.py:507] global step 2180: loss = 1.1377 (1.297 sec/step)\n",
            "I0919 17:19:05.481445 140474388072320 learning.py:507] global step 2181: loss = 0.9060 (1.315 sec/step)\n",
            "I0919 17:19:06.775177 140474388072320 learning.py:507] global step 2182: loss = 0.8737 (1.292 sec/step)\n",
            "I0919 17:19:08.085041 140474388072320 learning.py:507] global step 2183: loss = 0.9661 (1.308 sec/step)\n",
            "I0919 17:19:09.371267 140474388072320 learning.py:507] global step 2184: loss = 0.8344 (1.284 sec/step)\n",
            "I0919 17:19:10.661725 140474388072320 learning.py:507] global step 2185: loss = 0.7978 (1.289 sec/step)\n",
            "I0919 17:19:11.987610 140474388072320 learning.py:507] global step 2186: loss = 1.7083 (1.324 sec/step)\n",
            "I0919 17:19:13.298657 140474388072320 learning.py:507] global step 2187: loss = 1.0157 (1.309 sec/step)\n",
            "I0919 17:19:14.573945 140474388072320 learning.py:507] global step 2188: loss = 1.0468 (1.274 sec/step)\n",
            "I0919 17:19:15.874871 140474388072320 learning.py:507] global step 2189: loss = 1.1236 (1.299 sec/step)\n",
            "I0919 17:19:17.176446 140474388072320 learning.py:507] global step 2190: loss = 0.8676 (1.300 sec/step)\n",
            "I0919 17:19:18.513367 140474388072320 learning.py:507] global step 2191: loss = 0.9095 (1.335 sec/step)\n",
            "I0919 17:19:19.852643 140474388072320 learning.py:507] global step 2192: loss = 0.9228 (1.337 sec/step)\n",
            "I0919 17:19:21.160600 140474388072320 learning.py:507] global step 2193: loss = 0.8783 (1.306 sec/step)\n",
            "I0919 17:19:22.472476 140474388072320 learning.py:507] global step 2194: loss = 0.8087 (1.310 sec/step)\n",
            "I0919 17:19:23.764636 140474388072320 learning.py:507] global step 2195: loss = 1.1076 (1.290 sec/step)\n",
            "I0919 17:19:25.108171 140474388072320 learning.py:507] global step 2196: loss = 1.0696 (1.342 sec/step)\n",
            "I0919 17:19:26.408219 140474388072320 learning.py:507] global step 2197: loss = 0.9647 (1.298 sec/step)\n",
            "I0919 17:19:27.733740 140474388072320 learning.py:507] global step 2198: loss = 0.9236 (1.324 sec/step)\n",
            "I0919 17:19:29.076812 140474388072320 learning.py:507] global step 2199: loss = 1.0510 (1.341 sec/step)\n",
            "I0919 17:19:30.387402 140474388072320 learning.py:507] global step 2200: loss = 1.0449 (1.309 sec/step)\n",
            "I0919 17:19:31.695096 140474388072320 learning.py:507] global step 2201: loss = 0.9654 (1.305 sec/step)\n",
            "I0919 17:19:33.004739 140474388072320 learning.py:507] global step 2202: loss = 0.9177 (1.304 sec/step)\n",
            "I0919 17:19:34.335818 140474388072320 learning.py:507] global step 2203: loss = 0.8584 (1.329 sec/step)\n",
            "I0919 17:19:35.617501 140474388072320 learning.py:507] global step 2204: loss = 0.8509 (1.280 sec/step)\n",
            "I0919 17:19:36.943974 140474388072320 learning.py:507] global step 2205: loss = 1.2602 (1.324 sec/step)\n",
            "I0919 17:19:38.256350 140474388072320 learning.py:507] global step 2206: loss = 0.8278 (1.311 sec/step)\n",
            "I0919 17:19:39.564069 140474388072320 learning.py:507] global step 2207: loss = 0.9409 (1.306 sec/step)\n",
            "I0919 17:19:40.905891 140474388072320 learning.py:507] global step 2208: loss = 0.9955 (1.340 sec/step)\n",
            "I0919 17:19:42.208240 140474388072320 learning.py:507] global step 2209: loss = 1.0362 (1.301 sec/step)\n",
            "I0919 17:19:43.556931 140474388072320 learning.py:507] global step 2210: loss = 0.8938 (1.347 sec/step)\n",
            "I0919 17:19:44.871412 140474388072320 learning.py:507] global step 2211: loss = 0.8864 (1.313 sec/step)\n",
            "I0919 17:19:46.182239 140474388072320 learning.py:507] global step 2212: loss = 0.8908 (1.309 sec/step)\n",
            "I0919 17:19:47.481400 140474388072320 learning.py:507] global step 2213: loss = 0.8935 (1.298 sec/step)\n",
            "I0919 17:19:48.812592 140474388072320 learning.py:507] global step 2214: loss = 1.1705 (1.329 sec/step)\n",
            "I0919 17:19:50.125587 140474388072320 learning.py:507] global step 2215: loss = 1.0977 (1.311 sec/step)\n",
            "I0919 17:19:51.409493 140474388072320 learning.py:507] global step 2216: loss = 0.8779 (1.282 sec/step)\n",
            "I0919 17:19:52.736654 140474388072320 learning.py:507] global step 2217: loss = 0.9753 (1.325 sec/step)\n",
            "I0919 17:19:54.074325 140474388072320 learning.py:507] global step 2218: loss = 0.9979 (1.336 sec/step)\n",
            "I0919 17:19:55.385283 140474388072320 learning.py:507] global step 2219: loss = 1.1162 (1.309 sec/step)\n",
            "I0919 17:19:56.735150 140474388072320 learning.py:507] global step 2220: loss = 1.1572 (1.348 sec/step)\n",
            "I0919 17:19:58.087758 140474388072320 learning.py:507] global step 2221: loss = 0.9066 (1.350 sec/step)\n",
            "I0919 17:19:59.433739 140474388072320 learning.py:507] global step 2222: loss = 1.0714 (1.344 sec/step)\n",
            "I0919 17:20:00.727257 140474388072320 learning.py:507] global step 2223: loss = 0.7614 (1.292 sec/step)\n",
            "I0919 17:20:02.011488 140474388072320 learning.py:507] global step 2224: loss = 0.9545 (1.282 sec/step)\n",
            "I0919 17:20:03.312328 140474388072320 learning.py:507] global step 2225: loss = 1.0944 (1.299 sec/step)\n",
            "I0919 17:20:04.646970 140474388072320 learning.py:507] global step 2226: loss = 0.8850 (1.333 sec/step)\n",
            "I0919 17:20:05.969245 140474388072320 learning.py:507] global step 2227: loss = 1.3512 (1.320 sec/step)\n",
            "I0919 17:20:07.281172 140474388072320 learning.py:507] global step 2228: loss = 0.9343 (1.310 sec/step)\n",
            "I0919 17:20:08.613720 140474388072320 learning.py:507] global step 2229: loss = 1.0745 (1.331 sec/step)\n",
            "I0919 17:20:09.956927 140474388072320 learning.py:507] global step 2230: loss = 0.9142 (1.341 sec/step)\n",
            "I0919 17:20:11.302170 140474388072320 learning.py:507] global step 2231: loss = 1.0997 (1.343 sec/step)\n",
            "I0919 17:20:12.642400 140474388072320 learning.py:507] global step 2232: loss = 0.8587 (1.338 sec/step)\n",
            "I0919 17:20:13.943101 140474388072320 learning.py:507] global step 2233: loss = 1.1329 (1.299 sec/step)\n",
            "I0919 17:20:15.247039 140474388072320 learning.py:507] global step 2234: loss = 1.0279 (1.302 sec/step)\n",
            "I0919 17:20:16.562199 140474388072320 learning.py:507] global step 2235: loss = 1.0108 (1.313 sec/step)\n",
            "I0919 17:20:17.880763 140474388072320 learning.py:507] global step 2236: loss = 1.1890 (1.317 sec/step)\n",
            "I0919 17:20:19.228666 140474388072320 learning.py:507] global step 2237: loss = 0.7771 (1.346 sec/step)\n",
            "I0919 17:20:20.544955 140474388072320 learning.py:507] global step 2238: loss = 0.8502 (1.314 sec/step)\n",
            "I0919 17:20:21.842787 140474388072320 learning.py:507] global step 2239: loss = 0.8329 (1.296 sec/step)\n",
            "I0919 17:20:23.154160 140474388072320 learning.py:507] global step 2240: loss = 0.8945 (1.310 sec/step)\n",
            "I0919 17:20:24.436012 140474388072320 learning.py:507] global step 2241: loss = 1.1210 (1.280 sec/step)\n",
            "I0919 17:20:25.745522 140474388072320 learning.py:507] global step 2242: loss = 1.1361 (1.308 sec/step)\n",
            "I0919 17:20:26.873002 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "W0919 17:20:27.200982 140471314122496 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I0919 17:20:27.248060 140474388072320 learning.py:507] global step 2243: loss = 0.8602 (1.308 sec/step)\n",
            "I0919 17:20:29.548526 140471297337088 supervisor.py:1050] Recording summary at step 2243.\n",
            "I0919 17:20:30.250075 140474388072320 learning.py:507] global step 2244: loss = 1.0090 (2.982 sec/step)\n",
            "I0919 17:20:32.216278 140474388072320 learning.py:507] global step 2245: loss = 0.6915 (1.952 sec/step)\n",
            "I0919 17:20:33.526052 140474388072320 learning.py:507] global step 2246: loss = 0.7367 (1.308 sec/step)\n",
            "I0919 17:20:34.836606 140474388072320 learning.py:507] global step 2247: loss = 0.8437 (1.309 sec/step)\n",
            "I0919 17:20:36.127947 140474388072320 learning.py:507] global step 2248: loss = 0.9303 (1.289 sec/step)\n",
            "I0919 17:20:37.425263 140474388072320 learning.py:507] global step 2249: loss = 1.1884 (1.296 sec/step)\n",
            "I0919 17:20:38.737617 140474388072320 learning.py:507] global step 2250: loss = 0.8569 (1.311 sec/step)\n",
            "I0919 17:20:40.017265 140474388072320 learning.py:507] global step 2251: loss = 0.8899 (1.278 sec/step)\n",
            "I0919 17:20:41.344883 140474388072320 learning.py:507] global step 2252: loss = 1.2614 (1.325 sec/step)\n",
            "I0919 17:20:42.666877 140474388072320 learning.py:507] global step 2253: loss = 0.9224 (1.320 sec/step)\n",
            "I0919 17:20:43.975747 140474388072320 learning.py:507] global step 2254: loss = 0.9650 (1.307 sec/step)\n",
            "I0919 17:20:45.315378 140474388072320 learning.py:507] global step 2255: loss = 1.2012 (1.338 sec/step)\n",
            "I0919 17:20:46.614972 140474388072320 learning.py:507] global step 2256: loss = 0.8693 (1.298 sec/step)\n",
            "I0919 17:20:47.928822 140474388072320 learning.py:507] global step 2257: loss = 1.1766 (1.312 sec/step)\n",
            "I0919 17:20:49.258321 140474388072320 learning.py:507] global step 2258: loss = 1.0838 (1.328 sec/step)\n",
            "I0919 17:20:50.604015 140474388072320 learning.py:507] global step 2259: loss = 0.8976 (1.344 sec/step)\n",
            "I0919 17:20:51.898241 140474388072320 learning.py:507] global step 2260: loss = 1.1043 (1.292 sec/step)\n",
            "I0919 17:20:53.223424 140474388072320 learning.py:507] global step 2261: loss = 0.9611 (1.324 sec/step)\n",
            "I0919 17:20:54.579694 140474388072320 learning.py:507] global step 2262: loss = 0.8757 (1.355 sec/step)\n",
            "I0919 17:20:55.885000 140474388072320 learning.py:507] global step 2263: loss = 1.3233 (1.304 sec/step)\n",
            "I0919 17:20:57.215919 140474388072320 learning.py:507] global step 2264: loss = 0.9940 (1.329 sec/step)\n",
            "I0919 17:20:58.504461 140474388072320 learning.py:507] global step 2265: loss = 0.7423 (1.287 sec/step)\n",
            "I0919 17:20:59.794919 140474388072320 learning.py:507] global step 2266: loss = 1.4574 (1.289 sec/step)\n",
            "I0919 17:21:01.100430 140474388072320 learning.py:507] global step 2267: loss = 0.9909 (1.304 sec/step)\n",
            "I0919 17:21:02.407967 140474388072320 learning.py:507] global step 2268: loss = 0.8850 (1.306 sec/step)\n",
            "I0919 17:21:03.727581 140474388072320 learning.py:507] global step 2269: loss = 0.9188 (1.318 sec/step)\n",
            "I0919 17:21:05.036731 140474388072320 learning.py:507] global step 2270: loss = 1.0755 (1.307 sec/step)\n",
            "I0919 17:21:06.330590 140474388072320 learning.py:507] global step 2271: loss = 0.8742 (1.292 sec/step)\n",
            "I0919 17:21:07.630780 140474388072320 learning.py:507] global step 2272: loss = 0.8309 (1.298 sec/step)\n",
            "I0919 17:21:08.953691 140474388072320 learning.py:507] global step 2273: loss = 1.0190 (1.321 sec/step)\n",
            "I0919 17:21:10.230309 140474388072320 learning.py:507] global step 2274: loss = 0.8561 (1.275 sec/step)\n",
            "I0919 17:21:11.555316 140474388072320 learning.py:507] global step 2275: loss = 0.8571 (1.323 sec/step)\n",
            "I0919 17:21:12.824019 140474388072320 learning.py:507] global step 2276: loss = 1.0555 (1.267 sec/step)\n",
            "I0919 17:21:14.144229 140474388072320 learning.py:507] global step 2277: loss = 0.7229 (1.319 sec/step)\n",
            "I0919 17:21:15.453540 140474388072320 learning.py:507] global step 2278: loss = 1.0847 (1.308 sec/step)\n",
            "I0919 17:21:16.751753 140474388072320 learning.py:507] global step 2279: loss = 1.0675 (1.297 sec/step)\n",
            "I0919 17:21:18.051877 140474388072320 learning.py:507] global step 2280: loss = 0.9526 (1.299 sec/step)\n",
            "I0919 17:21:19.364800 140474388072320 learning.py:507] global step 2281: loss = 1.5227 (1.311 sec/step)\n",
            "I0919 17:21:20.682243 140474388072320 learning.py:507] global step 2282: loss = 1.1011 (1.316 sec/step)\n",
            "I0919 17:21:21.979899 140474388072320 learning.py:507] global step 2283: loss = 1.0275 (1.296 sec/step)\n",
            "I0919 17:21:23.286909 140474388072320 learning.py:507] global step 2284: loss = 1.0385 (1.305 sec/step)\n",
            "I0919 17:21:24.603460 140474388072320 learning.py:507] global step 2285: loss = 0.9937 (1.315 sec/step)\n",
            "I0919 17:21:25.929008 140474388072320 learning.py:507] global step 2286: loss = 0.7464 (1.324 sec/step)\n",
            "I0919 17:21:27.206252 140474388072320 learning.py:507] global step 2287: loss = 0.9838 (1.276 sec/step)\n",
            "I0919 17:21:28.517400 140474388072320 learning.py:507] global step 2288: loss = 1.1080 (1.309 sec/step)\n",
            "I0919 17:21:29.835560 140474388072320 learning.py:507] global step 2289: loss = 1.0424 (1.317 sec/step)\n",
            "I0919 17:21:31.156830 140474388072320 learning.py:507] global step 2290: loss = 0.9935 (1.319 sec/step)\n",
            "I0919 17:21:32.461468 140474388072320 learning.py:507] global step 2291: loss = 0.9836 (1.303 sec/step)\n",
            "I0919 17:21:33.735578 140474388072320 learning.py:507] global step 2292: loss = 0.8659 (1.273 sec/step)\n",
            "I0919 17:21:35.102711 140474388072320 learning.py:507] global step 2293: loss = 1.0978 (1.365 sec/step)\n",
            "I0919 17:21:36.418953 140474388072320 learning.py:507] global step 2294: loss = 0.8450 (1.314 sec/step)\n",
            "I0919 17:21:37.738320 140474388072320 learning.py:507] global step 2295: loss = 0.8548 (1.317 sec/step)\n",
            "I0919 17:21:39.095876 140474388072320 learning.py:507] global step 2296: loss = 0.9128 (1.356 sec/step)\n",
            "I0919 17:21:40.415462 140474388072320 learning.py:507] global step 2297: loss = 0.9995 (1.318 sec/step)\n",
            "I0919 17:21:41.722318 140474388072320 learning.py:507] global step 2298: loss = 0.9018 (1.305 sec/step)\n",
            "I0919 17:21:43.036039 140474388072320 learning.py:507] global step 2299: loss = 0.7858 (1.312 sec/step)\n",
            "I0919 17:21:44.341310 140474388072320 learning.py:507] global step 2300: loss = 0.9895 (1.303 sec/step)\n",
            "I0919 17:21:45.635286 140474388072320 learning.py:507] global step 2301: loss = 1.0189 (1.292 sec/step)\n",
            "I0919 17:21:46.965960 140474388072320 learning.py:507] global step 2302: loss = 1.0758 (1.329 sec/step)\n",
            "I0919 17:21:48.307104 140474388072320 learning.py:507] global step 2303: loss = 0.7810 (1.339 sec/step)\n",
            "I0919 17:21:49.586472 140474388072320 learning.py:507] global step 2304: loss = 0.9538 (1.277 sec/step)\n",
            "I0919 17:21:50.867253 140474388072320 learning.py:507] global step 2305: loss = 0.7378 (1.279 sec/step)\n",
            "I0919 17:21:52.146783 140474388072320 learning.py:507] global step 2306: loss = 1.0580 (1.278 sec/step)\n",
            "I0919 17:21:53.441965 140474388072320 learning.py:507] global step 2307: loss = 0.9689 (1.293 sec/step)\n",
            "I0919 17:21:54.778187 140474388072320 learning.py:507] global step 2308: loss = 0.9071 (1.334 sec/step)\n",
            "I0919 17:21:56.075777 140474388072320 learning.py:507] global step 2309: loss = 0.9266 (1.296 sec/step)\n",
            "I0919 17:21:57.372848 140474388072320 learning.py:507] global step 2310: loss = 1.0736 (1.295 sec/step)\n",
            "I0919 17:21:58.662879 140474388072320 learning.py:507] global step 2311: loss = 0.8671 (1.288 sec/step)\n",
            "I0919 17:21:59.972093 140474388072320 learning.py:507] global step 2312: loss = 1.0321 (1.307 sec/step)\n",
            "I0919 17:22:01.316842 140474388072320 learning.py:507] global step 2313: loss = 1.6079 (1.343 sec/step)\n",
            "I0919 17:22:02.626075 140474388072320 learning.py:507] global step 2314: loss = 0.8402 (1.307 sec/step)\n",
            "I0919 17:22:03.918755 140474388072320 learning.py:507] global step 2315: loss = 0.9542 (1.291 sec/step)\n",
            "I0919 17:22:05.214472 140474388072320 learning.py:507] global step 2316: loss = 0.9170 (1.294 sec/step)\n",
            "I0919 17:22:06.585024 140474388072320 learning.py:507] global step 2317: loss = 1.0296 (1.369 sec/step)\n",
            "I0919 17:22:07.888083 140474388072320 learning.py:507] global step 2318: loss = 0.9074 (1.301 sec/step)\n",
            "I0919 17:22:09.258759 140474388072320 learning.py:507] global step 2319: loss = 1.0719 (1.369 sec/step)\n",
            "I0919 17:22:10.585952 140474388072320 learning.py:507] global step 2320: loss = 1.0352 (1.325 sec/step)\n",
            "I0919 17:22:11.907455 140474388072320 learning.py:507] global step 2321: loss = 0.9032 (1.320 sec/step)\n",
            "I0919 17:22:13.246357 140474388072320 learning.py:507] global step 2322: loss = 0.9339 (1.337 sec/step)\n",
            "I0919 17:22:14.528594 140474388072320 learning.py:507] global step 2323: loss = 0.9887 (1.281 sec/step)\n",
            "I0919 17:22:15.859313 140474388072320 learning.py:507] global step 2324: loss = 1.1326 (1.329 sec/step)\n",
            "I0919 17:22:17.162236 140474388072320 learning.py:507] global step 2325: loss = 0.8802 (1.301 sec/step)\n",
            "I0919 17:22:18.462857 140474388072320 learning.py:507] global step 2326: loss = 1.0742 (1.299 sec/step)\n",
            "I0919 17:22:19.791671 140474388072320 learning.py:507] global step 2327: loss = 1.0390 (1.327 sec/step)\n",
            "I0919 17:22:21.097740 140474388072320 learning.py:507] global step 2328: loss = 0.9574 (1.304 sec/step)\n",
            "I0919 17:22:22.386831 140474388072320 learning.py:507] global step 2329: loss = 0.7719 (1.288 sec/step)\n",
            "I0919 17:22:23.700731 140474388072320 learning.py:507] global step 2330: loss = 1.1414 (1.312 sec/step)\n",
            "I0919 17:22:24.991503 140474388072320 learning.py:507] global step 2331: loss = 0.8278 (1.289 sec/step)\n",
            "I0919 17:22:26.280412 140474388072320 learning.py:507] global step 2332: loss = 0.7810 (1.287 sec/step)\n",
            "I0919 17:22:27.595326 140474388072320 learning.py:507] global step 2333: loss = 0.6905 (1.313 sec/step)\n",
            "I0919 17:22:29.385601 140471297337088 supervisor.py:1050] Recording summary at step 2333.\n",
            "I0919 17:22:29.817720 140474388072320 learning.py:507] global step 2334: loss = 0.9587 (2.004 sec/step)\n",
            "I0919 17:22:31.129729 140474388072320 learning.py:507] global step 2335: loss = 0.9991 (1.310 sec/step)\n",
            "I0919 17:22:32.455657 140474388072320 learning.py:507] global step 2336: loss = 0.7790 (1.324 sec/step)\n",
            "I0919 17:22:33.788084 140474388072320 learning.py:507] global step 2337: loss = 1.2779 (1.330 sec/step)\n",
            "I0919 17:22:35.107986 140474388072320 learning.py:507] global step 2338: loss = 1.1826 (1.317 sec/step)\n",
            "I0919 17:22:36.426961 140474388072320 learning.py:507] global step 2339: loss = 1.0309 (1.317 sec/step)\n",
            "I0919 17:22:37.746929 140474388072320 learning.py:507] global step 2340: loss = 0.9496 (1.318 sec/step)\n",
            "I0919 17:22:39.046377 140474388072320 learning.py:507] global step 2341: loss = 1.1777 (1.297 sec/step)\n",
            "I0919 17:22:40.350799 140474388072320 learning.py:507] global step 2342: loss = 0.9526 (1.303 sec/step)\n",
            "I0919 17:22:41.653753 140474388072320 learning.py:507] global step 2343: loss = 0.8258 (1.301 sec/step)\n",
            "I0919 17:22:42.966377 140474388072320 learning.py:507] global step 2344: loss = 1.0888 (1.311 sec/step)\n",
            "I0919 17:22:44.250087 140474388072320 learning.py:507] global step 2345: loss = 1.2922 (1.282 sec/step)\n",
            "I0919 17:22:45.548547 140474388072320 learning.py:507] global step 2346: loss = 0.9904 (1.296 sec/step)\n",
            "I0919 17:22:46.860714 140474388072320 learning.py:507] global step 2347: loss = 1.3941 (1.311 sec/step)\n",
            "I0919 17:22:48.150259 140474388072320 learning.py:507] global step 2348: loss = 0.8920 (1.288 sec/step)\n",
            "I0919 17:22:49.435039 140474388072320 learning.py:507] global step 2349: loss = 1.1151 (1.283 sec/step)\n",
            "I0919 17:22:50.719748 140474388072320 learning.py:507] global step 2350: loss = 0.8594 (1.283 sec/step)\n",
            "I0919 17:22:52.030776 140474388072320 learning.py:507] global step 2351: loss = 0.9239 (1.309 sec/step)\n",
            "I0919 17:22:53.305828 140474388072320 learning.py:507] global step 2352: loss = 0.8101 (1.274 sec/step)\n",
            "I0919 17:22:54.601538 140474388072320 learning.py:507] global step 2353: loss = 0.7279 (1.294 sec/step)\n",
            "I0919 17:22:55.918970 140474388072320 learning.py:507] global step 2354: loss = 0.9013 (1.315 sec/step)\n",
            "I0919 17:22:57.253209 140474388072320 learning.py:507] global step 2355: loss = 0.9453 (1.333 sec/step)\n",
            "I0919 17:22:58.561595 140474388072320 learning.py:507] global step 2356: loss = 1.1636 (1.307 sec/step)\n",
            "I0919 17:22:59.894426 140474388072320 learning.py:507] global step 2357: loss = 0.9429 (1.331 sec/step)\n",
            "I0919 17:23:01.189379 140474388072320 learning.py:507] global step 2358: loss = 1.5722 (1.293 sec/step)\n",
            "I0919 17:23:02.557440 140474388072320 learning.py:507] global step 2359: loss = 0.8416 (1.366 sec/step)\n",
            "I0919 17:23:03.849617 140474388072320 learning.py:507] global step 2360: loss = 0.7796 (1.291 sec/step)\n",
            "I0919 17:23:05.138710 140474388072320 learning.py:507] global step 2361: loss = 0.7207 (1.288 sec/step)\n",
            "I0919 17:23:06.442280 140474388072320 learning.py:507] global step 2362: loss = 0.8650 (1.302 sec/step)\n",
            "I0919 17:23:07.747025 140474388072320 learning.py:507] global step 2363: loss = 0.9949 (1.303 sec/step)\n",
            "I0919 17:23:09.048821 140474388072320 learning.py:507] global step 2364: loss = 0.9846 (1.300 sec/step)\n",
            "I0919 17:23:10.347292 140474388072320 learning.py:507] global step 2365: loss = 1.0191 (1.297 sec/step)\n",
            "I0919 17:23:11.621654 140474388072320 learning.py:507] global step 2366: loss = 0.8457 (1.272 sec/step)\n",
            "I0919 17:23:12.910825 140474388072320 learning.py:507] global step 2367: loss = 0.7765 (1.287 sec/step)\n",
            "I0919 17:23:14.204318 140474388072320 learning.py:507] global step 2368: loss = 0.8377 (1.292 sec/step)\n",
            "I0919 17:23:15.536712 140474388072320 learning.py:507] global step 2369: loss = 0.8714 (1.331 sec/step)\n",
            "I0919 17:23:16.830404 140474388072320 learning.py:507] global step 2370: loss = 0.8287 (1.292 sec/step)\n",
            "I0919 17:23:18.132631 140474388072320 learning.py:507] global step 2371: loss = 0.8996 (1.300 sec/step)\n",
            "I0919 17:23:19.430982 140474388072320 learning.py:507] global step 2372: loss = 0.8214 (1.296 sec/step)\n",
            "I0919 17:23:20.713812 140474388072320 learning.py:507] global step 2373: loss = 1.0010 (1.281 sec/step)\n",
            "I0919 17:23:22.045984 140474388072320 learning.py:507] global step 2374: loss = 1.4704 (1.330 sec/step)\n",
            "I0919 17:23:23.349350 140474388072320 learning.py:507] global step 2375: loss = 0.8073 (1.302 sec/step)\n",
            "I0919 17:23:24.690186 140474388072320 learning.py:507] global step 2376: loss = 1.0137 (1.339 sec/step)\n",
            "I0919 17:23:26.011190 140474388072320 learning.py:507] global step 2377: loss = 0.9939 (1.319 sec/step)\n",
            "I0919 17:23:27.364027 140474388072320 learning.py:507] global step 2378: loss = 0.9578 (1.351 sec/step)\n",
            "I0919 17:23:28.664907 140474388072320 learning.py:507] global step 2379: loss = 0.8725 (1.299 sec/step)\n",
            "I0919 17:23:29.953058 140474388072320 learning.py:507] global step 2380: loss = 0.9311 (1.287 sec/step)\n",
            "I0919 17:23:31.274147 140474388072320 learning.py:507] global step 2381: loss = 1.1916 (1.319 sec/step)\n",
            "I0919 17:23:32.548402 140474388072320 learning.py:507] global step 2382: loss = 1.1883 (1.273 sec/step)\n",
            "I0919 17:23:33.894757 140474388072320 learning.py:507] global step 2383: loss = 0.8859 (1.345 sec/step)\n",
            "I0919 17:23:35.214921 140474388072320 learning.py:507] global step 2384: loss = 0.8407 (1.318 sec/step)\n",
            "I0919 17:23:36.531500 140474388072320 learning.py:507] global step 2385: loss = 1.1801 (1.315 sec/step)\n",
            "I0919 17:23:37.829893 140474388072320 learning.py:507] global step 2386: loss = 1.0941 (1.297 sec/step)\n",
            "I0919 17:23:39.145904 140474388072320 learning.py:507] global step 2387: loss = 0.8254 (1.314 sec/step)\n",
            "I0919 17:23:40.459448 140474388072320 learning.py:507] global step 2388: loss = 1.0185 (1.312 sec/step)\n",
            "I0919 17:23:41.779875 140474388072320 learning.py:507] global step 2389: loss = 0.9406 (1.319 sec/step)\n",
            "I0919 17:23:43.073072 140474388072320 learning.py:507] global step 2390: loss = 0.9005 (1.291 sec/step)\n",
            "I0919 17:23:44.403861 140474388072320 learning.py:507] global step 2391: loss = 0.9973 (1.329 sec/step)\n",
            "I0919 17:23:45.714911 140474388072320 learning.py:507] global step 2392: loss = 1.0527 (1.309 sec/step)\n",
            "I0919 17:23:46.998198 140474388072320 learning.py:507] global step 2393: loss = 0.9397 (1.282 sec/step)\n",
            "I0919 17:23:48.323803 140474388072320 learning.py:507] global step 2394: loss = 1.0860 (1.324 sec/step)\n",
            "I0919 17:23:49.672091 140474388072320 learning.py:507] global step 2395: loss = 0.9475 (1.346 sec/step)\n",
            "I0919 17:23:50.999675 140474388072320 learning.py:507] global step 2396: loss = 1.1578 (1.326 sec/step)\n",
            "I0919 17:23:52.313466 140474388072320 learning.py:507] global step 2397: loss = 1.3431 (1.312 sec/step)\n",
            "I0919 17:23:53.620661 140474388072320 learning.py:507] global step 2398: loss = 0.8695 (1.306 sec/step)\n",
            "I0919 17:23:54.953695 140474388072320 learning.py:507] global step 2399: loss = 1.0161 (1.331 sec/step)\n",
            "I0919 17:23:56.264447 140474388072320 learning.py:507] global step 2400: loss = 0.8214 (1.309 sec/step)\n",
            "I0919 17:23:57.576869 140474388072320 learning.py:507] global step 2401: loss = 0.8341 (1.311 sec/step)\n",
            "I0919 17:23:58.883969 140474388072320 learning.py:507] global step 2402: loss = 0.8343 (1.305 sec/step)\n",
            "I0919 17:24:00.205384 140474388072320 learning.py:507] global step 2403: loss = 1.0455 (1.319 sec/step)\n",
            "I0919 17:24:01.524285 140474388072320 learning.py:507] global step 2404: loss = 0.9394 (1.317 sec/step)\n",
            "I0919 17:24:02.846903 140474388072320 learning.py:507] global step 2405: loss = 0.9436 (1.321 sec/step)\n",
            "I0919 17:24:04.185585 140474388072320 learning.py:507] global step 2406: loss = 1.3861 (1.335 sec/step)\n",
            "I0919 17:24:05.477608 140474388072320 learning.py:507] global step 2407: loss = 0.7271 (1.290 sec/step)\n",
            "I0919 17:24:06.782191 140474388072320 learning.py:507] global step 2408: loss = 0.6193 (1.303 sec/step)\n",
            "I0919 17:24:08.106028 140474388072320 learning.py:507] global step 2409: loss = 0.9429 (1.322 sec/step)\n",
            "I0919 17:24:09.414087 140474388072320 learning.py:507] global step 2410: loss = 1.0786 (1.306 sec/step)\n",
            "I0919 17:24:10.684870 140474388072320 learning.py:507] global step 2411: loss = 0.9221 (1.269 sec/step)\n",
            "I0919 17:24:12.017764 140474388072320 learning.py:507] global step 2412: loss = 0.9962 (1.331 sec/step)\n",
            "I0919 17:24:13.317339 140474388072320 learning.py:507] global step 2413: loss = 1.0391 (1.298 sec/step)\n",
            "I0919 17:24:14.638930 140474388072320 learning.py:507] global step 2414: loss = 0.8633 (1.320 sec/step)\n",
            "I0919 17:24:15.928317 140474388072320 learning.py:507] global step 2415: loss = 0.8019 (1.287 sec/step)\n",
            "I0919 17:24:17.232269 140474388072320 learning.py:507] global step 2416: loss = 0.9890 (1.302 sec/step)\n",
            "I0919 17:24:18.547590 140474388072320 learning.py:507] global step 2417: loss = 1.0537 (1.314 sec/step)\n",
            "I0919 17:24:19.846976 140474388072320 learning.py:507] global step 2418: loss = 1.1816 (1.298 sec/step)\n",
            "I0919 17:24:21.144725 140474388072320 learning.py:507] global step 2419: loss = 0.8262 (1.296 sec/step)\n",
            "I0919 17:24:22.486614 140474388072320 learning.py:507] global step 2420: loss = 0.9543 (1.340 sec/step)\n",
            "I0919 17:24:23.785365 140474388072320 learning.py:507] global step 2421: loss = 1.0292 (1.297 sec/step)\n",
            "I0919 17:24:25.101834 140474388072320 learning.py:507] global step 2422: loss = 0.8994 (1.314 sec/step)\n",
            "I0919 17:24:26.392822 140474388072320 learning.py:507] global step 2423: loss = 1.6181 (1.289 sec/step)\n",
            "I0919 17:24:28.451183 140474388072320 learning.py:507] global step 2424: loss = 0.7490 (2.044 sec/step)\n",
            "I0919 17:24:28.767661 140471297337088 supervisor.py:1050] Recording summary at step 2424.\n",
            "I0919 17:24:29.853056 140474388072320 learning.py:507] global step 2425: loss = 0.9672 (1.400 sec/step)\n",
            "I0919 17:24:31.154642 140474388072320 learning.py:507] global step 2426: loss = 0.9566 (1.300 sec/step)\n",
            "I0919 17:24:32.484298 140474388072320 learning.py:507] global step 2427: loss = 0.9012 (1.328 sec/step)\n",
            "I0919 17:24:33.801327 140474388072320 learning.py:507] global step 2428: loss = 0.8797 (1.315 sec/step)\n",
            "I0919 17:24:35.109810 140474388072320 learning.py:507] global step 2429: loss = 1.4062 (1.307 sec/step)\n",
            "I0919 17:24:36.421484 140474388072320 learning.py:507] global step 2430: loss = 0.9480 (1.310 sec/step)\n",
            "I0919 17:24:37.729491 140474388072320 learning.py:507] global step 2431: loss = 1.0744 (1.306 sec/step)\n",
            "I0919 17:24:39.044627 140474388072320 learning.py:507] global step 2432: loss = 1.0060 (1.313 sec/step)\n",
            "I0919 17:24:40.359600 140474388072320 learning.py:507] global step 2433: loss = 0.9761 (1.313 sec/step)\n",
            "I0919 17:24:41.688019 140474388072320 learning.py:507] global step 2434: loss = 0.8803 (1.327 sec/step)\n",
            "I0919 17:24:43.045649 140474388072320 learning.py:507] global step 2435: loss = 0.9395 (1.356 sec/step)\n",
            "I0919 17:24:44.366415 140474388072320 learning.py:507] global step 2436: loss = 0.9412 (1.319 sec/step)\n",
            "I0919 17:24:45.658447 140474388072320 learning.py:507] global step 2437: loss = 0.9431 (1.290 sec/step)\n",
            "I0919 17:24:46.950472 140474388072320 learning.py:507] global step 2438: loss = 0.9030 (1.290 sec/step)\n",
            "I0919 17:24:48.245033 140474388072320 learning.py:507] global step 2439: loss = 1.0467 (1.293 sec/step)\n",
            "I0919 17:24:49.530739 140474388072320 learning.py:507] global step 2440: loss = 0.8825 (1.284 sec/step)\n",
            "I0919 17:24:50.830374 140474388072320 learning.py:507] global step 2441: loss = 1.2711 (1.298 sec/step)\n",
            "I0919 17:24:52.140048 140474388072320 learning.py:507] global step 2442: loss = 1.0030 (1.308 sec/step)\n",
            "I0919 17:24:53.453149 140474388072320 learning.py:507] global step 2443: loss = 0.8493 (1.311 sec/step)\n",
            "I0919 17:24:54.746504 140474388072320 learning.py:507] global step 2444: loss = 0.8649 (1.292 sec/step)\n",
            "I0919 17:24:56.075459 140474388072320 learning.py:507] global step 2445: loss = 1.3848 (1.327 sec/step)\n",
            "I0919 17:24:57.388382 140474388072320 learning.py:507] global step 2446: loss = 1.0007 (1.311 sec/step)\n",
            "I0919 17:24:58.727370 140474388072320 learning.py:507] global step 2447: loss = 0.9736 (1.337 sec/step)\n",
            "I0919 17:25:00.032005 140474388072320 learning.py:507] global step 2448: loss = 1.0006 (1.303 sec/step)\n",
            "I0919 17:25:01.362576 140474388072320 learning.py:507] global step 2449: loss = 0.9787 (1.329 sec/step)\n",
            "I0919 17:25:02.681668 140474388072320 learning.py:507] global step 2450: loss = 1.0024 (1.317 sec/step)\n",
            "I0919 17:25:03.979353 140474388072320 learning.py:507] global step 2451: loss = 0.8786 (1.296 sec/step)\n",
            "I0919 17:25:05.329629 140474388072320 learning.py:507] global step 2452: loss = 0.9916 (1.349 sec/step)\n",
            "I0919 17:25:06.629868 140474388072320 learning.py:507] global step 2453: loss = 1.1313 (1.299 sec/step)\n",
            "I0919 17:25:07.937180 140474388072320 learning.py:507] global step 2454: loss = 0.7895 (1.305 sec/step)\n",
            "I0919 17:25:09.249684 140474388072320 learning.py:507] global step 2455: loss = 0.8379 (1.311 sec/step)\n",
            "I0919 17:25:10.563261 140474388072320 learning.py:507] global step 2456: loss = 0.9407 (1.304 sec/step)\n",
            "I0919 17:25:11.907423 140474388072320 learning.py:507] global step 2457: loss = 0.9660 (1.342 sec/step)\n",
            "I0919 17:25:13.213933 140474388072320 learning.py:507] global step 2458: loss = 0.9037 (1.305 sec/step)\n",
            "I0919 17:25:14.490242 140474388072320 learning.py:507] global step 2459: loss = 0.9444 (1.275 sec/step)\n",
            "I0919 17:25:15.783324 140474388072320 learning.py:507] global step 2460: loss = 0.7535 (1.291 sec/step)\n",
            "I0919 17:25:17.088342 140474388072320 learning.py:507] global step 2461: loss = 0.9465 (1.303 sec/step)\n",
            "I0919 17:25:18.365083 140474388072320 learning.py:507] global step 2462: loss = 0.9604 (1.275 sec/step)\n",
            "I0919 17:25:19.683384 140474388072320 learning.py:507] global step 2463: loss = 0.8442 (1.317 sec/step)\n",
            "I0919 17:25:20.991596 140474388072320 learning.py:507] global step 2464: loss = 0.9200 (1.306 sec/step)\n",
            "I0919 17:25:22.329044 140474388072320 learning.py:507] global step 2465: loss = 0.9379 (1.336 sec/step)\n",
            "I0919 17:25:23.643548 140474388072320 learning.py:507] global step 2466: loss = 0.8758 (1.313 sec/step)\n",
            "I0919 17:25:24.942291 140474388072320 learning.py:507] global step 2467: loss = 1.2525 (1.297 sec/step)\n",
            "I0919 17:25:26.264147 140474388072320 learning.py:507] global step 2468: loss = 1.0674 (1.320 sec/step)\n",
            "I0919 17:25:27.632535 140474388072320 learning.py:507] global step 2469: loss = 0.7394 (1.366 sec/step)\n",
            "I0919 17:25:28.952480 140474388072320 learning.py:507] global step 2470: loss = 0.9426 (1.318 sec/step)\n",
            "I0919 17:25:30.303184 140474388072320 learning.py:507] global step 2471: loss = 0.8976 (1.349 sec/step)\n",
            "I0919 17:25:31.643074 140474388072320 learning.py:507] global step 2472: loss = 0.9806 (1.338 sec/step)\n",
            "I0919 17:25:32.942651 140474388072320 learning.py:507] global step 2473: loss = 0.9513 (1.298 sec/step)\n",
            "I0919 17:25:34.226211 140474388072320 learning.py:507] global step 2474: loss = 0.9776 (1.281 sec/step)\n",
            "I0919 17:25:35.587898 140474388072320 learning.py:507] global step 2475: loss = 0.8660 (1.360 sec/step)\n",
            "I0919 17:25:36.888499 140474388072320 learning.py:507] global step 2476: loss = 0.9722 (1.298 sec/step)\n",
            "I0919 17:25:38.205158 140474388072320 learning.py:507] global step 2477: loss = 0.9416 (1.315 sec/step)\n",
            "I0919 17:25:39.497682 140474388072320 learning.py:507] global step 2478: loss = 1.2454 (1.291 sec/step)\n",
            "I0919 17:25:40.797029 140474388072320 learning.py:507] global step 2479: loss = 0.9651 (1.298 sec/step)\n",
            "I0919 17:25:42.067401 140474388072320 learning.py:507] global step 2480: loss = 1.1025 (1.269 sec/step)\n",
            "I0919 17:25:43.368944 140474388072320 learning.py:507] global step 2481: loss = 1.1561 (1.300 sec/step)\n",
            "I0919 17:25:44.674029 140474388072320 learning.py:507] global step 2482: loss = 1.0158 (1.303 sec/step)\n",
            "I0919 17:25:46.006544 140474388072320 learning.py:507] global step 2483: loss = 0.8321 (1.331 sec/step)\n",
            "I0919 17:25:47.302527 140474388072320 learning.py:507] global step 2484: loss = 1.5244 (1.294 sec/step)\n",
            "I0919 17:25:48.610758 140474388072320 learning.py:507] global step 2485: loss = 1.2695 (1.306 sec/step)\n",
            "I0919 17:25:49.959542 140474388072320 learning.py:507] global step 2486: loss = 1.2132 (1.347 sec/step)\n",
            "I0919 17:25:51.277746 140474388072320 learning.py:507] global step 2487: loss = 1.1370 (1.316 sec/step)\n",
            "I0919 17:25:52.580434 140474388072320 learning.py:507] global step 2488: loss = 0.8423 (1.301 sec/step)\n",
            "I0919 17:25:53.901343 140474388072320 learning.py:507] global step 2489: loss = 0.9501 (1.319 sec/step)\n",
            "I0919 17:25:55.209156 140474388072320 learning.py:507] global step 2490: loss = 1.0418 (1.306 sec/step)\n",
            "I0919 17:25:56.505688 140474388072320 learning.py:507] global step 2491: loss = 1.2941 (1.295 sec/step)\n",
            "I0919 17:25:57.807751 140474388072320 learning.py:507] global step 2492: loss = 1.0975 (1.300 sec/step)\n",
            "I0919 17:25:59.129081 140474388072320 learning.py:507] global step 2493: loss = 0.8472 (1.319 sec/step)\n",
            "I0919 17:26:00.464952 140474388072320 learning.py:507] global step 2494: loss = 0.9808 (1.334 sec/step)\n",
            "I0919 17:26:01.758445 140474388072320 learning.py:507] global step 2495: loss = 0.7694 (1.292 sec/step)\n",
            "I0919 17:26:03.096824 140474388072320 learning.py:507] global step 2496: loss = 0.8818 (1.336 sec/step)\n",
            "I0919 17:26:04.389858 140474388072320 learning.py:507] global step 2497: loss = 1.0128 (1.291 sec/step)\n",
            "I0919 17:26:05.693867 140474388072320 learning.py:507] global step 2498: loss = 0.9231 (1.302 sec/step)\n",
            "I0919 17:26:07.012891 140474388072320 learning.py:507] global step 2499: loss = 1.3146 (1.317 sec/step)\n",
            "I0919 17:26:08.339490 140474388072320 learning.py:507] global step 2500: loss = 0.9878 (1.325 sec/step)\n",
            "I0919 17:26:09.660588 140474388072320 learning.py:507] global step 2501: loss = 0.9113 (1.320 sec/step)\n",
            "I0919 17:26:10.966156 140474388072320 learning.py:507] global step 2502: loss = 1.0606 (1.304 sec/step)\n",
            "I0919 17:26:12.262440 140474388072320 learning.py:507] global step 2503: loss = 1.0781 (1.295 sec/step)\n",
            "I0919 17:26:13.538924 140474388072320 learning.py:507] global step 2504: loss = 1.2764 (1.275 sec/step)\n",
            "I0919 17:26:14.873812 140474388072320 learning.py:507] global step 2505: loss = 0.9750 (1.328 sec/step)\n",
            "I0919 17:26:16.197014 140474388072320 learning.py:507] global step 2506: loss = 0.8954 (1.321 sec/step)\n",
            "I0919 17:26:17.547355 140474388072320 learning.py:507] global step 2507: loss = 1.1714 (1.349 sec/step)\n",
            "I0919 17:26:18.856611 140474388072320 learning.py:507] global step 2508: loss = 0.9946 (1.308 sec/step)\n",
            "I0919 17:26:20.171991 140474388072320 learning.py:507] global step 2509: loss = 0.9154 (1.314 sec/step)\n",
            "I0919 17:26:21.491951 140474388072320 learning.py:507] global step 2510: loss = 0.7876 (1.318 sec/step)\n",
            "I0919 17:26:22.846683 140474388072320 learning.py:507] global step 2511: loss = 1.1239 (1.353 sec/step)\n",
            "I0919 17:26:24.163501 140474388072320 learning.py:507] global step 2512: loss = 0.7834 (1.315 sec/step)\n",
            "I0919 17:26:25.484833 140474388072320 learning.py:507] global step 2513: loss = 0.8563 (1.319 sec/step)\n",
            "I0919 17:26:26.813051 140474388072320 learning.py:507] global step 2514: loss = 1.1137 (1.326 sec/step)\n",
            "I0919 17:26:28.492882 140474388072320 learning.py:507] global step 2515: loss = 0.8373 (1.676 sec/step)\n",
            "I0919 17:26:29.928459 140471297337088 supervisor.py:1050] Recording summary at step 2515.\n",
            "I0919 17:26:30.402029 140474388072320 learning.py:507] global step 2516: loss = 0.9999 (1.907 sec/step)\n",
            "I0919 17:26:31.734559 140474388072320 learning.py:507] global step 2517: loss = 0.9366 (1.331 sec/step)\n",
            "I0919 17:26:33.040630 140474388072320 learning.py:507] global step 2518: loss = 0.8383 (1.304 sec/step)\n",
            "I0919 17:26:34.344583 140474388072320 learning.py:507] global step 2519: loss = 0.8533 (1.302 sec/step)\n",
            "I0919 17:26:35.664691 140474388072320 learning.py:507] global step 2520: loss = 0.8879 (1.318 sec/step)\n",
            "I0919 17:26:37.001148 140474388072320 learning.py:507] global step 2521: loss = 0.8440 (1.334 sec/step)\n",
            "I0919 17:26:38.338040 140474388072320 learning.py:507] global step 2522: loss = 0.9201 (1.335 sec/step)\n",
            "I0919 17:26:39.695054 140474388072320 learning.py:507] global step 2523: loss = 0.7978 (1.355 sec/step)\n",
            "I0919 17:26:41.030880 140474388072320 learning.py:507] global step 2524: loss = 0.8822 (1.334 sec/step)\n",
            "I0919 17:26:42.325102 140474388072320 learning.py:507] global step 2525: loss = 1.0745 (1.292 sec/step)\n",
            "I0919 17:26:43.633768 140474388072320 learning.py:507] global step 2526: loss = 1.2644 (1.307 sec/step)\n",
            "I0919 17:26:44.926080 140474388072320 learning.py:507] global step 2527: loss = 0.9072 (1.291 sec/step)\n",
            "I0919 17:26:46.257688 140474388072320 learning.py:507] global step 2528: loss = 0.7446 (1.330 sec/step)\n",
            "I0919 17:26:47.531526 140474388072320 learning.py:507] global step 2529: loss = 0.8562 (1.272 sec/step)\n",
            "I0919 17:26:48.847507 140474388072320 learning.py:507] global step 2530: loss = 1.0612 (1.314 sec/step)\n",
            "I0919 17:26:50.175128 140474388072320 learning.py:507] global step 2531: loss = 0.9439 (1.326 sec/step)\n",
            "I0919 17:26:51.492233 140474388072320 learning.py:507] global step 2532: loss = 0.7718 (1.316 sec/step)\n",
            "I0919 17:26:52.796952 140474388072320 learning.py:507] global step 2533: loss = 0.9141 (1.303 sec/step)\n",
            "I0919 17:26:54.117996 140474388072320 learning.py:507] global step 2534: loss = 0.8061 (1.319 sec/step)\n",
            "I0919 17:26:55.463783 140474388072320 learning.py:507] global step 2535: loss = 1.0239 (1.344 sec/step)\n",
            "I0919 17:26:56.751890 140474388072320 learning.py:507] global step 2536: loss = 1.3499 (1.286 sec/step)\n",
            "I0919 17:26:58.053235 140474388072320 learning.py:507] global step 2537: loss = 0.9988 (1.299 sec/step)\n",
            "I0919 17:26:59.380697 140474388072320 learning.py:507] global step 2538: loss = 0.8893 (1.325 sec/step)\n",
            "I0919 17:27:00.692224 140474388072320 learning.py:507] global step 2539: loss = 1.0827 (1.310 sec/step)\n",
            "I0919 17:27:01.978442 140474388072320 learning.py:507] global step 2540: loss = 0.8524 (1.285 sec/step)\n",
            "I0919 17:27:03.333498 140474388072320 learning.py:507] global step 2541: loss = 1.0849 (1.353 sec/step)\n",
            "I0919 17:27:04.635361 140474388072320 learning.py:507] global step 2542: loss = 0.9833 (1.300 sec/step)\n",
            "I0919 17:27:05.965465 140474388072320 learning.py:507] global step 2543: loss = 1.0539 (1.328 sec/step)\n",
            "I0919 17:27:07.269866 140474388072320 learning.py:507] global step 2544: loss = 0.8158 (1.303 sec/step)\n",
            "I0919 17:27:08.532026 140474388072320 learning.py:507] global step 2545: loss = 0.9088 (1.260 sec/step)\n",
            "I0919 17:27:09.817190 140474388072320 learning.py:507] global step 2546: loss = 0.9040 (1.283 sec/step)\n",
            "I0919 17:27:11.106880 140474388072320 learning.py:507] global step 2547: loss = 0.9683 (1.288 sec/step)\n",
            "I0919 17:27:12.383965 140474388072320 learning.py:507] global step 2548: loss = 0.9103 (1.275 sec/step)\n",
            "I0919 17:27:13.679877 140474388072320 learning.py:507] global step 2549: loss = 0.8445 (1.294 sec/step)\n",
            "I0919 17:27:14.987628 140474388072320 learning.py:507] global step 2550: loss = 0.7640 (1.306 sec/step)\n",
            "I0919 17:27:16.350142 140474388072320 learning.py:507] global step 2551: loss = 1.3331 (1.360 sec/step)\n",
            "I0919 17:27:17.675044 140474388072320 learning.py:507] global step 2552: loss = 0.7325 (1.323 sec/step)\n",
            "I0919 17:27:18.988487 140474388072320 learning.py:507] global step 2553: loss = 1.0329 (1.311 sec/step)\n",
            "I0919 17:27:20.299479 140474388072320 learning.py:507] global step 2554: loss = 1.1227 (1.309 sec/step)\n",
            "I0919 17:27:21.614448 140474388072320 learning.py:507] global step 2555: loss = 0.8736 (1.313 sec/step)\n",
            "I0919 17:27:22.923898 140474388072320 learning.py:507] global step 2556: loss = 1.0202 (1.308 sec/step)\n",
            "I0919 17:27:24.216639 140474388072320 learning.py:507] global step 2557: loss = 1.0732 (1.291 sec/step)\n",
            "I0919 17:27:25.527050 140474388072320 learning.py:507] global step 2558: loss = 1.0378 (1.309 sec/step)\n",
            "I0919 17:27:26.830974 140474388072320 learning.py:507] global step 2559: loss = 0.8723 (1.302 sec/step)\n",
            "I0919 17:27:28.163953 140474388072320 learning.py:507] global step 2560: loss = 0.9934 (1.331 sec/step)\n",
            "I0919 17:27:29.519889 140474388072320 learning.py:507] global step 2561: loss = 1.0780 (1.354 sec/step)\n",
            "I0919 17:27:30.829629 140474388072320 learning.py:507] global step 2562: loss = 0.8077 (1.308 sec/step)\n",
            "I0919 17:27:32.145096 140474388072320 learning.py:507] global step 2563: loss = 0.9654 (1.314 sec/step)\n",
            "I0919 17:27:33.456693 140474388072320 learning.py:507] global step 2564: loss = 0.9807 (1.310 sec/step)\n",
            "I0919 17:27:34.751029 140474388072320 learning.py:507] global step 2565: loss = 0.8485 (1.293 sec/step)\n",
            "I0919 17:27:36.049030 140474388072320 learning.py:507] global step 2566: loss = 0.9970 (1.296 sec/step)\n",
            "I0919 17:27:37.377248 140474388072320 learning.py:507] global step 2567: loss = 0.8720 (1.327 sec/step)\n",
            "I0919 17:27:38.718741 140474388072320 learning.py:507] global step 2568: loss = 1.2286 (1.340 sec/step)\n",
            "I0919 17:27:40.020853 140474388072320 learning.py:507] global step 2569: loss = 0.7616 (1.300 sec/step)\n",
            "I0919 17:27:41.314483 140474388072320 learning.py:507] global step 2570: loss = 0.9131 (1.292 sec/step)\n",
            "I0919 17:27:42.613746 140474388072320 learning.py:507] global step 2571: loss = 0.9848 (1.297 sec/step)\n",
            "I0919 17:27:43.929848 140474388072320 learning.py:507] global step 2572: loss = 1.0649 (1.314 sec/step)\n",
            "I0919 17:27:45.228931 140474388072320 learning.py:507] global step 2573: loss = 1.0224 (1.297 sec/step)\n",
            "I0919 17:27:46.542747 140474388072320 learning.py:507] global step 2574: loss = 1.0459 (1.312 sec/step)\n",
            "I0919 17:27:47.893409 140474388072320 learning.py:507] global step 2575: loss = 1.1591 (1.349 sec/step)\n",
            "I0919 17:27:49.231801 140474388072320 learning.py:507] global step 2576: loss = 0.8418 (1.336 sec/step)\n",
            "I0919 17:27:50.535208 140474388072320 learning.py:507] global step 2577: loss = 1.0505 (1.302 sec/step)\n",
            "I0919 17:27:51.848220 140474388072320 learning.py:507] global step 2578: loss = 1.0750 (1.311 sec/step)\n",
            "I0919 17:27:53.138937 140474388072320 learning.py:507] global step 2579: loss = 0.9777 (1.289 sec/step)\n",
            "I0919 17:27:54.474688 140474388072320 learning.py:507] global step 2580: loss = 1.0456 (1.333 sec/step)\n",
            "I0919 17:27:55.775835 140474388072320 learning.py:507] global step 2581: loss = 1.2151 (1.299 sec/step)\n",
            "I0919 17:27:57.062136 140474388072320 learning.py:507] global step 2582: loss = 0.8362 (1.284 sec/step)\n",
            "I0919 17:27:58.380681 140474388072320 learning.py:507] global step 2583: loss = 1.0094 (1.317 sec/step)\n",
            "I0919 17:27:59.704369 140474388072320 learning.py:507] global step 2584: loss = 1.1657 (1.322 sec/step)\n",
            "I0919 17:28:01.014919 140474388072320 learning.py:507] global step 2585: loss = 0.9116 (1.309 sec/step)\n",
            "I0919 17:28:02.317166 140474388072320 learning.py:507] global step 2586: loss = 1.2844 (1.300 sec/step)\n",
            "I0919 17:28:03.625663 140474388072320 learning.py:507] global step 2587: loss = 0.8375 (1.307 sec/step)\n",
            "I0919 17:28:04.947510 140474388072320 learning.py:507] global step 2588: loss = 0.8575 (1.320 sec/step)\n",
            "I0919 17:28:06.238146 140474388072320 learning.py:507] global step 2589: loss = 0.9584 (1.289 sec/step)\n",
            "I0919 17:28:07.522905 140474388072320 learning.py:507] global step 2590: loss = 0.9844 (1.283 sec/step)\n",
            "I0919 17:28:08.827132 140474388072320 learning.py:507] global step 2591: loss = 1.0972 (1.303 sec/step)\n",
            "I0919 17:28:10.160451 140474388072320 learning.py:507] global step 2592: loss = 0.7147 (1.331 sec/step)\n",
            "I0919 17:28:11.477559 140474388072320 learning.py:507] global step 2593: loss = 1.3159 (1.315 sec/step)\n",
            "I0919 17:28:12.772876 140474388072320 learning.py:507] global step 2594: loss = 0.8006 (1.294 sec/step)\n",
            "I0919 17:28:14.067416 140474388072320 learning.py:507] global step 2595: loss = 0.7444 (1.293 sec/step)\n",
            "I0919 17:28:15.379323 140474388072320 learning.py:507] global step 2596: loss = 1.4621 (1.310 sec/step)\n",
            "I0919 17:28:16.736718 140474388072320 learning.py:507] global step 2597: loss = 0.9566 (1.356 sec/step)\n",
            "I0919 17:28:18.040785 140474388072320 learning.py:507] global step 2598: loss = 0.9618 (1.302 sec/step)\n",
            "I0919 17:28:19.370335 140474388072320 learning.py:507] global step 2599: loss = 1.0550 (1.328 sec/step)\n",
            "I0919 17:28:20.677613 140474388072320 learning.py:507] global step 2600: loss = 0.9790 (1.306 sec/step)\n",
            "I0919 17:28:21.973794 140474388072320 learning.py:507] global step 2601: loss = 0.9791 (1.294 sec/step)\n",
            "I0919 17:28:23.260360 140474388072320 learning.py:507] global step 2602: loss = 1.1415 (1.284 sec/step)\n",
            "I0919 17:28:24.563929 140474388072320 learning.py:507] global step 2603: loss = 0.8170 (1.301 sec/step)\n",
            "I0919 17:28:25.853498 140474388072320 learning.py:507] global step 2604: loss = 1.0586 (1.288 sec/step)\n",
            "I0919 17:28:27.335099 140474388072320 learning.py:507] global step 2605: loss = 1.0532 (1.414 sec/step)\n",
            "I0919 17:28:28.749982 140471297337088 supervisor.py:1050] Recording summary at step 2605.\n",
            "I0919 17:28:29.292701 140474388072320 learning.py:507] global step 2606: loss = 0.8681 (1.859 sec/step)\n",
            "I0919 17:28:30.579482 140474388072320 learning.py:507] global step 2607: loss = 0.8766 (1.285 sec/step)\n",
            "I0919 17:28:31.882484 140474388072320 learning.py:507] global step 2608: loss = 1.0351 (1.301 sec/step)\n",
            "I0919 17:28:33.183095 140474388072320 learning.py:507] global step 2609: loss = 1.0472 (1.299 sec/step)\n",
            "I0919 17:28:34.479561 140474388072320 learning.py:507] global step 2610: loss = 0.8701 (1.295 sec/step)\n",
            "I0919 17:28:35.755229 140474388072320 learning.py:507] global step 2611: loss = 0.7590 (1.274 sec/step)\n",
            "I0919 17:28:37.059596 140474388072320 learning.py:507] global step 2612: loss = 1.2321 (1.302 sec/step)\n",
            "I0919 17:28:38.382606 140474388072320 learning.py:507] global step 2613: loss = 0.9761 (1.321 sec/step)\n",
            "I0919 17:28:39.711715 140474388072320 learning.py:507] global step 2614: loss = 0.8577 (1.327 sec/step)\n",
            "I0919 17:28:40.999435 140474388072320 learning.py:507] global step 2615: loss = 0.9944 (1.286 sec/step)\n",
            "I0919 17:28:42.316296 140474388072320 learning.py:507] global step 2616: loss = 0.9710 (1.315 sec/step)\n",
            "I0919 17:28:43.620460 140474388072320 learning.py:507] global step 2617: loss = 0.9681 (1.302 sec/step)\n",
            "I0919 17:28:44.931427 140474388072320 learning.py:507] global step 2618: loss = 0.7794 (1.309 sec/step)\n",
            "I0919 17:28:46.235061 140474388072320 learning.py:507] global step 2619: loss = 0.9396 (1.302 sec/step)\n",
            "I0919 17:28:47.531347 140474388072320 learning.py:507] global step 2620: loss = 0.8128 (1.294 sec/step)\n",
            "I0919 17:28:48.841666 140474388072320 learning.py:507] global step 2621: loss = 0.9682 (1.309 sec/step)\n",
            "I0919 17:28:50.148843 140474388072320 learning.py:507] global step 2622: loss = 1.0353 (1.305 sec/step)\n",
            "I0919 17:28:51.487910 140474388072320 learning.py:507] global step 2623: loss = 1.1431 (1.337 sec/step)\n",
            "I0919 17:28:52.799262 140474388072320 learning.py:507] global step 2624: loss = 1.1004 (1.309 sec/step)\n",
            "I0919 17:28:54.104701 140474388072320 learning.py:507] global step 2625: loss = 0.8496 (1.303 sec/step)\n",
            "I0919 17:28:55.455005 140474388072320 learning.py:507] global step 2626: loss = 0.9301 (1.348 sec/step)\n",
            "I0919 17:28:56.790738 140474388072320 learning.py:507] global step 2627: loss = 1.2302 (1.334 sec/step)\n",
            "I0919 17:28:58.068684 140474388072320 learning.py:507] global step 2628: loss = 0.9650 (1.276 sec/step)\n",
            "I0919 17:28:59.374006 140474388072320 learning.py:507] global step 2629: loss = 0.8775 (1.304 sec/step)\n",
            "I0919 17:29:00.694828 140474388072320 learning.py:507] global step 2630: loss = 0.7811 (1.319 sec/step)\n",
            "I0919 17:29:01.982779 140474388072320 learning.py:507] global step 2631: loss = 0.8448 (1.286 sec/step)\n",
            "I0919 17:29:03.364614 140474388072320 learning.py:507] global step 2632: loss = 0.7028 (1.380 sec/step)\n",
            "I0919 17:29:04.699562 140474388072320 learning.py:507] global step 2633: loss = 1.1058 (1.333 sec/step)\n",
            "I0919 17:29:06.008646 140474388072320 learning.py:507] global step 2634: loss = 0.8454 (1.307 sec/step)\n",
            "I0919 17:29:07.326572 140474388072320 learning.py:507] global step 2635: loss = 1.1591 (1.316 sec/step)\n",
            "I0919 17:29:08.623610 140474388072320 learning.py:507] global step 2636: loss = 0.8488 (1.295 sec/step)\n",
            "I0919 17:29:09.932571 140474388072320 learning.py:507] global step 2637: loss = 0.8858 (1.307 sec/step)\n",
            "I0919 17:29:11.213948 140474388072320 learning.py:507] global step 2638: loss = 0.8375 (1.280 sec/step)\n",
            "I0919 17:29:12.529827 140474388072320 learning.py:507] global step 2639: loss = 0.7992 (1.314 sec/step)\n",
            "I0919 17:29:13.833024 140474388072320 learning.py:507] global step 2640: loss = 0.8995 (1.302 sec/step)\n",
            "I0919 17:29:15.140902 140474388072320 learning.py:507] global step 2641: loss = 0.7755 (1.306 sec/step)\n",
            "I0919 17:29:16.426466 140474388072320 learning.py:507] global step 2642: loss = 0.9332 (1.284 sec/step)\n",
            "I0919 17:29:17.747942 140474388072320 learning.py:507] global step 2643: loss = 0.9268 (1.320 sec/step)\n",
            "I0919 17:29:19.073163 140474388072320 learning.py:507] global step 2644: loss = 0.9805 (1.323 sec/step)\n",
            "I0919 17:29:20.396606 140474388072320 learning.py:507] global step 2645: loss = 1.0291 (1.322 sec/step)\n",
            "I0919 17:29:21.696734 140474388072320 learning.py:507] global step 2646: loss = 0.8908 (1.298 sec/step)\n",
            "I0919 17:29:22.980324 140474388072320 learning.py:507] global step 2647: loss = 0.8624 (1.282 sec/step)\n",
            "I0919 17:29:24.310971 140474388072320 learning.py:507] global step 2648: loss = 0.9891 (1.329 sec/step)\n",
            "I0919 17:29:25.607996 140474388072320 learning.py:507] global step 2649: loss = 1.0388 (1.295 sec/step)\n",
            "I0919 17:29:26.911205 140474388072320 learning.py:507] global step 2650: loss = 0.9131 (1.301 sec/step)\n",
            "I0919 17:29:28.206347 140474388072320 learning.py:507] global step 2651: loss = 0.8066 (1.293 sec/step)\n",
            "I0919 17:29:29.514214 140474388072320 learning.py:507] global step 2652: loss = 0.9053 (1.306 sec/step)\n",
            "I0919 17:29:30.802469 140474388072320 learning.py:507] global step 2653: loss = 1.0363 (1.287 sec/step)\n",
            "I0919 17:29:32.111346 140474388072320 learning.py:507] global step 2654: loss = 1.0420 (1.307 sec/step)\n",
            "I0919 17:29:33.393903 140474388072320 learning.py:507] global step 2655: loss = 1.2177 (1.280 sec/step)\n",
            "I0919 17:29:34.684097 140474388072320 learning.py:507] global step 2656: loss = 0.8819 (1.288 sec/step)\n",
            "I0919 17:29:35.987046 140474388072320 learning.py:507] global step 2657: loss = 0.8291 (1.301 sec/step)\n",
            "I0919 17:29:37.303577 140474388072320 learning.py:507] global step 2658: loss = 0.9834 (1.314 sec/step)\n",
            "I0919 17:29:38.625530 140474388072320 learning.py:507] global step 2659: loss = 0.9793 (1.320 sec/step)\n",
            "I0919 17:29:39.959481 140474388072320 learning.py:507] global step 2660: loss = 0.9458 (1.332 sec/step)\n",
            "I0919 17:29:41.254841 140474388072320 learning.py:507] global step 2661: loss = 0.8737 (1.293 sec/step)\n",
            "I0919 17:29:42.542416 140474388072320 learning.py:507] global step 2662: loss = 0.9392 (1.286 sec/step)\n",
            "I0919 17:29:43.877059 140474388072320 learning.py:507] global step 2663: loss = 0.9078 (1.333 sec/step)\n",
            "I0919 17:29:45.178287 140474388072320 learning.py:507] global step 2664: loss = 0.9171 (1.300 sec/step)\n",
            "I0919 17:29:46.499223 140474388072320 learning.py:507] global step 2665: loss = 0.7697 (1.319 sec/step)\n",
            "I0919 17:29:47.815975 140474388072320 learning.py:507] global step 2666: loss = 0.7068 (1.315 sec/step)\n",
            "I0919 17:29:49.105333 140474388072320 learning.py:507] global step 2667: loss = 0.9310 (1.288 sec/step)\n",
            "I0919 17:29:50.399384 140474388072320 learning.py:507] global step 2668: loss = 1.0564 (1.292 sec/step)\n",
            "I0919 17:29:51.728271 140474388072320 learning.py:507] global step 2669: loss = 0.8450 (1.326 sec/step)\n",
            "I0919 17:29:53.016993 140474388072320 learning.py:507] global step 2670: loss = 0.9484 (1.285 sec/step)\n",
            "I0919 17:29:54.329030 140474388072320 learning.py:507] global step 2671: loss = 0.9448 (1.310 sec/step)\n",
            "I0919 17:29:55.632894 140474388072320 learning.py:507] global step 2672: loss = 1.3925 (1.302 sec/step)\n",
            "I0919 17:29:56.931651 140474388072320 learning.py:507] global step 2673: loss = 1.0566 (1.297 sec/step)\n",
            "I0919 17:29:58.219345 140474388072320 learning.py:507] global step 2674: loss = 0.8870 (1.286 sec/step)\n",
            "I0919 17:29:59.514702 140474388072320 learning.py:507] global step 2675: loss = 0.8916 (1.293 sec/step)\n",
            "I0919 17:30:00.870712 140474388072320 learning.py:507] global step 2676: loss = 1.0022 (1.354 sec/step)\n",
            "I0919 17:30:02.152894 140474388072320 learning.py:507] global step 2677: loss = 0.9145 (1.280 sec/step)\n",
            "I0919 17:30:03.449944 140474388072320 learning.py:507] global step 2678: loss = 0.7963 (1.295 sec/step)\n",
            "I0919 17:30:04.781219 140474388072320 learning.py:507] global step 2679: loss = 1.2564 (1.330 sec/step)\n",
            "I0919 17:30:06.068799 140474388072320 learning.py:507] global step 2680: loss = 1.1391 (1.286 sec/step)\n",
            "I0919 17:30:07.356378 140474388072320 learning.py:507] global step 2681: loss = 0.9144 (1.286 sec/step)\n",
            "I0919 17:30:08.643734 140474388072320 learning.py:507] global step 2682: loss = 1.1012 (1.286 sec/step)\n",
            "I0919 17:30:09.956851 140474388072320 learning.py:507] global step 2683: loss = 0.8086 (1.311 sec/step)\n",
            "I0919 17:30:11.250292 140474388072320 learning.py:507] global step 2684: loss = 0.9170 (1.292 sec/step)\n",
            "I0919 17:30:12.599472 140474388072320 learning.py:507] global step 2685: loss = 0.7510 (1.348 sec/step)\n",
            "I0919 17:30:13.905564 140474388072320 learning.py:507] global step 2686: loss = 1.1860 (1.304 sec/step)\n",
            "I0919 17:30:15.216933 140474388072320 learning.py:507] global step 2687: loss = 0.8765 (1.310 sec/step)\n",
            "I0919 17:30:16.492517 140474388072320 learning.py:507] global step 2688: loss = 0.9844 (1.274 sec/step)\n",
            "I0919 17:30:17.811640 140474388072320 learning.py:507] global step 2689: loss = 0.9861 (1.317 sec/step)\n",
            "I0919 17:30:19.142971 140474388072320 learning.py:507] global step 2690: loss = 1.0062 (1.329 sec/step)\n",
            "I0919 17:30:20.458127 140474388072320 learning.py:507] global step 2691: loss = 0.7829 (1.313 sec/step)\n",
            "I0919 17:30:21.759475 140474388072320 learning.py:507] global step 2692: loss = 0.7290 (1.300 sec/step)\n",
            "I0919 17:30:23.065646 140474388072320 learning.py:507] global step 2693: loss = 0.8924 (1.304 sec/step)\n",
            "I0919 17:30:24.381903 140474388072320 learning.py:507] global step 2694: loss = 1.0357 (1.314 sec/step)\n",
            "I0919 17:30:25.702650 140474388072320 learning.py:507] global step 2695: loss = 0.8344 (1.319 sec/step)\n",
            "I0919 17:30:26.872467 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 17:30:27.415487 140474388072320 learning.py:507] global step 2696: loss = 0.9945 (1.296 sec/step)\n",
            "I0919 17:30:29.478082 140471297337088 supervisor.py:1050] Recording summary at step 2696.\n",
            "I0919 17:30:29.987183 140474388072320 learning.py:507] global step 2697: loss = 0.8386 (2.549 sec/step)\n",
            "I0919 17:30:31.974873 140474388072320 learning.py:507] global step 2698: loss = 0.9092 (1.966 sec/step)\n",
            "I0919 17:30:33.573750 140474388072320 learning.py:507] global step 2699: loss = 0.9825 (1.597 sec/step)\n",
            "I0919 17:30:34.888736 140474388072320 learning.py:507] global step 2700: loss = 0.8395 (1.313 sec/step)\n",
            "I0919 17:30:36.184587 140474388072320 learning.py:507] global step 2701: loss = 0.8946 (1.294 sec/step)\n",
            "I0919 17:30:37.497931 140474388072320 learning.py:507] global step 2702: loss = 0.8353 (1.312 sec/step)\n",
            "I0919 17:30:38.849843 140474388072320 learning.py:507] global step 2703: loss = 0.9995 (1.350 sec/step)\n",
            "I0919 17:30:40.148798 140474388072320 learning.py:507] global step 2704: loss = 1.0537 (1.297 sec/step)\n",
            "I0919 17:30:41.453316 140474388072320 learning.py:507] global step 2705: loss = 1.0693 (1.303 sec/step)\n",
            "I0919 17:30:42.751005 140474388072320 learning.py:507] global step 2706: loss = 0.8092 (1.296 sec/step)\n",
            "I0919 17:30:44.048163 140474388072320 learning.py:507] global step 2707: loss = 0.9703 (1.295 sec/step)\n",
            "I0919 17:30:45.350075 140474388072320 learning.py:507] global step 2708: loss = 0.9198 (1.300 sec/step)\n",
            "I0919 17:30:46.625636 140474388072320 learning.py:507] global step 2709: loss = 0.8952 (1.273 sec/step)\n",
            "I0919 17:30:47.938378 140474388072320 learning.py:507] global step 2710: loss = 0.8695 (1.311 sec/step)\n",
            "I0919 17:30:49.228871 140474388072320 learning.py:507] global step 2711: loss = 1.0108 (1.289 sec/step)\n",
            "I0919 17:30:50.516097 140474388072320 learning.py:507] global step 2712: loss = 1.1793 (1.285 sec/step)\n",
            "I0919 17:30:51.867670 140474388072320 learning.py:507] global step 2713: loss = 0.9501 (1.350 sec/step)\n",
            "I0919 17:30:53.217068 140474388072320 learning.py:507] global step 2714: loss = 0.7748 (1.348 sec/step)\n",
            "I0919 17:30:54.520183 140474388072320 learning.py:507] global step 2715: loss = 1.1055 (1.301 sec/step)\n",
            "I0919 17:30:55.865417 140474388072320 learning.py:507] global step 2716: loss = 0.8556 (1.344 sec/step)\n",
            "I0919 17:30:57.174832 140474388072320 learning.py:507] global step 2717: loss = 1.3389 (1.307 sec/step)\n",
            "I0919 17:30:58.506377 140474388072320 learning.py:507] global step 2718: loss = 0.9532 (1.330 sec/step)\n",
            "I0919 17:30:59.816020 140474388072320 learning.py:507] global step 2719: loss = 1.0572 (1.308 sec/step)\n",
            "I0919 17:31:01.117502 140474388072320 learning.py:507] global step 2720: loss = 0.9743 (1.300 sec/step)\n",
            "I0919 17:31:02.418390 140474388072320 learning.py:507] global step 2721: loss = 1.0637 (1.299 sec/step)\n",
            "I0919 17:31:03.733052 140474388072320 learning.py:507] global step 2722: loss = 0.9821 (1.311 sec/step)\n",
            "I0919 17:31:05.028877 140474388072320 learning.py:507] global step 2723: loss = 1.0341 (1.294 sec/step)\n",
            "I0919 17:31:06.349456 140474388072320 learning.py:507] global step 2724: loss = 0.8351 (1.319 sec/step)\n",
            "I0919 17:31:07.644093 140474388072320 learning.py:507] global step 2725: loss = 0.8081 (1.293 sec/step)\n",
            "I0919 17:31:08.972973 140474388072320 learning.py:507] global step 2726: loss = 0.9660 (1.327 sec/step)\n",
            "I0919 17:31:10.280985 140474388072320 learning.py:507] global step 2727: loss = 1.0731 (1.306 sec/step)\n",
            "I0919 17:31:11.617199 140474388072320 learning.py:507] global step 2728: loss = 0.7602 (1.334 sec/step)\n",
            "I0919 17:31:12.937927 140474388072320 learning.py:507] global step 2729: loss = 0.9544 (1.319 sec/step)\n",
            "I0919 17:31:14.256756 140474388072320 learning.py:507] global step 2730: loss = 1.0108 (1.317 sec/step)\n",
            "I0919 17:31:15.567310 140474388072320 learning.py:507] global step 2731: loss = 1.1221 (1.309 sec/step)\n",
            "I0919 17:31:16.892224 140474388072320 learning.py:507] global step 2732: loss = 1.1908 (1.323 sec/step)\n",
            "I0919 17:31:18.223581 140474388072320 learning.py:507] global step 2733: loss = 0.9150 (1.330 sec/step)\n",
            "I0919 17:31:19.539383 140474388072320 learning.py:507] global step 2734: loss = 0.9580 (1.314 sec/step)\n",
            "I0919 17:31:20.885292 140474388072320 learning.py:507] global step 2735: loss = 0.8035 (1.344 sec/step)\n",
            "I0919 17:31:22.181974 140474388072320 learning.py:507] global step 2736: loss = 0.8767 (1.295 sec/step)\n",
            "I0919 17:31:23.481059 140474388072320 learning.py:507] global step 2737: loss = 0.9601 (1.297 sec/step)\n",
            "I0919 17:31:24.780832 140474388072320 learning.py:507] global step 2738: loss = 0.9270 (1.298 sec/step)\n",
            "I0919 17:31:26.090230 140474388072320 learning.py:507] global step 2739: loss = 0.9370 (1.308 sec/step)\n",
            "I0919 17:31:27.410316 140474388072320 learning.py:507] global step 2740: loss = 0.9830 (1.318 sec/step)\n",
            "I0919 17:31:28.715909 140474388072320 learning.py:507] global step 2741: loss = 0.8729 (1.304 sec/step)\n",
            "I0919 17:31:30.011603 140474388072320 learning.py:507] global step 2742: loss = 1.0438 (1.294 sec/step)\n",
            "I0919 17:31:31.314764 140474388072320 learning.py:507] global step 2743: loss = 1.0857 (1.302 sec/step)\n",
            "I0919 17:31:32.630582 140474388072320 learning.py:507] global step 2744: loss = 0.9323 (1.314 sec/step)\n",
            "I0919 17:31:33.924376 140474388072320 learning.py:507] global step 2745: loss = 0.9418 (1.292 sec/step)\n",
            "I0919 17:31:35.245273 140474388072320 learning.py:507] global step 2746: loss = 0.9530 (1.319 sec/step)\n",
            "I0919 17:31:36.546963 140474388072320 learning.py:507] global step 2747: loss = 0.7182 (1.300 sec/step)\n",
            "I0919 17:31:37.881298 140474388072320 learning.py:507] global step 2748: loss = 0.8692 (1.332 sec/step)\n",
            "I0919 17:31:39.186590 140474388072320 learning.py:507] global step 2749: loss = 0.8186 (1.304 sec/step)\n",
            "I0919 17:31:40.521188 140474388072320 learning.py:507] global step 2750: loss = 1.0621 (1.333 sec/step)\n",
            "I0919 17:31:41.842026 140474388072320 learning.py:507] global step 2751: loss = 0.9733 (1.319 sec/step)\n",
            "I0919 17:31:43.130162 140474388072320 learning.py:507] global step 2752: loss = 0.8932 (1.287 sec/step)\n",
            "I0919 17:31:44.426542 140474388072320 learning.py:507] global step 2753: loss = 0.7125 (1.295 sec/step)\n",
            "I0919 17:31:45.757471 140474388072320 learning.py:507] global step 2754: loss = 1.4263 (1.329 sec/step)\n",
            "I0919 17:31:47.055299 140474388072320 learning.py:507] global step 2755: loss = 0.9578 (1.296 sec/step)\n",
            "I0919 17:31:48.338673 140474388072320 learning.py:507] global step 2756: loss = 1.0472 (1.282 sec/step)\n",
            "I0919 17:31:49.722738 140474388072320 learning.py:507] global step 2757: loss = 1.0404 (1.382 sec/step)\n",
            "I0919 17:31:51.027586 140474388072320 learning.py:507] global step 2758: loss = 0.8070 (1.303 sec/step)\n",
            "I0919 17:31:52.327773 140474388072320 learning.py:507] global step 2759: loss = 0.8905 (1.299 sec/step)\n",
            "I0919 17:31:53.639911 140474388072320 learning.py:507] global step 2760: loss = 0.8542 (1.310 sec/step)\n",
            "I0919 17:31:54.955408 140474388072320 learning.py:507] global step 2761: loss = 0.7668 (1.314 sec/step)\n",
            "I0919 17:31:56.260915 140474388072320 learning.py:507] global step 2762: loss = 1.4140 (1.304 sec/step)\n",
            "I0919 17:31:57.536226 140474388072320 learning.py:507] global step 2763: loss = 0.9062 (1.273 sec/step)\n",
            "I0919 17:31:58.853379 140474388072320 learning.py:507] global step 2764: loss = 0.8784 (1.315 sec/step)\n",
            "I0919 17:32:00.193084 140474388072320 learning.py:507] global step 2765: loss = 0.9313 (1.338 sec/step)\n",
            "I0919 17:32:01.536578 140474388072320 learning.py:507] global step 2766: loss = 0.9258 (1.342 sec/step)\n",
            "I0919 17:32:02.849163 140474388072320 learning.py:507] global step 2767: loss = 0.9856 (1.310 sec/step)\n",
            "I0919 17:32:04.181952 140474388072320 learning.py:507] global step 2768: loss = 0.9698 (1.331 sec/step)\n",
            "I0919 17:32:05.470752 140474388072320 learning.py:507] global step 2769: loss = 0.8173 (1.287 sec/step)\n",
            "I0919 17:32:06.830387 140474388072320 learning.py:507] global step 2770: loss = 0.7707 (1.358 sec/step)\n",
            "I0919 17:32:08.126844 140474388072320 learning.py:507] global step 2771: loss = 0.9873 (1.295 sec/step)\n",
            "I0919 17:32:09.438628 140474388072320 learning.py:507] global step 2772: loss = 0.9878 (1.310 sec/step)\n",
            "I0919 17:32:10.729016 140474388072320 learning.py:507] global step 2773: loss = 0.9263 (1.289 sec/step)\n",
            "I0919 17:32:12.060478 140474388072320 learning.py:507] global step 2774: loss = 0.9740 (1.330 sec/step)\n",
            "I0919 17:32:13.356734 140474388072320 learning.py:507] global step 2775: loss = 0.8102 (1.294 sec/step)\n",
            "I0919 17:32:14.701705 140474388072320 learning.py:507] global step 2776: loss = 1.0076 (1.343 sec/step)\n",
            "I0919 17:32:16.015324 140474388072320 learning.py:507] global step 2777: loss = 0.8147 (1.312 sec/step)\n",
            "I0919 17:32:17.314007 140474388072320 learning.py:507] global step 2778: loss = 0.8094 (1.297 sec/step)\n",
            "I0919 17:32:18.629578 140474388072320 learning.py:507] global step 2779: loss = 1.1089 (1.314 sec/step)\n",
            "I0919 17:32:19.959609 140474388072320 learning.py:507] global step 2780: loss = 0.9994 (1.328 sec/step)\n",
            "I0919 17:32:21.304023 140474388072320 learning.py:507] global step 2781: loss = 1.0237 (1.343 sec/step)\n",
            "I0919 17:32:22.592428 140474388072320 learning.py:507] global step 2782: loss = 0.7886 (1.287 sec/step)\n",
            "I0919 17:32:23.919706 140474388072320 learning.py:507] global step 2783: loss = 1.1107 (1.325 sec/step)\n",
            "I0919 17:32:25.246956 140474388072320 learning.py:507] global step 2784: loss = 0.7785 (1.325 sec/step)\n",
            "I0919 17:32:26.571568 140474388072320 learning.py:507] global step 2785: loss = 0.7476 (1.323 sec/step)\n",
            "I0919 17:32:27.909003 140474388072320 learning.py:507] global step 2786: loss = 0.9727 (1.335 sec/step)\n",
            "I0919 17:32:30.009067 140474388072320 learning.py:507] global step 2787: loss = 0.8303 (2.093 sec/step)\n",
            "I0919 17:32:30.013674 140471297337088 supervisor.py:1050] Recording summary at step 2787.\n",
            "I0919 17:32:31.317168 140474388072320 learning.py:507] global step 2788: loss = 0.9454 (1.306 sec/step)\n",
            "I0919 17:32:32.620532 140474388072320 learning.py:507] global step 2789: loss = 0.9253 (1.301 sec/step)\n",
            "I0919 17:32:33.993695 140474388072320 learning.py:507] global step 2790: loss = 0.9680 (1.371 sec/step)\n",
            "I0919 17:32:35.316034 140474388072320 learning.py:507] global step 2791: loss = 0.8444 (1.321 sec/step)\n",
            "I0919 17:32:36.636701 140474388072320 learning.py:507] global step 2792: loss = 1.3426 (1.319 sec/step)\n",
            "I0919 17:32:37.976332 140474388072320 learning.py:507] global step 2793: loss = 0.8659 (1.338 sec/step)\n",
            "I0919 17:32:39.289323 140474388072320 learning.py:507] global step 2794: loss = 0.9896 (1.311 sec/step)\n",
            "I0919 17:32:40.620488 140474388072320 learning.py:507] global step 2795: loss = 0.9122 (1.329 sec/step)\n",
            "I0919 17:32:41.926933 140474388072320 learning.py:507] global step 2796: loss = 0.9164 (1.305 sec/step)\n",
            "I0919 17:32:43.212785 140474388072320 learning.py:507] global step 2797: loss = 0.9711 (1.284 sec/step)\n",
            "I0919 17:32:44.559557 140474388072320 learning.py:507] global step 2798: loss = 0.7973 (1.345 sec/step)\n",
            "I0919 17:32:45.865294 140474388072320 learning.py:507] global step 2799: loss = 1.1726 (1.302 sec/step)\n",
            "I0919 17:32:47.186507 140474388072320 learning.py:507] global step 2800: loss = 0.7957 (1.319 sec/step)\n",
            "I0919 17:32:48.523586 140474388072320 learning.py:507] global step 2801: loss = 0.8768 (1.335 sec/step)\n",
            "I0919 17:32:49.832987 140474388072320 learning.py:507] global step 2802: loss = 0.9531 (1.308 sec/step)\n",
            "I0919 17:32:51.143769 140474388072320 learning.py:507] global step 2803: loss = 0.8913 (1.309 sec/step)\n",
            "I0919 17:32:52.458163 140474388072320 learning.py:507] global step 2804: loss = 0.8849 (1.313 sec/step)\n",
            "I0919 17:32:53.754264 140474388072320 learning.py:507] global step 2805: loss = 1.1792 (1.294 sec/step)\n",
            "I0919 17:32:55.140173 140474388072320 learning.py:507] global step 2806: loss = 1.3225 (1.383 sec/step)\n",
            "I0919 17:32:56.462734 140474388072320 learning.py:507] global step 2807: loss = 0.8317 (1.321 sec/step)\n",
            "I0919 17:32:57.757579 140474388072320 learning.py:507] global step 2808: loss = 0.9646 (1.293 sec/step)\n",
            "I0919 17:32:59.098563 140474388072320 learning.py:507] global step 2809: loss = 0.9576 (1.339 sec/step)\n",
            "I0919 17:33:00.412547 140474388072320 learning.py:507] global step 2810: loss = 1.0124 (1.312 sec/step)\n",
            "I0919 17:33:01.711891 140474388072320 learning.py:507] global step 2811: loss = 1.2356 (1.298 sec/step)\n",
            "I0919 17:33:03.026395 140474388072320 learning.py:507] global step 2812: loss = 0.7704 (1.313 sec/step)\n",
            "I0919 17:33:04.319994 140474388072320 learning.py:507] global step 2813: loss = 1.0827 (1.292 sec/step)\n",
            "I0919 17:33:05.667844 140474388072320 learning.py:507] global step 2814: loss = 0.8824 (1.346 sec/step)\n",
            "I0919 17:33:06.974690 140474388072320 learning.py:507] global step 2815: loss = 0.7902 (1.305 sec/step)\n",
            "I0919 17:33:08.272583 140474388072320 learning.py:507] global step 2816: loss = 0.8909 (1.296 sec/step)\n",
            "I0919 17:33:09.614366 140474388072320 learning.py:507] global step 2817: loss = 0.8804 (1.340 sec/step)\n",
            "I0919 17:33:10.892718 140474388072320 learning.py:507] global step 2818: loss = 1.0220 (1.276 sec/step)\n",
            "I0919 17:33:12.230731 140474388072320 learning.py:507] global step 2819: loss = 0.8031 (1.336 sec/step)\n",
            "I0919 17:33:13.541497 140474388072320 learning.py:507] global step 2820: loss = 0.9205 (1.309 sec/step)\n",
            "I0919 17:33:14.865238 140474388072320 learning.py:507] global step 2821: loss = 0.8089 (1.322 sec/step)\n",
            "I0919 17:33:16.195851 140474388072320 learning.py:507] global step 2822: loss = 1.2565 (1.328 sec/step)\n",
            "I0919 17:33:17.476903 140474388072320 learning.py:507] global step 2823: loss = 0.7807 (1.277 sec/step)\n",
            "I0919 17:33:18.805850 140474388072320 learning.py:507] global step 2824: loss = 1.1422 (1.324 sec/step)\n",
            "I0919 17:33:20.145382 140474388072320 learning.py:507] global step 2825: loss = 0.8773 (1.338 sec/step)\n",
            "I0919 17:33:21.525054 140474388072320 learning.py:507] global step 2826: loss = 0.9043 (1.378 sec/step)\n",
            "I0919 17:33:22.833159 140474388072320 learning.py:507] global step 2827: loss = 0.8368 (1.306 sec/step)\n",
            "I0919 17:33:24.137269 140474388072320 learning.py:507] global step 2828: loss = 0.7336 (1.302 sec/step)\n",
            "I0919 17:33:25.480016 140474388072320 learning.py:507] global step 2829: loss = 0.8943 (1.341 sec/step)\n",
            "I0919 17:33:26.818258 140474388072320 learning.py:507] global step 2830: loss = 0.8060 (1.337 sec/step)\n",
            "I0919 17:33:28.116843 140474388072320 learning.py:507] global step 2831: loss = 0.7873 (1.296 sec/step)\n",
            "I0919 17:33:29.452613 140474388072320 learning.py:507] global step 2832: loss = 0.9441 (1.334 sec/step)\n",
            "I0919 17:33:30.788363 140474388072320 learning.py:507] global step 2833: loss = 1.0539 (1.334 sec/step)\n",
            "I0919 17:33:32.078318 140474388072320 learning.py:507] global step 2834: loss = 0.9198 (1.288 sec/step)\n",
            "I0919 17:33:33.424179 140474388072320 learning.py:507] global step 2835: loss = 0.9729 (1.344 sec/step)\n",
            "I0919 17:33:34.723737 140474388072320 learning.py:507] global step 2836: loss = 0.8779 (1.298 sec/step)\n",
            "I0919 17:33:36.057451 140474388072320 learning.py:507] global step 2837: loss = 1.0778 (1.331 sec/step)\n",
            "I0919 17:33:37.361066 140474388072320 learning.py:507] global step 2838: loss = 0.8001 (1.302 sec/step)\n",
            "I0919 17:33:38.703826 140474388072320 learning.py:507] global step 2839: loss = 0.9503 (1.341 sec/step)\n",
            "I0919 17:33:40.054754 140474388072320 learning.py:507] global step 2840: loss = 0.7532 (1.349 sec/step)\n",
            "I0919 17:33:41.372193 140474388072320 learning.py:507] global step 2841: loss = 1.5636 (1.315 sec/step)\n",
            "I0919 17:33:42.728034 140474388072320 learning.py:507] global step 2842: loss = 1.1072 (1.354 sec/step)\n",
            "I0919 17:33:44.031761 140474388072320 learning.py:507] global step 2843: loss = 0.7317 (1.302 sec/step)\n",
            "I0919 17:33:45.363774 140474388072320 learning.py:507] global step 2844: loss = 1.0784 (1.330 sec/step)\n",
            "I0919 17:33:46.641942 140474388072320 learning.py:507] global step 2845: loss = 1.1835 (1.277 sec/step)\n",
            "I0919 17:33:47.937418 140474388072320 learning.py:507] global step 2846: loss = 0.7624 (1.294 sec/step)\n",
            "I0919 17:33:49.280940 140474388072320 learning.py:507] global step 2847: loss = 0.9247 (1.342 sec/step)\n",
            "I0919 17:33:50.640558 140474388072320 learning.py:507] global step 2848: loss = 0.8022 (1.358 sec/step)\n",
            "I0919 17:33:51.968858 140474388072320 learning.py:507] global step 2849: loss = 0.9358 (1.326 sec/step)\n",
            "I0919 17:33:53.305607 140474388072320 learning.py:507] global step 2850: loss = 0.8025 (1.335 sec/step)\n",
            "I0919 17:33:54.634264 140474388072320 learning.py:507] global step 2851: loss = 0.8136 (1.327 sec/step)\n",
            "I0919 17:33:55.947604 140474388072320 learning.py:507] global step 2852: loss = 0.7299 (1.311 sec/step)\n",
            "I0919 17:33:57.300575 140474388072320 learning.py:507] global step 2853: loss = 0.9373 (1.351 sec/step)\n",
            "I0919 17:33:58.604868 140474388072320 learning.py:507] global step 2854: loss = 0.9219 (1.300 sec/step)\n",
            "I0919 17:33:59.920690 140474388072320 learning.py:507] global step 2855: loss = 1.1917 (1.314 sec/step)\n",
            "I0919 17:34:01.272530 140474388072320 learning.py:507] global step 2856: loss = 1.1137 (1.350 sec/step)\n",
            "I0919 17:34:02.573889 140474388072320 learning.py:507] global step 2857: loss = 0.9134 (1.300 sec/step)\n",
            "I0919 17:34:03.879561 140474388072320 learning.py:507] global step 2858: loss = 1.4338 (1.304 sec/step)\n",
            "I0919 17:34:05.165046 140474388072320 learning.py:507] global step 2859: loss = 0.9339 (1.283 sec/step)\n",
            "I0919 17:34:06.515171 140474388072320 learning.py:507] global step 2860: loss = 0.7814 (1.348 sec/step)\n",
            "I0919 17:34:07.871902 140474388072320 learning.py:507] global step 2861: loss = 0.8397 (1.355 sec/step)\n",
            "I0919 17:34:09.205436 140474388072320 learning.py:507] global step 2862: loss = 0.9695 (1.332 sec/step)\n",
            "I0919 17:34:10.510168 140474388072320 learning.py:507] global step 2863: loss = 0.9365 (1.303 sec/step)\n",
            "I0919 17:34:11.838711 140474388072320 learning.py:507] global step 2864: loss = 0.8966 (1.327 sec/step)\n",
            "I0919 17:34:13.141025 140474388072320 learning.py:507] global step 2865: loss = 0.7327 (1.301 sec/step)\n",
            "I0919 17:34:14.465490 140474388072320 learning.py:507] global step 2866: loss = 0.8719 (1.322 sec/step)\n",
            "I0919 17:34:15.783198 140474388072320 learning.py:507] global step 2867: loss = 0.9803 (1.316 sec/step)\n",
            "I0919 17:34:17.043642 140474388072320 learning.py:507] global step 2868: loss = 1.4037 (1.258 sec/step)\n",
            "I0919 17:34:18.336482 140474388072320 learning.py:507] global step 2869: loss = 0.9682 (1.291 sec/step)\n",
            "I0919 17:34:19.621787 140474388072320 learning.py:507] global step 2870: loss = 0.9110 (1.284 sec/step)\n",
            "I0919 17:34:20.967936 140474388072320 learning.py:507] global step 2871: loss = 1.0258 (1.344 sec/step)\n",
            "I0919 17:34:22.282318 140474388072320 learning.py:507] global step 2872: loss = 1.2629 (1.313 sec/step)\n",
            "I0919 17:34:23.609391 140474388072320 learning.py:507] global step 2873: loss = 0.7372 (1.325 sec/step)\n",
            "I0919 17:34:24.898662 140474388072320 learning.py:507] global step 2874: loss = 1.0671 (1.288 sec/step)\n",
            "I0919 17:34:26.189240 140474388072320 learning.py:507] global step 2875: loss = 0.8777 (1.289 sec/step)\n",
            "I0919 17:34:28.262006 140474388072320 learning.py:507] global step 2876: loss = 1.0912 (2.071 sec/step)\n",
            "I0919 17:34:28.550152 140471297337088 supervisor.py:1050] Recording summary at step 2876.\n",
            "I0919 17:34:29.581370 140474388072320 learning.py:507] global step 2877: loss = 1.0853 (1.318 sec/step)\n",
            "I0919 17:34:30.860970 140474388072320 learning.py:507] global step 2878: loss = 0.8371 (1.277 sec/step)\n",
            "I0919 17:34:32.185992 140474388072320 learning.py:507] global step 2879: loss = 0.9170 (1.323 sec/step)\n",
            "I0919 17:34:33.487494 140474388072320 learning.py:507] global step 2880: loss = 0.7871 (1.300 sec/step)\n",
            "I0919 17:34:34.797759 140474388072320 learning.py:507] global step 2881: loss = 0.8093 (1.308 sec/step)\n",
            "I0919 17:34:36.097209 140474388072320 learning.py:507] global step 2882: loss = 0.8699 (1.298 sec/step)\n",
            "I0919 17:34:37.419341 140474388072320 learning.py:507] global step 2883: loss = 0.8263 (1.320 sec/step)\n",
            "I0919 17:34:38.713807 140474388072320 learning.py:507] global step 2884: loss = 0.9998 (1.293 sec/step)\n",
            "I0919 17:34:40.008048 140474388072320 learning.py:507] global step 2885: loss = 0.9662 (1.292 sec/step)\n",
            "I0919 17:34:41.373450 140474388072320 learning.py:507] global step 2886: loss = 1.0435 (1.364 sec/step)\n",
            "I0919 17:34:42.712316 140474388072320 learning.py:507] global step 2887: loss = 1.0278 (1.337 sec/step)\n",
            "I0919 17:34:44.041071 140474388072320 learning.py:507] global step 2888: loss = 0.8843 (1.327 sec/step)\n",
            "I0919 17:34:45.372988 140474388072320 learning.py:507] global step 2889: loss = 1.0128 (1.330 sec/step)\n",
            "I0919 17:34:46.702202 140474388072320 learning.py:507] global step 2890: loss = 1.0389 (1.327 sec/step)\n",
            "I0919 17:34:47.999289 140474388072320 learning.py:507] global step 2891: loss = 1.0978 (1.295 sec/step)\n",
            "I0919 17:34:49.264332 140474388072320 learning.py:507] global step 2892: loss = 0.9236 (1.263 sec/step)\n",
            "I0919 17:34:50.589056 140474388072320 learning.py:507] global step 2893: loss = 0.9774 (1.323 sec/step)\n",
            "I0919 17:34:51.886392 140474388072320 learning.py:507] global step 2894: loss = 0.9268 (1.296 sec/step)\n",
            "I0919 17:34:53.195353 140474388072320 learning.py:507] global step 2895: loss = 1.0990 (1.307 sec/step)\n",
            "I0919 17:34:54.498719 140474388072320 learning.py:507] global step 2896: loss = 1.3095 (1.301 sec/step)\n",
            "I0919 17:34:55.787103 140474388072320 learning.py:507] global step 2897: loss = 0.8736 (1.287 sec/step)\n",
            "I0919 17:34:57.108752 140474388072320 learning.py:507] global step 2898: loss = 1.0005 (1.320 sec/step)\n",
            "I0919 17:34:58.399843 140474388072320 learning.py:507] global step 2899: loss = 1.1780 (1.290 sec/step)\n",
            "I0919 17:34:59.698339 140474388072320 learning.py:507] global step 2900: loss = 0.9930 (1.297 sec/step)\n",
            "I0919 17:35:00.998167 140474388072320 learning.py:507] global step 2901: loss = 0.9565 (1.298 sec/step)\n",
            "I0919 17:35:02.325630 140474388072320 learning.py:507] global step 2902: loss = 0.9263 (1.325 sec/step)\n",
            "I0919 17:35:03.652177 140474388072320 learning.py:507] global step 2903: loss = 1.0375 (1.325 sec/step)\n",
            "I0919 17:35:04.979834 140474388072320 learning.py:507] global step 2904: loss = 0.7997 (1.326 sec/step)\n",
            "I0919 17:35:06.283238 140474388072320 learning.py:507] global step 2905: loss = 1.0156 (1.302 sec/step)\n",
            "I0919 17:35:07.593381 140474388072320 learning.py:507] global step 2906: loss = 0.9499 (1.308 sec/step)\n",
            "I0919 17:35:08.883081 140474388072320 learning.py:507] global step 2907: loss = 0.6572 (1.288 sec/step)\n",
            "I0919 17:35:10.213877 140474388072320 learning.py:507] global step 2908: loss = 0.9225 (1.329 sec/step)\n",
            "I0919 17:35:11.511265 140474388072320 learning.py:507] global step 2909: loss = 0.8890 (1.295 sec/step)\n",
            "I0919 17:35:12.811652 140474388072320 learning.py:507] global step 2910: loss = 0.6870 (1.299 sec/step)\n",
            "I0919 17:35:14.135457 140474388072320 learning.py:507] global step 2911: loss = 0.8822 (1.322 sec/step)\n",
            "I0919 17:35:15.443870 140474388072320 learning.py:507] global step 2912: loss = 1.1671 (1.307 sec/step)\n",
            "I0919 17:35:16.727607 140474388072320 learning.py:507] global step 2913: loss = 0.9058 (1.282 sec/step)\n",
            "I0919 17:35:18.035149 140474388072320 learning.py:507] global step 2914: loss = 0.9743 (1.306 sec/step)\n",
            "I0919 17:35:19.329807 140474388072320 learning.py:507] global step 2915: loss = 0.8917 (1.293 sec/step)\n",
            "I0919 17:35:20.623832 140474388072320 learning.py:507] global step 2916: loss = 0.7247 (1.292 sec/step)\n",
            "I0919 17:35:21.953485 140474388072320 learning.py:507] global step 2917: loss = 0.8811 (1.328 sec/step)\n",
            "I0919 17:35:23.278552 140474388072320 learning.py:507] global step 2918: loss = 0.8467 (1.323 sec/step)\n",
            "I0919 17:35:24.576968 140474388072320 learning.py:507] global step 2919: loss = 1.2888 (1.297 sec/step)\n",
            "I0919 17:35:25.861841 140474388072320 learning.py:507] global step 2920: loss = 0.8626 (1.283 sec/step)\n",
            "I0919 17:35:27.178314 140474388072320 learning.py:507] global step 2921: loss = 1.1642 (1.315 sec/step)\n",
            "I0919 17:35:28.521096 140474388072320 learning.py:507] global step 2922: loss = 1.0622 (1.341 sec/step)\n",
            "I0919 17:35:29.869268 140474388072320 learning.py:507] global step 2923: loss = 0.8367 (1.346 sec/step)\n",
            "I0919 17:35:31.203180 140474388072320 learning.py:507] global step 2924: loss = 0.9255 (1.332 sec/step)\n",
            "I0919 17:35:32.560180 140474388072320 learning.py:507] global step 2925: loss = 0.8389 (1.355 sec/step)\n",
            "I0919 17:35:33.863858 140474388072320 learning.py:507] global step 2926: loss = 0.8635 (1.302 sec/step)\n",
            "I0919 17:35:35.186203 140474388072320 learning.py:507] global step 2927: loss = 0.9752 (1.321 sec/step)\n",
            "I0919 17:35:36.491342 140474388072320 learning.py:507] global step 2928: loss = 0.8589 (1.303 sec/step)\n",
            "I0919 17:35:37.768671 140474388072320 learning.py:507] global step 2929: loss = 0.7476 (1.276 sec/step)\n",
            "I0919 17:35:39.096800 140474388072320 learning.py:507] global step 2930: loss = 0.9375 (1.326 sec/step)\n",
            "I0919 17:35:40.412329 140474388072320 learning.py:507] global step 2931: loss = 0.7778 (1.314 sec/step)\n",
            "I0919 17:35:41.744322 140474388072320 learning.py:507] global step 2932: loss = 1.4293 (1.331 sec/step)\n",
            "I0919 17:35:43.046562 140474388072320 learning.py:507] global step 2933: loss = 0.8847 (1.300 sec/step)\n",
            "I0919 17:35:44.405580 140474388072320 learning.py:507] global step 2934: loss = 1.0552 (1.357 sec/step)\n",
            "I0919 17:35:45.688204 140474388072320 learning.py:507] global step 2935: loss = 1.1597 (1.281 sec/step)\n",
            "I0919 17:35:46.988964 140474388072320 learning.py:507] global step 2936: loss = 0.6758 (1.299 sec/step)\n",
            "I0919 17:35:48.305722 140474388072320 learning.py:507] global step 2937: loss = 0.9224 (1.315 sec/step)\n",
            "I0919 17:35:49.627789 140474388072320 learning.py:507] global step 2938: loss = 0.8905 (1.320 sec/step)\n",
            "I0919 17:35:50.929791 140474388072320 learning.py:507] global step 2939: loss = 0.9113 (1.299 sec/step)\n",
            "I0919 17:35:52.260153 140474388072320 learning.py:507] global step 2940: loss = 0.9863 (1.329 sec/step)\n",
            "I0919 17:35:53.559501 140474388072320 learning.py:507] global step 2941: loss = 0.8299 (1.298 sec/step)\n",
            "I0919 17:35:54.894409 140474388072320 learning.py:507] global step 2942: loss = 1.0753 (1.333 sec/step)\n",
            "I0919 17:35:56.185827 140474388072320 learning.py:507] global step 2943: loss = 1.0289 (1.290 sec/step)\n",
            "I0919 17:35:57.486814 140474388072320 learning.py:507] global step 2944: loss = 0.7875 (1.299 sec/step)\n",
            "I0919 17:35:58.812438 140474388072320 learning.py:507] global step 2945: loss = 1.2273 (1.324 sec/step)\n",
            "I0919 17:36:00.116413 140474388072320 learning.py:507] global step 2946: loss = 0.9849 (1.302 sec/step)\n",
            "I0919 17:36:01.424757 140474388072320 learning.py:507] global step 2947: loss = 0.9380 (1.307 sec/step)\n",
            "I0919 17:36:02.745176 140474388072320 learning.py:507] global step 2948: loss = 0.9877 (1.319 sec/step)\n",
            "I0919 17:36:04.043936 140474388072320 learning.py:507] global step 2949: loss = 0.9935 (1.297 sec/step)\n",
            "I0919 17:36:05.353590 140474388072320 learning.py:507] global step 2950: loss = 0.9395 (1.308 sec/step)\n",
            "I0919 17:36:06.650355 140474388072320 learning.py:507] global step 2951: loss = 0.7393 (1.295 sec/step)\n",
            "I0919 17:36:07.984940 140474388072320 learning.py:507] global step 2952: loss = 1.0065 (1.333 sec/step)\n",
            "I0919 17:36:09.299453 140474388072320 learning.py:507] global step 2953: loss = 0.8544 (1.313 sec/step)\n",
            "I0919 17:36:10.582547 140474388072320 learning.py:507] global step 2954: loss = 0.8938 (1.281 sec/step)\n",
            "I0919 17:36:11.874211 140474388072320 learning.py:507] global step 2955: loss = 1.0050 (1.290 sec/step)\n",
            "I0919 17:36:13.177596 140474388072320 learning.py:507] global step 2956: loss = 1.2094 (1.301 sec/step)\n",
            "I0919 17:36:14.486417 140474388072320 learning.py:507] global step 2957: loss = 0.7613 (1.307 sec/step)\n",
            "I0919 17:36:15.787157 140474388072320 learning.py:507] global step 2958: loss = 0.9277 (1.299 sec/step)\n",
            "I0919 17:36:17.136145 140474388072320 learning.py:507] global step 2959: loss = 0.9274 (1.347 sec/step)\n",
            "I0919 17:36:18.460235 140474388072320 learning.py:507] global step 2960: loss = 0.8842 (1.322 sec/step)\n",
            "I0919 17:36:19.760070 140474388072320 learning.py:507] global step 2961: loss = 0.8015 (1.296 sec/step)\n",
            "I0919 17:36:21.057219 140474388072320 learning.py:507] global step 2962: loss = 1.0054 (1.295 sec/step)\n",
            "I0919 17:36:22.386465 140474388072320 learning.py:507] global step 2963: loss = 0.7361 (1.328 sec/step)\n",
            "I0919 17:36:23.684323 140474388072320 learning.py:507] global step 2964: loss = 0.7508 (1.296 sec/step)\n",
            "I0919 17:36:25.024060 140474388072320 learning.py:507] global step 2965: loss = 1.0332 (1.338 sec/step)\n",
            "I0919 17:36:26.334935 140474388072320 learning.py:507] global step 2966: loss = 0.8497 (1.309 sec/step)\n",
            "I0919 17:36:27.700850 140474388072320 learning.py:507] global step 2967: loss = 0.8132 (1.364 sec/step)\n",
            "I0919 17:36:29.501056 140471297337088 supervisor.py:1050] Recording summary at step 2967.\n",
            "I0919 17:36:29.906331 140474388072320 learning.py:507] global step 2968: loss = 0.8726 (2.201 sec/step)\n",
            "I0919 17:36:31.180322 140474388072320 learning.py:507] global step 2969: loss = 0.9894 (1.272 sec/step)\n",
            "I0919 17:36:32.478478 140474388072320 learning.py:507] global step 2970: loss = 1.1003 (1.296 sec/step)\n",
            "I0919 17:36:33.798028 140474388072320 learning.py:507] global step 2971: loss = 0.9109 (1.318 sec/step)\n",
            "I0919 17:36:35.115942 140474388072320 learning.py:507] global step 2972: loss = 0.8220 (1.316 sec/step)\n",
            "I0919 17:36:36.441922 140474388072320 learning.py:507] global step 2973: loss = 0.8021 (1.324 sec/step)\n",
            "I0919 17:36:37.763401 140474388072320 learning.py:507] global step 2974: loss = 0.8798 (1.319 sec/step)\n",
            "I0919 17:36:39.055686 140474388072320 learning.py:507] global step 2975: loss = 0.7858 (1.291 sec/step)\n",
            "I0919 17:36:40.350286 140474388072320 learning.py:507] global step 2976: loss = 0.6711 (1.293 sec/step)\n",
            "I0919 17:36:41.657479 140474388072320 learning.py:507] global step 2977: loss = 0.9408 (1.305 sec/step)\n",
            "I0919 17:36:42.981671 140474388072320 learning.py:507] global step 2978: loss = 0.8154 (1.323 sec/step)\n",
            "I0919 17:36:44.286288 140474388072320 learning.py:507] global step 2979: loss = 0.9361 (1.303 sec/step)\n",
            "I0919 17:36:45.610218 140474388072320 learning.py:507] global step 2980: loss = 1.0037 (1.322 sec/step)\n",
            "I0919 17:36:46.912178 140474388072320 learning.py:507] global step 2981: loss = 0.8161 (1.301 sec/step)\n",
            "I0919 17:36:48.241765 140474388072320 learning.py:507] global step 2982: loss = 1.1008 (1.328 sec/step)\n",
            "I0919 17:36:49.558880 140474388072320 learning.py:507] global step 2983: loss = 1.2009 (1.315 sec/step)\n",
            "I0919 17:36:50.851861 140474388072320 learning.py:507] global step 2984: loss = 0.8253 (1.291 sec/step)\n",
            "I0919 17:36:52.206255 140474388072320 learning.py:507] global step 2985: loss = 1.2067 (1.353 sec/step)\n",
            "I0919 17:36:53.542588 140474388072320 learning.py:507] global step 2986: loss = 0.7772 (1.335 sec/step)\n",
            "I0919 17:36:54.866470 140474388072320 learning.py:507] global step 2987: loss = 1.0070 (1.322 sec/step)\n",
            "I0919 17:36:56.203452 140474388072320 learning.py:507] global step 2988: loss = 0.9613 (1.335 sec/step)\n",
            "I0919 17:36:57.524333 140474388072320 learning.py:507] global step 2989: loss = 0.9098 (1.319 sec/step)\n",
            "I0919 17:36:58.891402 140474388072320 learning.py:507] global step 2990: loss = 0.7903 (1.365 sec/step)\n",
            "I0919 17:37:00.249974 140474388072320 learning.py:507] global step 2991: loss = 0.9388 (1.357 sec/step)\n",
            "I0919 17:37:01.554743 140474388072320 learning.py:507] global step 2992: loss = 0.9441 (1.303 sec/step)\n",
            "I0919 17:37:02.868047 140474388072320 learning.py:507] global step 2993: loss = 1.1291 (1.312 sec/step)\n",
            "I0919 17:37:04.190788 140474388072320 learning.py:507] global step 2994: loss = 0.9058 (1.321 sec/step)\n",
            "I0919 17:37:05.505012 140474388072320 learning.py:507] global step 2995: loss = 0.8178 (1.312 sec/step)\n",
            "I0919 17:37:06.807712 140474388072320 learning.py:507] global step 2996: loss = 1.0111 (1.301 sec/step)\n",
            "I0919 17:37:08.119076 140474388072320 learning.py:507] global step 2997: loss = 0.9973 (1.310 sec/step)\n",
            "I0919 17:37:09.458488 140474388072320 learning.py:507] global step 2998: loss = 1.1385 (1.338 sec/step)\n",
            "I0919 17:37:10.762868 140474388072320 learning.py:507] global step 2999: loss = 1.1210 (1.303 sec/step)\n",
            "I0919 17:37:12.088265 140474388072320 learning.py:507] global step 3000: loss = 1.0895 (1.324 sec/step)\n",
            "I0919 17:37:13.419930 140474388072320 learning.py:507] global step 3001: loss = 1.0863 (1.330 sec/step)\n",
            "I0919 17:37:14.762423 140474388072320 learning.py:507] global step 3002: loss = 0.9960 (1.341 sec/step)\n",
            "I0919 17:37:16.074423 140474388072320 learning.py:507] global step 3003: loss = 0.9264 (1.310 sec/step)\n",
            "I0919 17:37:17.391865 140474388072320 learning.py:507] global step 3004: loss = 0.9968 (1.315 sec/step)\n",
            "I0919 17:37:18.673830 140474388072320 learning.py:507] global step 3005: loss = 1.2022 (1.280 sec/step)\n",
            "I0919 17:37:19.975455 140474388072320 learning.py:507] global step 3006: loss = 1.4553 (1.300 sec/step)\n",
            "I0919 17:37:21.281542 140474388072320 learning.py:507] global step 3007: loss = 0.8353 (1.304 sec/step)\n",
            "I0919 17:37:22.622329 140474388072320 learning.py:507] global step 3008: loss = 0.7512 (1.339 sec/step)\n",
            "I0919 17:37:23.947822 140474388072320 learning.py:507] global step 3009: loss = 0.9631 (1.324 sec/step)\n",
            "I0919 17:37:25.303750 140474388072320 learning.py:507] global step 3010: loss = 0.7321 (1.354 sec/step)\n",
            "I0919 17:37:26.614274 140474388072320 learning.py:507] global step 3011: loss = 1.0290 (1.309 sec/step)\n",
            "I0919 17:37:27.907315 140474388072320 learning.py:507] global step 3012: loss = 1.0164 (1.291 sec/step)\n",
            "I0919 17:37:29.226696 140474388072320 learning.py:507] global step 3013: loss = 0.8428 (1.318 sec/step)\n",
            "I0919 17:37:30.546309 140474388072320 learning.py:507] global step 3014: loss = 0.9085 (1.318 sec/step)\n",
            "I0919 17:37:31.857079 140474388072320 learning.py:507] global step 3015: loss = 0.9557 (1.309 sec/step)\n",
            "I0919 17:37:33.167746 140474388072320 learning.py:507] global step 3016: loss = 0.8692 (1.309 sec/step)\n",
            "I0919 17:37:34.530597 140474388072320 learning.py:507] global step 3017: loss = 0.9194 (1.361 sec/step)\n",
            "I0919 17:37:35.862456 140474388072320 learning.py:507] global step 3018: loss = 0.8885 (1.328 sec/step)\n",
            "I0919 17:37:37.204858 140474388072320 learning.py:507] global step 3019: loss = 1.0416 (1.341 sec/step)\n",
            "I0919 17:37:38.501391 140474388072320 learning.py:507] global step 3020: loss = 0.7437 (1.295 sec/step)\n",
            "I0919 17:37:39.807326 140474388072320 learning.py:507] global step 3021: loss = 0.9355 (1.304 sec/step)\n",
            "I0919 17:37:41.134361 140474388072320 learning.py:507] global step 3022: loss = 0.7703 (1.325 sec/step)\n",
            "I0919 17:37:42.442358 140474388072320 learning.py:507] global step 3023: loss = 1.2973 (1.306 sec/step)\n",
            "I0919 17:37:43.813271 140474388072320 learning.py:507] global step 3024: loss = 0.9064 (1.369 sec/step)\n",
            "I0919 17:37:45.120649 140474388072320 learning.py:507] global step 3025: loss = 0.9712 (1.306 sec/step)\n",
            "I0919 17:37:46.442914 140474388072320 learning.py:507] global step 3026: loss = 1.0215 (1.321 sec/step)\n",
            "I0919 17:37:47.799029 140474388072320 learning.py:507] global step 3027: loss = 0.7753 (1.354 sec/step)\n",
            "I0919 17:37:49.147808 140474388072320 learning.py:507] global step 3028: loss = 0.8939 (1.347 sec/step)\n",
            "I0919 17:37:50.474097 140474388072320 learning.py:507] global step 3029: loss = 0.9411 (1.325 sec/step)\n",
            "I0919 17:37:51.767646 140474388072320 learning.py:507] global step 3030: loss = 0.8258 (1.292 sec/step)\n",
            "I0919 17:37:53.126496 140474388072320 learning.py:507] global step 3031: loss = 0.9420 (1.357 sec/step)\n",
            "I0919 17:37:54.426701 140474388072320 learning.py:507] global step 3032: loss = 1.0028 (1.299 sec/step)\n",
            "I0919 17:37:55.742060 140474388072320 learning.py:507] global step 3033: loss = 0.8160 (1.313 sec/step)\n",
            "I0919 17:37:57.044766 140474388072320 learning.py:507] global step 3034: loss = 0.8850 (1.301 sec/step)\n",
            "I0919 17:37:58.323596 140474388072320 learning.py:507] global step 3035: loss = 1.0055 (1.277 sec/step)\n",
            "I0919 17:37:59.615664 140474388072320 learning.py:507] global step 3036: loss = 0.8794 (1.290 sec/step)\n",
            "I0919 17:38:00.923237 140474388072320 learning.py:507] global step 3037: loss = 1.0891 (1.306 sec/step)\n",
            "I0919 17:38:02.281167 140474388072320 learning.py:507] global step 3038: loss = 1.2422 (1.355 sec/step)\n",
            "I0919 17:38:03.567739 140474388072320 learning.py:507] global step 3039: loss = 0.8604 (1.285 sec/step)\n",
            "I0919 17:38:04.849697 140474388072320 learning.py:507] global step 3040: loss = 0.9467 (1.280 sec/step)\n",
            "I0919 17:38:06.139686 140474388072320 learning.py:507] global step 3041: loss = 0.9769 (1.288 sec/step)\n",
            "I0919 17:38:07.434653 140474388072320 learning.py:507] global step 3042: loss = 0.9437 (1.293 sec/step)\n",
            "I0919 17:38:08.741333 140474388072320 learning.py:507] global step 3043: loss = 1.0821 (1.305 sec/step)\n",
            "I0919 17:38:10.095429 140474388072320 learning.py:507] global step 3044: loss = 1.1199 (1.353 sec/step)\n",
            "I0919 17:38:11.401082 140474388072320 learning.py:507] global step 3045: loss = 0.8735 (1.304 sec/step)\n",
            "I0919 17:38:12.697408 140474388072320 learning.py:507] global step 3046: loss = 1.0133 (1.295 sec/step)\n",
            "I0919 17:38:14.010909 140474388072320 learning.py:507] global step 3047: loss = 0.8637 (1.312 sec/step)\n",
            "I0919 17:38:15.294748 140474388072320 learning.py:507] global step 3048: loss = 1.0978 (1.282 sec/step)\n",
            "I0919 17:38:16.632672 140474388072320 learning.py:507] global step 3049: loss = 0.9888 (1.336 sec/step)\n",
            "I0919 17:38:17.937617 140474388072320 learning.py:507] global step 3050: loss = 1.4545 (1.303 sec/step)\n",
            "I0919 17:38:19.239425 140474388072320 learning.py:507] global step 3051: loss = 0.9066 (1.300 sec/step)\n",
            "I0919 17:38:20.587346 140474388072320 learning.py:507] global step 3052: loss = 0.8834 (1.346 sec/step)\n",
            "I0919 17:38:21.867717 140474388072320 learning.py:507] global step 3053: loss = 0.8102 (1.279 sec/step)\n",
            "I0919 17:38:23.165321 140474388072320 learning.py:507] global step 3054: loss = 0.7006 (1.296 sec/step)\n",
            "I0919 17:38:24.552454 140474388072320 learning.py:507] global step 3055: loss = 1.0903 (1.385 sec/step)\n",
            "I0919 17:38:25.885580 140474388072320 learning.py:507] global step 3056: loss = 0.9301 (1.331 sec/step)\n",
            "I0919 17:38:27.256269 140474388072320 learning.py:507] global step 3057: loss = 1.1329 (1.367 sec/step)\n",
            "I0919 17:38:28.916965 140471297337088 supervisor.py:1050] Recording summary at step 3057.\n",
            "I0919 17:38:29.358071 140474388072320 learning.py:507] global step 3058: loss = 1.1269 (1.987 sec/step)\n",
            "I0919 17:38:30.682946 140474388072320 learning.py:507] global step 3059: loss = 0.9580 (1.323 sec/step)\n",
            "I0919 17:38:31.951540 140474388072320 learning.py:507] global step 3060: loss = 0.8120 (1.267 sec/step)\n",
            "I0919 17:38:33.270292 140474388072320 learning.py:507] global step 3061: loss = 1.1077 (1.317 sec/step)\n",
            "I0919 17:38:34.591475 140474388072320 learning.py:507] global step 3062: loss = 0.7757 (1.320 sec/step)\n",
            "I0919 17:38:35.902517 140474388072320 learning.py:507] global step 3063: loss = 0.9385 (1.309 sec/step)\n",
            "I0919 17:38:37.214855 140474388072320 learning.py:507] global step 3064: loss = 1.2109 (1.311 sec/step)\n",
            "I0919 17:38:38.526227 140474388072320 learning.py:507] global step 3065: loss = 0.8521 (1.309 sec/step)\n",
            "I0919 17:38:39.837672 140474388072320 learning.py:507] global step 3066: loss = 0.8746 (1.310 sec/step)\n",
            "I0919 17:38:41.130665 140474388072320 learning.py:507] global step 3067: loss = 0.8982 (1.291 sec/step)\n",
            "I0919 17:38:42.445791 140474388072320 learning.py:507] global step 3068: loss = 0.8264 (1.313 sec/step)\n",
            "I0919 17:38:43.782720 140474388072320 learning.py:507] global step 3069: loss = 0.9154 (1.335 sec/step)\n",
            "I0919 17:38:45.121618 140474388072320 learning.py:507] global step 3070: loss = 0.7611 (1.337 sec/step)\n",
            "I0919 17:38:46.426851 140474388072320 learning.py:507] global step 3071: loss = 0.6456 (1.304 sec/step)\n",
            "I0919 17:38:47.761105 140474388072320 learning.py:507] global step 3072: loss = 0.9760 (1.333 sec/step)\n",
            "I0919 17:38:49.079884 140474388072320 learning.py:507] global step 3073: loss = 0.7277 (1.317 sec/step)\n",
            "I0919 17:38:50.375284 140474388072320 learning.py:507] global step 3074: loss = 1.1614 (1.294 sec/step)\n",
            "I0919 17:38:51.686297 140474388072320 learning.py:507] global step 3075: loss = 0.9897 (1.309 sec/step)\n",
            "I0919 17:38:52.989722 140474388072320 learning.py:507] global step 3076: loss = 0.9795 (1.302 sec/step)\n",
            "I0919 17:38:54.338804 140474388072320 learning.py:507] global step 3077: loss = 1.0766 (1.347 sec/step)\n",
            "I0919 17:38:55.737634 140474388072320 learning.py:507] global step 3078: loss = 0.7781 (1.397 sec/step)\n",
            "I0919 17:38:57.045778 140474388072320 learning.py:507] global step 3079: loss = 1.1796 (1.306 sec/step)\n",
            "I0919 17:38:58.365336 140474388072320 learning.py:507] global step 3080: loss = 1.1956 (1.318 sec/step)\n",
            "I0919 17:38:59.654237 140474388072320 learning.py:507] global step 3081: loss = 1.0603 (1.287 sec/step)\n",
            "I0919 17:39:00.953195 140474388072320 learning.py:507] global step 3082: loss = 1.0256 (1.297 sec/step)\n",
            "I0919 17:39:02.277509 140474388072320 learning.py:507] global step 3083: loss = 1.0839 (1.323 sec/step)\n",
            "I0919 17:39:03.595968 140474388072320 learning.py:507] global step 3084: loss = 0.9924 (1.317 sec/step)\n",
            "I0919 17:39:04.903593 140474388072320 learning.py:507] global step 3085: loss = 0.9534 (1.306 sec/step)\n",
            "I0919 17:39:06.271240 140474388072320 learning.py:507] global step 3086: loss = 0.9794 (1.366 sec/step)\n",
            "I0919 17:39:07.590925 140474388072320 learning.py:507] global step 3087: loss = 0.9204 (1.318 sec/step)\n",
            "I0919 17:39:08.939977 140474388072320 learning.py:507] global step 3088: loss = 1.0433 (1.347 sec/step)\n",
            "I0919 17:39:10.250684 140474388072320 learning.py:507] global step 3089: loss = 0.8504 (1.309 sec/step)\n",
            "I0919 17:39:11.565088 140474388072320 learning.py:507] global step 3090: loss = 0.9933 (1.313 sec/step)\n",
            "I0919 17:39:12.889259 140474388072320 learning.py:507] global step 3091: loss = 0.9264 (1.322 sec/step)\n",
            "I0919 17:39:14.192567 140474388072320 learning.py:507] global step 3092: loss = 0.8343 (1.301 sec/step)\n",
            "I0919 17:39:15.534981 140474388072320 learning.py:507] global step 3093: loss = 0.8638 (1.340 sec/step)\n",
            "I0919 17:39:16.868952 140474388072320 learning.py:507] global step 3094: loss = 0.7829 (1.332 sec/step)\n",
            "I0919 17:39:18.239145 140474388072320 learning.py:507] global step 3095: loss = 1.0146 (1.368 sec/step)\n",
            "I0919 17:39:19.595024 140474388072320 learning.py:507] global step 3096: loss = 0.8103 (1.354 sec/step)\n",
            "I0919 17:39:20.899717 140474388072320 learning.py:507] global step 3097: loss = 1.1332 (1.303 sec/step)\n",
            "I0919 17:39:22.244375 140474388072320 learning.py:507] global step 3098: loss = 0.9717 (1.343 sec/step)\n",
            "I0919 17:39:23.595897 140474388072320 learning.py:507] global step 3099: loss = 1.0299 (1.350 sec/step)\n",
            "I0919 17:39:24.898076 140474388072320 learning.py:507] global step 3100: loss = 0.9787 (1.300 sec/step)\n",
            "I0919 17:39:26.188287 140474388072320 learning.py:507] global step 3101: loss = 0.8213 (1.289 sec/step)\n",
            "I0919 17:39:27.555190 140474388072320 learning.py:507] global step 3102: loss = 0.9444 (1.365 sec/step)\n",
            "I0919 17:39:28.865799 140474388072320 learning.py:507] global step 3103: loss = 1.3038 (1.309 sec/step)\n",
            "I0919 17:39:30.173633 140474388072320 learning.py:507] global step 3104: loss = 0.9620 (1.306 sec/step)\n",
            "I0919 17:39:31.531063 140474388072320 learning.py:507] global step 3105: loss = 0.8487 (1.356 sec/step)\n",
            "I0919 17:39:32.857881 140474388072320 learning.py:507] global step 3106: loss = 0.8367 (1.325 sec/step)\n",
            "I0919 17:39:34.156621 140474388072320 learning.py:507] global step 3107: loss = 0.7782 (1.297 sec/step)\n",
            "I0919 17:39:35.432359 140474388072320 learning.py:507] global step 3108: loss = 1.0038 (1.274 sec/step)\n",
            "I0919 17:39:36.762300 140474388072320 learning.py:507] global step 3109: loss = 0.8029 (1.328 sec/step)\n",
            "I0919 17:39:38.087234 140474388072320 learning.py:507] global step 3110: loss = 1.1011 (1.323 sec/step)\n",
            "I0919 17:39:39.373994 140474388072320 learning.py:507] global step 3111: loss = 0.7452 (1.285 sec/step)\n",
            "I0919 17:39:40.715090 140474388072320 learning.py:507] global step 3112: loss = 0.7091 (1.339 sec/step)\n",
            "I0919 17:39:42.037415 140474388072320 learning.py:507] global step 3113: loss = 0.9391 (1.320 sec/step)\n",
            "I0919 17:39:43.376960 140474388072320 learning.py:507] global step 3114: loss = 0.9160 (1.338 sec/step)\n",
            "I0919 17:39:44.683170 140474388072320 learning.py:507] global step 3115: loss = 1.3506 (1.304 sec/step)\n",
            "I0919 17:39:45.988071 140474388072320 learning.py:507] global step 3116: loss = 0.9975 (1.303 sec/step)\n",
            "I0919 17:39:47.292315 140474388072320 learning.py:507] global step 3117: loss = 0.8467 (1.303 sec/step)\n",
            "I0919 17:39:48.599998 140474388072320 learning.py:507] global step 3118: loss = 1.0849 (1.306 sec/step)\n",
            "I0919 17:39:49.912652 140474388072320 learning.py:507] global step 3119: loss = 1.0558 (1.311 sec/step)\n",
            "I0919 17:39:51.231348 140474388072320 learning.py:507] global step 3120: loss = 1.1513 (1.317 sec/step)\n",
            "I0919 17:39:52.531693 140474388072320 learning.py:507] global step 3121: loss = 0.9415 (1.298 sec/step)\n",
            "I0919 17:39:53.851881 140474388072320 learning.py:507] global step 3122: loss = 0.9853 (1.319 sec/step)\n",
            "I0919 17:39:55.149077 140474388072320 learning.py:507] global step 3123: loss = 0.7354 (1.295 sec/step)\n",
            "I0919 17:39:56.475405 140474388072320 learning.py:507] global step 3124: loss = 0.9141 (1.325 sec/step)\n",
            "I0919 17:39:57.742280 140474388072320 learning.py:507] global step 3125: loss = 0.8485 (1.265 sec/step)\n",
            "I0919 17:39:59.074052 140474388072320 learning.py:507] global step 3126: loss = 0.7883 (1.330 sec/step)\n",
            "I0919 17:40:00.429646 140474388072320 learning.py:507] global step 3127: loss = 1.0474 (1.353 sec/step)\n",
            "I0919 17:40:01.745738 140474388072320 learning.py:507] global step 3128: loss = 1.2699 (1.314 sec/step)\n",
            "I0919 17:40:03.047527 140474388072320 learning.py:507] global step 3129: loss = 0.9298 (1.300 sec/step)\n",
            "I0919 17:40:04.351419 140474388072320 learning.py:507] global step 3130: loss = 0.7825 (1.302 sec/step)\n",
            "I0919 17:40:05.649781 140474388072320 learning.py:507] global step 3131: loss = 0.8196 (1.297 sec/step)\n",
            "I0919 17:40:06.960545 140474388072320 learning.py:507] global step 3132: loss = 1.0204 (1.309 sec/step)\n",
            "I0919 17:40:08.277731 140474388072320 learning.py:507] global step 3133: loss = 0.9542 (1.316 sec/step)\n",
            "I0919 17:40:09.575820 140474388072320 learning.py:507] global step 3134: loss = 0.7827 (1.296 sec/step)\n",
            "I0919 17:40:10.881575 140474388072320 learning.py:507] global step 3135: loss = 0.7783 (1.304 sec/step)\n",
            "I0919 17:40:12.172174 140474388072320 learning.py:507] global step 3136: loss = 0.8011 (1.289 sec/step)\n",
            "I0919 17:40:13.533423 140474388072320 learning.py:507] global step 3137: loss = 1.1208 (1.359 sec/step)\n",
            "I0919 17:40:14.820681 140474388072320 learning.py:507] global step 3138: loss = 0.7590 (1.286 sec/step)\n",
            "I0919 17:40:16.110384 140474388072320 learning.py:507] global step 3139: loss = 0.8021 (1.288 sec/step)\n",
            "I0919 17:40:17.424346 140474388072320 learning.py:507] global step 3140: loss = 1.2182 (1.312 sec/step)\n",
            "I0919 17:40:18.711920 140474388072320 learning.py:507] global step 3141: loss = 0.9443 (1.286 sec/step)\n",
            "I0919 17:40:20.046252 140474388072320 learning.py:507] global step 3142: loss = 0.9490 (1.333 sec/step)\n",
            "I0919 17:40:21.373147 140474388072320 learning.py:507] global step 3143: loss = 0.9338 (1.325 sec/step)\n",
            "I0919 17:40:22.665715 140474388072320 learning.py:507] global step 3144: loss = 0.9157 (1.291 sec/step)\n",
            "I0919 17:40:23.991461 140474388072320 learning.py:507] global step 3145: loss = 0.9340 (1.324 sec/step)\n",
            "I0919 17:40:25.372900 140474388072320 learning.py:507] global step 3146: loss = 0.8689 (1.379 sec/step)\n",
            "I0919 17:40:26.686836 140474388072320 learning.py:507] global step 3147: loss = 0.9814 (1.312 sec/step)\n",
            "I0919 17:40:26.872853 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 17:40:29.138283 140474388072320 learning.py:507] global step 3148: loss = 0.7994 (2.438 sec/step)\n",
            "I0919 17:40:29.164978 140471297337088 supervisor.py:1050] Recording summary at step 3148.\n",
            "I0919 17:40:31.010855 140474388072320 learning.py:507] global step 3149: loss = 0.9954 (1.851 sec/step)\n",
            "I0919 17:40:32.987096 140474388072320 learning.py:507] global step 3150: loss = 0.9404 (1.973 sec/step)\n",
            "I0919 17:40:34.325072 140474388072320 learning.py:507] global step 3151: loss = 0.9919 (1.336 sec/step)\n",
            "I0919 17:40:35.656404 140474388072320 learning.py:507] global step 3152: loss = 1.0248 (1.330 sec/step)\n",
            "I0919 17:40:36.965068 140474388072320 learning.py:507] global step 3153: loss = 0.7345 (1.307 sec/step)\n",
            "I0919 17:40:38.254466 140474388072320 learning.py:507] global step 3154: loss = 0.8455 (1.287 sec/step)\n",
            "I0919 17:40:39.578802 140474388072320 learning.py:507] global step 3155: loss = 1.0228 (1.322 sec/step)\n",
            "I0919 17:40:40.888292 140474388072320 learning.py:507] global step 3156: loss = 0.9088 (1.308 sec/step)\n",
            "I0919 17:40:42.180999 140474388072320 learning.py:507] global step 3157: loss = 0.9028 (1.291 sec/step)\n",
            "I0919 17:40:43.533164 140474388072320 learning.py:507] global step 3158: loss = 1.0353 (1.351 sec/step)\n",
            "I0919 17:40:44.878550 140474388072320 learning.py:507] global step 3159: loss = 1.2634 (1.344 sec/step)\n",
            "I0919 17:40:46.187769 140474388072320 learning.py:507] global step 3160: loss = 0.7844 (1.308 sec/step)\n",
            "I0919 17:40:47.515492 140474388072320 learning.py:507] global step 3161: loss = 0.9096 (1.326 sec/step)\n",
            "I0919 17:40:48.818639 140474388072320 learning.py:507] global step 3162: loss = 1.1226 (1.302 sec/step)\n",
            "I0919 17:40:50.110251 140474388072320 learning.py:507] global step 3163: loss = 1.0695 (1.290 sec/step)\n",
            "I0919 17:40:51.429004 140474388072320 learning.py:507] global step 3164: loss = 0.8083 (1.317 sec/step)\n",
            "I0919 17:40:52.764899 140474388072320 learning.py:507] global step 3165: loss = 1.4287 (1.334 sec/step)\n",
            "I0919 17:40:54.069689 140474388072320 learning.py:507] global step 3166: loss = 0.9253 (1.302 sec/step)\n",
            "I0919 17:40:55.344218 140474388072320 learning.py:507] global step 3167: loss = 1.1923 (1.272 sec/step)\n",
            "I0919 17:40:56.687308 140474388072320 learning.py:507] global step 3168: loss = 0.8831 (1.341 sec/step)\n",
            "I0919 17:40:57.998336 140474388072320 learning.py:507] global step 3169: loss = 0.9320 (1.309 sec/step)\n",
            "I0919 17:40:59.308408 140474388072320 learning.py:507] global step 3170: loss = 0.7617 (1.308 sec/step)\n",
            "I0919 17:41:00.595556 140474388072320 learning.py:507] global step 3171: loss = 0.8159 (1.285 sec/step)\n",
            "I0919 17:41:01.902021 140474388072320 learning.py:507] global step 3172: loss = 0.7867 (1.304 sec/step)\n",
            "I0919 17:41:03.221713 140474388072320 learning.py:507] global step 3173: loss = 0.8152 (1.318 sec/step)\n",
            "I0919 17:41:04.521972 140474388072320 learning.py:507] global step 3174: loss = 0.9796 (1.299 sec/step)\n",
            "I0919 17:41:05.864995 140474388072320 learning.py:507] global step 3175: loss = 0.9226 (1.341 sec/step)\n",
            "I0919 17:41:07.143831 140474388072320 learning.py:507] global step 3176: loss = 1.1844 (1.277 sec/step)\n",
            "I0919 17:41:08.470611 140474388072320 learning.py:507] global step 3177: loss = 0.8648 (1.325 sec/step)\n",
            "I0919 17:41:09.789984 140474388072320 learning.py:507] global step 3178: loss = 1.0881 (1.317 sec/step)\n",
            "I0919 17:41:11.120904 140474388072320 learning.py:507] global step 3179: loss = 0.9257 (1.329 sec/step)\n",
            "I0919 17:41:12.423362 140474388072320 learning.py:507] global step 3180: loss = 0.9310 (1.301 sec/step)\n",
            "I0919 17:41:13.710907 140474388072320 learning.py:507] global step 3181: loss = 0.8430 (1.286 sec/step)\n",
            "I0919 17:41:15.025912 140474388072320 learning.py:507] global step 3182: loss = 0.7335 (1.313 sec/step)\n",
            "I0919 17:41:16.324448 140474388072320 learning.py:507] global step 3183: loss = 0.9328 (1.297 sec/step)\n",
            "I0919 17:41:17.618932 140474388072320 learning.py:507] global step 3184: loss = 1.0378 (1.293 sec/step)\n",
            "I0919 17:41:18.944809 140474388072320 learning.py:507] global step 3185: loss = 0.8472 (1.324 sec/step)\n",
            "I0919 17:41:20.242361 140474388072320 learning.py:507] global step 3186: loss = 1.0123 (1.296 sec/step)\n",
            "I0919 17:41:21.554605 140474388072320 learning.py:507] global step 3187: loss = 0.7816 (1.310 sec/step)\n",
            "I0919 17:41:22.861855 140474388072320 learning.py:507] global step 3188: loss = 0.8415 (1.305 sec/step)\n",
            "I0919 17:41:24.174834 140474388072320 learning.py:507] global step 3189: loss = 0.8055 (1.311 sec/step)\n",
            "I0919 17:41:25.502345 140474388072320 learning.py:507] global step 3190: loss = 1.0022 (1.326 sec/step)\n",
            "I0919 17:41:26.812412 140474388072320 learning.py:507] global step 3191: loss = 0.9761 (1.308 sec/step)\n",
            "I0919 17:41:28.141369 140474388072320 learning.py:507] global step 3192: loss = 0.8866 (1.327 sec/step)\n",
            "I0919 17:41:29.502629 140474388072320 learning.py:507] global step 3193: loss = 0.7305 (1.359 sec/step)\n",
            "I0919 17:41:30.801367 140474388072320 learning.py:507] global step 3194: loss = 0.8290 (1.297 sec/step)\n",
            "I0919 17:41:32.076066 140474388072320 learning.py:507] global step 3195: loss = 0.8111 (1.273 sec/step)\n",
            "I0919 17:41:33.423722 140474388072320 learning.py:507] global step 3196: loss = 0.9376 (1.346 sec/step)\n",
            "I0919 17:41:34.732595 140474388072320 learning.py:507] global step 3197: loss = 0.8740 (1.307 sec/step)\n",
            "I0919 17:41:36.046550 140474388072320 learning.py:507] global step 3198: loss = 0.7997 (1.312 sec/step)\n",
            "I0919 17:41:37.366995 140474388072320 learning.py:507] global step 3199: loss = 1.1161 (1.319 sec/step)\n",
            "I0919 17:41:38.681493 140474388072320 learning.py:507] global step 3200: loss = 0.8148 (1.313 sec/step)\n",
            "I0919 17:41:39.961690 140474388072320 learning.py:507] global step 3201: loss = 1.0532 (1.278 sec/step)\n",
            "I0919 17:41:41.272279 140474388072320 learning.py:507] global step 3202: loss = 0.6761 (1.308 sec/step)\n",
            "I0919 17:41:42.585439 140474388072320 learning.py:507] global step 3203: loss = 1.0594 (1.312 sec/step)\n",
            "I0919 17:41:43.876889 140474388072320 learning.py:507] global step 3204: loss = 0.9130 (1.290 sec/step)\n",
            "I0919 17:41:45.188515 140474388072320 learning.py:507] global step 3205: loss = 0.8160 (1.309 sec/step)\n",
            "I0919 17:41:46.525582 140474388072320 learning.py:507] global step 3206: loss = 0.9893 (1.335 sec/step)\n",
            "I0919 17:41:47.824944 140474388072320 learning.py:507] global step 3207: loss = 0.8862 (1.297 sec/step)\n",
            "I0919 17:41:49.134207 140474388072320 learning.py:507] global step 3208: loss = 0.7397 (1.307 sec/step)\n",
            "I0919 17:41:50.479386 140474388072320 learning.py:507] global step 3209: loss = 0.9949 (1.343 sec/step)\n",
            "I0919 17:41:51.777766 140474388072320 learning.py:507] global step 3210: loss = 1.0747 (1.296 sec/step)\n",
            "I0919 17:41:53.057446 140474388072320 learning.py:507] global step 3211: loss = 0.7002 (1.278 sec/step)\n",
            "I0919 17:41:54.373799 140474388072320 learning.py:507] global step 3212: loss = 1.0081 (1.315 sec/step)\n",
            "I0919 17:41:55.716931 140474388072320 learning.py:507] global step 3213: loss = 0.9997 (1.341 sec/step)\n",
            "I0919 17:41:57.025597 140474388072320 learning.py:507] global step 3214: loss = 0.9181 (1.307 sec/step)\n",
            "I0919 17:41:58.316775 140474388072320 learning.py:507] global step 3215: loss = 0.9345 (1.290 sec/step)\n",
            "I0919 17:41:59.640360 140474388072320 learning.py:507] global step 3216: loss = 1.0253 (1.322 sec/step)\n",
            "I0919 17:42:00.927815 140474388072320 learning.py:507] global step 3217: loss = 0.7403 (1.285 sec/step)\n",
            "I0919 17:42:02.231832 140474388072320 learning.py:507] global step 3218: loss = 0.9082 (1.302 sec/step)\n",
            "I0919 17:42:03.547327 140474388072320 learning.py:507] global step 3219: loss = 0.8491 (1.314 sec/step)\n",
            "I0919 17:42:04.857266 140474388072320 learning.py:507] global step 3220: loss = 1.0055 (1.308 sec/step)\n",
            "I0919 17:42:06.169631 140474388072320 learning.py:507] global step 3221: loss = 1.0905 (1.310 sec/step)\n",
            "I0919 17:42:07.489069 140474388072320 learning.py:507] global step 3222: loss = 0.9004 (1.318 sec/step)\n",
            "I0919 17:42:08.790950 140474388072320 learning.py:507] global step 3223: loss = 0.7949 (1.300 sec/step)\n",
            "I0919 17:42:10.112643 140474388072320 learning.py:507] global step 3224: loss = 0.9715 (1.320 sec/step)\n",
            "I0919 17:42:11.388524 140474388072320 learning.py:507] global step 3225: loss = 0.8411 (1.274 sec/step)\n",
            "I0919 17:42:12.733906 140474388072320 learning.py:507] global step 3226: loss = 0.8412 (1.344 sec/step)\n",
            "I0919 17:42:14.085961 140474388072320 learning.py:507] global step 3227: loss = 0.7480 (1.350 sec/step)\n",
            "I0919 17:42:15.439609 140474388072320 learning.py:507] global step 3228: loss = 0.8614 (1.352 sec/step)\n",
            "I0919 17:42:16.730887 140474388072320 learning.py:507] global step 3229: loss = 0.9622 (1.289 sec/step)\n",
            "I0919 17:42:18.013128 140474388072320 learning.py:507] global step 3230: loss = 1.1083 (1.280 sec/step)\n",
            "I0919 17:42:19.309021 140474388072320 learning.py:507] global step 3231: loss = 0.9199 (1.294 sec/step)\n",
            "I0919 17:42:20.602216 140474388072320 learning.py:507] global step 3232: loss = 1.0197 (1.292 sec/step)\n",
            "I0919 17:42:21.917264 140474388072320 learning.py:507] global step 3233: loss = 0.9216 (1.313 sec/step)\n",
            "I0919 17:42:23.261918 140474388072320 learning.py:507] global step 3234: loss = 0.8764 (1.343 sec/step)\n",
            "I0919 17:42:24.577934 140474388072320 learning.py:507] global step 3235: loss = 0.8772 (1.314 sec/step)\n",
            "I0919 17:42:25.982043 140474388072320 learning.py:507] global step 3236: loss = 0.8050 (1.402 sec/step)\n",
            "I0919 17:42:27.291665 140474388072320 learning.py:507] global step 3237: loss = 0.8246 (1.308 sec/step)\n",
            "I0919 17:42:29.086153 140471297337088 supervisor.py:1050] Recording summary at step 3237.\n",
            "I0919 17:42:29.520708 140474388072320 learning.py:507] global step 3238: loss = 1.1114 (2.227 sec/step)\n",
            "I0919 17:42:30.834543 140474388072320 learning.py:507] global step 3239: loss = 0.9522 (1.312 sec/step)\n",
            "I0919 17:42:32.130127 140474388072320 learning.py:507] global step 3240: loss = 1.0036 (1.294 sec/step)\n",
            "I0919 17:42:33.427732 140474388072320 learning.py:507] global step 3241: loss = 0.7138 (1.296 sec/step)\n",
            "I0919 17:42:34.723897 140474388072320 learning.py:507] global step 3242: loss = 0.8775 (1.295 sec/step)\n",
            "I0919 17:42:36.030824 140474388072320 learning.py:507] global step 3243: loss = 0.7821 (1.305 sec/step)\n",
            "I0919 17:42:37.332149 140474388072320 learning.py:507] global step 3244: loss = 0.7527 (1.300 sec/step)\n",
            "I0919 17:42:38.622925 140474388072320 learning.py:507] global step 3245: loss = 0.9333 (1.289 sec/step)\n",
            "I0919 17:42:39.906349 140474388072320 learning.py:507] global step 3246: loss = 1.0394 (1.282 sec/step)\n",
            "I0919 17:42:41.274647 140474388072320 learning.py:507] global step 3247: loss = 0.8115 (1.366 sec/step)\n",
            "I0919 17:42:42.586065 140474388072320 learning.py:507] global step 3248: loss = 0.8515 (1.310 sec/step)\n",
            "I0919 17:42:43.907272 140474388072320 learning.py:507] global step 3249: loss = 0.8752 (1.319 sec/step)\n",
            "I0919 17:42:45.243985 140474388072320 learning.py:507] global step 3250: loss = 1.2197 (1.335 sec/step)\n",
            "I0919 17:42:46.569758 140474388072320 learning.py:507] global step 3251: loss = 0.7646 (1.324 sec/step)\n",
            "I0919 17:42:47.906713 140474388072320 learning.py:507] global step 3252: loss = 0.8902 (1.335 sec/step)\n",
            "I0919 17:42:49.210650 140474388072320 learning.py:507] global step 3253: loss = 0.9073 (1.302 sec/step)\n",
            "I0919 17:42:50.549912 140474388072320 learning.py:507] global step 3254: loss = 0.8169 (1.337 sec/step)\n",
            "I0919 17:42:51.849199 140474388072320 learning.py:507] global step 3255: loss = 0.7875 (1.297 sec/step)\n",
            "I0919 17:42:53.229007 140474388072320 learning.py:507] global step 3256: loss = 0.8214 (1.378 sec/step)\n",
            "I0919 17:42:54.549036 140474388072320 learning.py:507] global step 3257: loss = 1.0299 (1.318 sec/step)\n",
            "I0919 17:42:55.869225 140474388072320 learning.py:507] global step 3258: loss = 0.9911 (1.319 sec/step)\n",
            "I0919 17:42:57.180660 140474388072320 learning.py:507] global step 3259: loss = 1.0671 (1.310 sec/step)\n",
            "I0919 17:42:58.489279 140474388072320 learning.py:507] global step 3260: loss = 0.9330 (1.307 sec/step)\n",
            "I0919 17:42:59.801340 140474388072320 learning.py:507] global step 3261: loss = 0.7413 (1.310 sec/step)\n",
            "I0919 17:43:01.139628 140474388072320 learning.py:507] global step 3262: loss = 0.8843 (1.336 sec/step)\n",
            "I0919 17:43:02.441757 140474388072320 learning.py:507] global step 3263: loss = 0.8536 (1.300 sec/step)\n",
            "I0919 17:43:03.801345 140474388072320 learning.py:507] global step 3264: loss = 1.1536 (1.358 sec/step)\n",
            "I0919 17:43:05.120001 140474388072320 learning.py:507] global step 3265: loss = 0.6268 (1.317 sec/step)\n",
            "I0919 17:43:06.429071 140474388072320 learning.py:507] global step 3266: loss = 0.8956 (1.307 sec/step)\n",
            "I0919 17:43:07.773283 140474388072320 learning.py:507] global step 3267: loss = 0.6764 (1.343 sec/step)\n",
            "I0919 17:43:09.104574 140474388072320 learning.py:507] global step 3268: loss = 0.9241 (1.330 sec/step)\n",
            "I0919 17:43:10.396228 140474388072320 learning.py:507] global step 3269: loss = 0.7685 (1.290 sec/step)\n",
            "I0919 17:43:11.718348 140474388072320 learning.py:507] global step 3270: loss = 0.7416 (1.320 sec/step)\n",
            "I0919 17:43:13.017045 140474388072320 learning.py:507] global step 3271: loss = 0.6965 (1.297 sec/step)\n",
            "I0919 17:43:14.307259 140474388072320 learning.py:507] global step 3272: loss = 0.9369 (1.288 sec/step)\n",
            "I0919 17:43:15.621416 140474388072320 learning.py:507] global step 3273: loss = 0.9212 (1.312 sec/step)\n",
            "I0919 17:43:16.938596 140474388072320 learning.py:507] global step 3274: loss = 0.7786 (1.315 sec/step)\n",
            "I0919 17:43:18.231746 140474388072320 learning.py:507] global step 3275: loss = 0.7043 (1.292 sec/step)\n",
            "I0919 17:43:19.573765 140474388072320 learning.py:507] global step 3276: loss = 0.8444 (1.340 sec/step)\n",
            "I0919 17:43:20.882559 140474388072320 learning.py:507] global step 3277: loss = 0.7746 (1.307 sec/step)\n",
            "I0919 17:43:22.190726 140474388072320 learning.py:507] global step 3278: loss = 1.1890 (1.307 sec/step)\n",
            "I0919 17:43:23.512859 140474388072320 learning.py:507] global step 3279: loss = 1.0797 (1.321 sec/step)\n",
            "I0919 17:43:24.845536 140474388072320 learning.py:507] global step 3280: loss = 0.7925 (1.331 sec/step)\n",
            "I0919 17:43:26.178253 140474388072320 learning.py:507] global step 3281: loss = 0.8730 (1.331 sec/step)\n",
            "I0919 17:43:27.492492 140474388072320 learning.py:507] global step 3282: loss = 1.0105 (1.313 sec/step)\n",
            "I0919 17:43:28.820492 140474388072320 learning.py:507] global step 3283: loss = 0.8521 (1.327 sec/step)\n",
            "I0919 17:43:30.110989 140474388072320 learning.py:507] global step 3284: loss = 1.0373 (1.289 sec/step)\n",
            "I0919 17:43:31.402618 140474388072320 learning.py:507] global step 3285: loss = 0.7757 (1.290 sec/step)\n",
            "I0919 17:43:32.745932 140474388072320 learning.py:507] global step 3286: loss = 0.8430 (1.342 sec/step)\n",
            "I0919 17:43:34.019453 140474388072320 learning.py:507] global step 3287: loss = 0.6483 (1.271 sec/step)\n",
            "I0919 17:43:35.300174 140474388072320 learning.py:507] global step 3288: loss = 0.9944 (1.279 sec/step)\n",
            "I0919 17:43:36.676851 140474388072320 learning.py:507] global step 3289: loss = 0.8607 (1.375 sec/step)\n",
            "I0919 17:43:37.988744 140474388072320 learning.py:507] global step 3290: loss = 0.8372 (1.310 sec/step)\n",
            "I0919 17:43:39.294891 140474388072320 learning.py:507] global step 3291: loss = 0.8402 (1.304 sec/step)\n",
            "I0919 17:43:40.651454 140474388072320 learning.py:507] global step 3292: loss = 0.7303 (1.355 sec/step)\n",
            "I0919 17:43:41.987301 140474388072320 learning.py:507] global step 3293: loss = 0.9268 (1.334 sec/step)\n",
            "I0919 17:43:43.293225 140474388072320 learning.py:507] global step 3294: loss = 0.7968 (1.304 sec/step)\n",
            "I0919 17:43:44.593828 140474388072320 learning.py:507] global step 3295: loss = 1.1534 (1.299 sec/step)\n",
            "I0919 17:43:45.895987 140474388072320 learning.py:507] global step 3296: loss = 0.8795 (1.300 sec/step)\n",
            "I0919 17:43:47.221601 140474388072320 learning.py:507] global step 3297: loss = 1.0188 (1.324 sec/step)\n",
            "I0919 17:43:48.551951 140474388072320 learning.py:507] global step 3298: loss = 0.7863 (1.329 sec/step)\n",
            "I0919 17:43:49.859697 140474388072320 learning.py:507] global step 3299: loss = 1.0097 (1.306 sec/step)\n",
            "I0919 17:43:51.148468 140474388072320 learning.py:507] global step 3300: loss = 0.8789 (1.287 sec/step)\n",
            "I0919 17:43:52.457161 140474388072320 learning.py:507] global step 3301: loss = 0.9157 (1.307 sec/step)\n",
            "I0919 17:43:53.762238 140474388072320 learning.py:507] global step 3302: loss = 1.0098 (1.303 sec/step)\n",
            "I0919 17:43:55.057706 140474388072320 learning.py:507] global step 3303: loss = 0.8370 (1.294 sec/step)\n",
            "I0919 17:43:56.338341 140474388072320 learning.py:507] global step 3304: loss = 1.0113 (1.279 sec/step)\n",
            "I0919 17:43:57.649981 140474388072320 learning.py:507] global step 3305: loss = 0.7671 (1.310 sec/step)\n",
            "I0919 17:43:58.965372 140474388072320 learning.py:507] global step 3306: loss = 1.0007 (1.314 sec/step)\n",
            "I0919 17:44:00.303388 140474388072320 learning.py:507] global step 3307: loss = 0.7338 (1.336 sec/step)\n",
            "I0919 17:44:01.628530 140474388072320 learning.py:507] global step 3308: loss = 1.0642 (1.324 sec/step)\n",
            "I0919 17:44:02.926568 140474388072320 learning.py:507] global step 3309: loss = 0.9438 (1.296 sec/step)\n",
            "I0919 17:44:04.235212 140474388072320 learning.py:507] global step 3310: loss = 0.8778 (1.307 sec/step)\n",
            "I0919 17:44:05.534871 140474388072320 learning.py:507] global step 3311: loss = 1.1687 (1.298 sec/step)\n",
            "I0919 17:44:06.890809 140474388072320 learning.py:507] global step 3312: loss = 0.7784 (1.354 sec/step)\n",
            "I0919 17:44:08.204139 140474388072320 learning.py:507] global step 3313: loss = 0.9930 (1.312 sec/step)\n",
            "I0919 17:44:09.483633 140474388072320 learning.py:507] global step 3314: loss = 1.2635 (1.278 sec/step)\n",
            "I0919 17:44:10.842796 140474388072320 learning.py:507] global step 3315: loss = 0.8555 (1.358 sec/step)\n",
            "I0919 17:44:12.152028 140474388072320 learning.py:507] global step 3316: loss = 0.9775 (1.307 sec/step)\n",
            "I0919 17:44:13.449162 140474388072320 learning.py:507] global step 3317: loss = 0.7782 (1.295 sec/step)\n",
            "I0919 17:44:14.778004 140474388072320 learning.py:507] global step 3318: loss = 1.0406 (1.327 sec/step)\n",
            "I0919 17:44:16.075465 140474388072320 learning.py:507] global step 3319: loss = 0.8479 (1.296 sec/step)\n",
            "I0919 17:44:17.394294 140474388072320 learning.py:507] global step 3320: loss = 1.0030 (1.317 sec/step)\n",
            "I0919 17:44:18.694499 140474388072320 learning.py:507] global step 3321: loss = 1.1001 (1.299 sec/step)\n",
            "I0919 17:44:20.039484 140474388072320 learning.py:507] global step 3322: loss = 1.0711 (1.343 sec/step)\n",
            "I0919 17:44:21.384388 140474388072320 learning.py:507] global step 3323: loss = 0.8363 (1.343 sec/step)\n",
            "I0919 17:44:22.711561 140474388072320 learning.py:507] global step 3324: loss = 0.7971 (1.325 sec/step)\n",
            "I0919 17:44:24.016356 140474388072320 learning.py:507] global step 3325: loss = 0.7963 (1.303 sec/step)\n",
            "I0919 17:44:25.335546 140474388072320 learning.py:507] global step 3326: loss = 0.9807 (1.317 sec/step)\n",
            "I0919 17:44:26.656980 140474388072320 learning.py:507] global step 3327: loss = 0.9606 (1.320 sec/step)\n",
            "I0919 17:44:28.492429 140471297337088 supervisor.py:1050] Recording summary at step 3327.\n",
            "I0919 17:44:28.945426 140474388072320 learning.py:507] global step 3328: loss = 0.8057 (2.287 sec/step)\n",
            "I0919 17:44:30.295013 140474388072320 learning.py:507] global step 3329: loss = 0.7780 (1.348 sec/step)\n",
            "I0919 17:44:31.616991 140474388072320 learning.py:507] global step 3330: loss = 0.8475 (1.320 sec/step)\n",
            "I0919 17:44:32.971921 140474388072320 learning.py:507] global step 3331: loss = 0.9046 (1.349 sec/step)\n",
            "I0919 17:44:34.297733 140474388072320 learning.py:507] global step 3332: loss = 0.9675 (1.324 sec/step)\n",
            "I0919 17:44:35.623344 140474388072320 learning.py:507] global step 3333: loss = 0.9897 (1.324 sec/step)\n",
            "I0919 17:44:36.911164 140474388072320 learning.py:507] global step 3334: loss = 0.7213 (1.286 sec/step)\n",
            "I0919 17:44:38.215935 140474388072320 learning.py:507] global step 3335: loss = 0.7817 (1.303 sec/step)\n",
            "I0919 17:44:39.531147 140474388072320 learning.py:507] global step 3336: loss = 0.8923 (1.313 sec/step)\n",
            "I0919 17:44:40.839977 140474388072320 learning.py:507] global step 3337: loss = 0.8156 (1.307 sec/step)\n",
            "I0919 17:44:42.118568 140474388072320 learning.py:507] global step 3338: loss = 1.0509 (1.277 sec/step)\n",
            "I0919 17:44:43.457042 140474388072320 learning.py:507] global step 3339: loss = 0.7837 (1.334 sec/step)\n",
            "I0919 17:44:44.815673 140474388072320 learning.py:507] global step 3340: loss = 0.9302 (1.357 sec/step)\n",
            "I0919 17:44:46.126657 140474388072320 learning.py:507] global step 3341: loss = 0.9641 (1.309 sec/step)\n",
            "I0919 17:44:47.489244 140474388072320 learning.py:507] global step 3342: loss = 0.6905 (1.361 sec/step)\n",
            "I0919 17:44:48.779180 140474388072320 learning.py:507] global step 3343: loss = 0.8648 (1.288 sec/step)\n",
            "I0919 17:44:50.123735 140474388072320 learning.py:507] global step 3344: loss = 0.9397 (1.343 sec/step)\n",
            "I0919 17:44:51.448829 140474388072320 learning.py:507] global step 3345: loss = 0.6848 (1.323 sec/step)\n",
            "I0919 17:44:52.761421 140474388072320 learning.py:507] global step 3346: loss = 0.8430 (1.311 sec/step)\n",
            "I0919 17:44:54.057923 140474388072320 learning.py:507] global step 3347: loss = 0.9358 (1.295 sec/step)\n",
            "I0919 17:44:55.428953 140474388072320 learning.py:507] global step 3348: loss = 1.1087 (1.369 sec/step)\n",
            "I0919 17:44:56.711874 140474388072320 learning.py:507] global step 3349: loss = 0.8705 (1.281 sec/step)\n",
            "I0919 17:44:58.008765 140474388072320 learning.py:507] global step 3350: loss = 1.2088 (1.295 sec/step)\n",
            "I0919 17:44:59.308903 140474388072320 learning.py:507] global step 3351: loss = 0.8357 (1.298 sec/step)\n",
            "I0919 17:45:00.616799 140474388072320 learning.py:507] global step 3352: loss = 0.8287 (1.306 sec/step)\n",
            "I0919 17:45:01.900645 140474388072320 learning.py:507] global step 3353: loss = 1.0049 (1.282 sec/step)\n",
            "I0919 17:45:03.240241 140474388072320 learning.py:507] global step 3354: loss = 0.9922 (1.338 sec/step)\n",
            "I0919 17:45:04.523179 140474388072320 learning.py:507] global step 3355: loss = 0.8329 (1.281 sec/step)\n",
            "I0919 17:45:05.872947 140474388072320 learning.py:507] global step 3356: loss = 0.7404 (1.348 sec/step)\n",
            "I0919 17:45:07.165981 140474388072320 learning.py:507] global step 3357: loss = 0.9799 (1.291 sec/step)\n",
            "I0919 17:45:08.441488 140474388072320 learning.py:507] global step 3358: loss = 0.8823 (1.274 sec/step)\n",
            "I0919 17:45:09.798146 140474388072320 learning.py:507] global step 3359: loss = 0.8850 (1.355 sec/step)\n",
            "I0919 17:45:11.114073 140474388072320 learning.py:507] global step 3360: loss = 1.0176 (1.315 sec/step)\n",
            "I0919 17:45:12.415189 140474388072320 learning.py:507] global step 3361: loss = 0.9045 (1.300 sec/step)\n",
            "I0919 17:45:13.714257 140474388072320 learning.py:507] global step 3362: loss = 0.8992 (1.297 sec/step)\n",
            "I0919 17:45:14.980150 140474388072320 learning.py:507] global step 3363: loss = 0.9126 (1.264 sec/step)\n",
            "I0919 17:45:16.311902 140474388072320 learning.py:507] global step 3364: loss = 1.0534 (1.330 sec/step)\n",
            "I0919 17:45:17.610061 140474388072320 learning.py:507] global step 3365: loss = 0.6886 (1.296 sec/step)\n",
            "I0919 17:45:18.938240 140474388072320 learning.py:507] global step 3366: loss = 0.9478 (1.326 sec/step)\n",
            "I0919 17:45:20.229345 140474388072320 learning.py:507] global step 3367: loss = 0.9041 (1.289 sec/step)\n",
            "I0919 17:45:21.519325 140474388072320 learning.py:507] global step 3368: loss = 1.0020 (1.288 sec/step)\n",
            "I0919 17:45:22.860450 140474388072320 learning.py:507] global step 3369: loss = 1.1864 (1.340 sec/step)\n",
            "I0919 17:45:24.217689 140474388072320 learning.py:507] global step 3370: loss = 1.0978 (1.356 sec/step)\n",
            "I0919 17:45:25.503937 140474388072320 learning.py:507] global step 3371: loss = 0.9478 (1.285 sec/step)\n",
            "I0919 17:45:26.806742 140474388072320 learning.py:507] global step 3372: loss = 1.2864 (1.301 sec/step)\n",
            "I0919 17:45:28.128706 140474388072320 learning.py:507] global step 3373: loss = 0.8467 (1.320 sec/step)\n",
            "I0919 17:45:29.461510 140474388072320 learning.py:507] global step 3374: loss = 0.9971 (1.331 sec/step)\n",
            "I0919 17:45:30.789999 140474388072320 learning.py:507] global step 3375: loss = 0.9777 (1.327 sec/step)\n",
            "I0919 17:45:32.088596 140474388072320 learning.py:507] global step 3376: loss = 0.7558 (1.297 sec/step)\n",
            "I0919 17:45:33.413306 140474388072320 learning.py:507] global step 3377: loss = 1.0121 (1.323 sec/step)\n",
            "I0919 17:45:34.739938 140474388072320 learning.py:507] global step 3378: loss = 0.7918 (1.325 sec/step)\n",
            "I0919 17:45:36.033713 140474388072320 learning.py:507] global step 3379: loss = 0.7844 (1.292 sec/step)\n",
            "I0919 17:45:37.383862 140474388072320 learning.py:507] global step 3380: loss = 0.9329 (1.348 sec/step)\n",
            "I0919 17:45:38.712589 140474388072320 learning.py:507] global step 3381: loss = 0.9661 (1.327 sec/step)\n",
            "I0919 17:45:40.038173 140474388072320 learning.py:507] global step 3382: loss = 0.8574 (1.324 sec/step)\n",
            "I0919 17:45:41.336369 140474388072320 learning.py:507] global step 3383: loss = 0.7779 (1.296 sec/step)\n",
            "I0919 17:45:42.643676 140474388072320 learning.py:507] global step 3384: loss = 0.8833 (1.305 sec/step)\n",
            "I0919 17:45:44.004256 140474388072320 learning.py:507] global step 3385: loss = 0.9594 (1.359 sec/step)\n",
            "I0919 17:45:45.327368 140474388072320 learning.py:507] global step 3386: loss = 0.7605 (1.321 sec/step)\n",
            "I0919 17:45:46.606498 140474388072320 learning.py:507] global step 3387: loss = 0.7783 (1.277 sec/step)\n",
            "I0919 17:45:47.949335 140474388072320 learning.py:507] global step 3388: loss = 0.8021 (1.341 sec/step)\n",
            "I0919 17:45:49.247281 140474388072320 learning.py:507] global step 3389: loss = 1.1163 (1.296 sec/step)\n",
            "I0919 17:45:50.550220 140474388072320 learning.py:507] global step 3390: loss = 0.8507 (1.301 sec/step)\n",
            "I0919 17:45:51.862178 140474388072320 learning.py:507] global step 3391: loss = 0.8866 (1.310 sec/step)\n",
            "I0919 17:45:53.145459 140474388072320 learning.py:507] global step 3392: loss = 1.2622 (1.282 sec/step)\n",
            "I0919 17:45:54.447559 140474388072320 learning.py:507] global step 3393: loss = 1.1029 (1.300 sec/step)\n",
            "I0919 17:45:55.737407 140474388072320 learning.py:507] global step 3394: loss = 0.9051 (1.288 sec/step)\n",
            "I0919 17:45:57.057732 140474388072320 learning.py:507] global step 3395: loss = 0.6968 (1.319 sec/step)\n",
            "I0919 17:45:58.420157 140474388072320 learning.py:507] global step 3396: loss = 0.9531 (1.361 sec/step)\n",
            "I0919 17:45:59.721831 140474388072320 learning.py:507] global step 3397: loss = 0.7876 (1.300 sec/step)\n",
            "I0919 17:46:01.001996 140474388072320 learning.py:507] global step 3398: loss = 1.0115 (1.278 sec/step)\n",
            "I0919 17:46:02.304734 140474388072320 learning.py:507] global step 3399: loss = 0.8667 (1.301 sec/step)\n",
            "I0919 17:46:03.625174 140474388072320 learning.py:507] global step 3400: loss = 0.9332 (1.319 sec/step)\n",
            "I0919 17:46:04.908313 140474388072320 learning.py:507] global step 3401: loss = 1.3180 (1.281 sec/step)\n",
            "I0919 17:46:06.244532 140474388072320 learning.py:507] global step 3402: loss = 0.8346 (1.335 sec/step)\n",
            "I0919 17:46:07.597012 140474388072320 learning.py:507] global step 3403: loss = 0.8384 (1.350 sec/step)\n",
            "I0919 17:46:08.913074 140474388072320 learning.py:507] global step 3404: loss = 1.1017 (1.314 sec/step)\n",
            "I0919 17:46:10.216483 140474388072320 learning.py:507] global step 3405: loss = 0.7708 (1.302 sec/step)\n",
            "I0919 17:46:11.525854 140474388072320 learning.py:507] global step 3406: loss = 0.8667 (1.308 sec/step)\n",
            "I0919 17:46:12.882585 140474388072320 learning.py:507] global step 3407: loss = 0.9252 (1.355 sec/step)\n",
            "I0919 17:46:14.199493 140474388072320 learning.py:507] global step 3408: loss = 1.1659 (1.315 sec/step)\n",
            "I0919 17:46:15.490688 140474388072320 learning.py:507] global step 3409: loss = 0.9294 (1.289 sec/step)\n",
            "I0919 17:46:16.834496 140474388072320 learning.py:507] global step 3410: loss = 1.1178 (1.342 sec/step)\n",
            "I0919 17:46:18.171792 140474388072320 learning.py:507] global step 3411: loss = 1.0142 (1.335 sec/step)\n",
            "I0919 17:46:19.456104 140474388072320 learning.py:507] global step 3412: loss = 0.9004 (1.282 sec/step)\n",
            "I0919 17:46:20.760406 140474388072320 learning.py:507] global step 3413: loss = 0.9215 (1.303 sec/step)\n",
            "I0919 17:46:22.082257 140474388072320 learning.py:507] global step 3414: loss = 1.0432 (1.320 sec/step)\n",
            "I0919 17:46:23.419548 140474388072320 learning.py:507] global step 3415: loss = 0.7569 (1.336 sec/step)\n",
            "I0919 17:46:24.765526 140474388072320 learning.py:507] global step 3416: loss = 0.7133 (1.344 sec/step)\n",
            "I0919 17:46:26.066042 140474388072320 learning.py:507] global step 3417: loss = 0.7943 (1.299 sec/step)\n",
            "I0919 17:46:27.628556 140474388072320 learning.py:507] global step 3418: loss = 0.8091 (1.438 sec/step)\n",
            "I0919 17:46:28.614449 140471297337088 supervisor.py:1050] Recording summary at step 3418.\n",
            "I0919 17:46:29.711559 140474388072320 learning.py:507] global step 3419: loss = 0.9941 (2.018 sec/step)\n",
            "I0919 17:46:31.027365 140474388072320 learning.py:507] global step 3420: loss = 0.9756 (1.314 sec/step)\n",
            "I0919 17:46:32.345988 140474388072320 learning.py:507] global step 3421: loss = 0.8285 (1.317 sec/step)\n",
            "I0919 17:46:33.620255 140474388072320 learning.py:507] global step 3422: loss = 0.7636 (1.273 sec/step)\n",
            "I0919 17:46:34.940845 140474388072320 learning.py:507] global step 3423: loss = 0.9361 (1.319 sec/step)\n",
            "I0919 17:46:36.261737 140474388072320 learning.py:507] global step 3424: loss = 1.1665 (1.319 sec/step)\n",
            "I0919 17:46:37.553282 140474388072320 learning.py:507] global step 3425: loss = 0.7499 (1.290 sec/step)\n",
            "I0919 17:46:38.827493 140474388072320 learning.py:507] global step 3426: loss = 1.2985 (1.273 sec/step)\n",
            "I0919 17:46:40.170370 140474388072320 learning.py:507] global step 3427: loss = 0.9156 (1.341 sec/step)\n",
            "I0919 17:46:41.509978 140474388072320 learning.py:507] global step 3428: loss = 0.8958 (1.338 sec/step)\n",
            "I0919 17:46:42.868309 140474388072320 learning.py:507] global step 3429: loss = 0.9837 (1.357 sec/step)\n",
            "I0919 17:46:44.174259 140474388072320 learning.py:507] global step 3430: loss = 0.9556 (1.304 sec/step)\n",
            "I0919 17:46:45.464451 140474388072320 learning.py:507] global step 3431: loss = 1.1116 (1.289 sec/step)\n",
            "I0919 17:46:46.833894 140474388072320 learning.py:507] global step 3432: loss = 0.8114 (1.368 sec/step)\n",
            "I0919 17:46:48.154946 140474388072320 learning.py:507] global step 3433: loss = 0.6918 (1.319 sec/step)\n",
            "I0919 17:46:49.486320 140474388072320 learning.py:507] global step 3434: loss = 0.9943 (1.329 sec/step)\n",
            "I0919 17:46:50.826172 140474388072320 learning.py:507] global step 3435: loss = 0.6388 (1.338 sec/step)\n",
            "I0919 17:46:52.143933 140474388072320 learning.py:507] global step 3436: loss = 0.7535 (1.316 sec/step)\n",
            "I0919 17:46:53.416996 140474388072320 learning.py:507] global step 3437: loss = 0.8955 (1.271 sec/step)\n",
            "I0919 17:46:54.718544 140474388072320 learning.py:507] global step 3438: loss = 1.1742 (1.300 sec/step)\n",
            "I0919 17:46:56.045804 140474388072320 learning.py:507] global step 3439: loss = 1.2785 (1.325 sec/step)\n",
            "I0919 17:46:57.335961 140474388072320 learning.py:507] global step 3440: loss = 0.9468 (1.288 sec/step)\n",
            "I0919 17:46:58.648792 140474388072320 learning.py:507] global step 3441: loss = 0.7985 (1.311 sec/step)\n",
            "I0919 17:46:59.965856 140474388072320 learning.py:507] global step 3442: loss = 0.7790 (1.315 sec/step)\n",
            "I0919 17:47:01.315707 140474388072320 learning.py:507] global step 3443: loss = 1.2245 (1.348 sec/step)\n",
            "I0919 17:47:02.618200 140474388072320 learning.py:507] global step 3444: loss = 0.7935 (1.300 sec/step)\n",
            "I0919 17:47:03.924660 140474388072320 learning.py:507] global step 3445: loss = 0.8965 (1.304 sec/step)\n",
            "I0919 17:47:05.196876 140474388072320 learning.py:507] global step 3446: loss = 0.7553 (1.271 sec/step)\n",
            "I0919 17:47:06.498898 140474388072320 learning.py:507] global step 3447: loss = 0.9864 (1.300 sec/step)\n",
            "I0919 17:47:07.857249 140474388072320 learning.py:507] global step 3448: loss = 0.8033 (1.357 sec/step)\n",
            "I0919 17:47:09.163346 140474388072320 learning.py:507] global step 3449: loss = 0.9929 (1.304 sec/step)\n",
            "I0919 17:47:10.474442 140474388072320 learning.py:507] global step 3450: loss = 0.9667 (1.310 sec/step)\n",
            "I0919 17:47:11.781341 140474388072320 learning.py:507] global step 3451: loss = 0.8503 (1.305 sec/step)\n",
            "I0919 17:47:13.089082 140474388072320 learning.py:507] global step 3452: loss = 0.8036 (1.306 sec/step)\n",
            "I0919 17:47:14.431773 140474388072320 learning.py:507] global step 3453: loss = 1.0243 (1.341 sec/step)\n",
            "I0919 17:47:15.760355 140474388072320 learning.py:507] global step 3454: loss = 0.8268 (1.327 sec/step)\n",
            "I0919 17:47:17.030616 140474388072320 learning.py:507] global step 3455: loss = 0.7583 (1.268 sec/step)\n",
            "I0919 17:47:18.361737 140474388072320 learning.py:507] global step 3456: loss = 1.1503 (1.329 sec/step)\n",
            "I0919 17:47:19.684343 140474388072320 learning.py:507] global step 3457: loss = 0.9052 (1.321 sec/step)\n",
            "I0919 17:47:21.002458 140474388072320 learning.py:507] global step 3458: loss = 0.8104 (1.316 sec/step)\n",
            "I0919 17:47:22.316725 140474388072320 learning.py:507] global step 3459: loss = 0.9390 (1.312 sec/step)\n",
            "I0919 17:47:23.607911 140474388072320 learning.py:507] global step 3460: loss = 1.1362 (1.289 sec/step)\n",
            "I0919 17:47:24.924575 140474388072320 learning.py:507] global step 3461: loss = 1.2396 (1.315 sec/step)\n",
            "I0919 17:47:26.268068 140474388072320 learning.py:507] global step 3462: loss = 0.8567 (1.342 sec/step)\n",
            "I0919 17:47:27.588977 140474388072320 learning.py:507] global step 3463: loss = 0.8497 (1.319 sec/step)\n",
            "I0919 17:47:28.892510 140474388072320 learning.py:507] global step 3464: loss = 0.8712 (1.302 sec/step)\n",
            "I0919 17:47:30.240185 140474388072320 learning.py:507] global step 3465: loss = 0.7455 (1.346 sec/step)\n",
            "I0919 17:47:31.527887 140474388072320 learning.py:507] global step 3466: loss = 0.7305 (1.286 sec/step)\n",
            "I0919 17:47:32.867460 140474388072320 learning.py:507] global step 3467: loss = 0.8615 (1.338 sec/step)\n",
            "I0919 17:47:34.168525 140474388072320 learning.py:507] global step 3468: loss = 0.8827 (1.299 sec/step)\n",
            "I0919 17:47:35.476213 140474388072320 learning.py:507] global step 3469: loss = 0.7529 (1.304 sec/step)\n",
            "I0919 17:47:36.790024 140474388072320 learning.py:507] global step 3470: loss = 0.7899 (1.312 sec/step)\n",
            "I0919 17:47:38.112844 140474388072320 learning.py:507] global step 3471: loss = 0.8113 (1.321 sec/step)\n",
            "I0919 17:47:39.459913 140474388072320 learning.py:507] global step 3472: loss = 0.7858 (1.345 sec/step)\n",
            "I0919 17:47:40.753663 140474388072320 learning.py:507] global step 3473: loss = 0.7278 (1.292 sec/step)\n",
            "I0919 17:47:42.071369 140474388072320 learning.py:507] global step 3474: loss = 0.9611 (1.316 sec/step)\n",
            "I0919 17:47:43.392139 140474388072320 learning.py:507] global step 3475: loss = 1.3230 (1.319 sec/step)\n",
            "I0919 17:47:44.706588 140474388072320 learning.py:507] global step 3476: loss = 0.8312 (1.312 sec/step)\n",
            "I0919 17:47:46.015280 140474388072320 learning.py:507] global step 3477: loss = 0.9690 (1.307 sec/step)\n",
            "I0919 17:47:47.314455 140474388072320 learning.py:507] global step 3478: loss = 1.1654 (1.297 sec/step)\n",
            "I0919 17:47:48.635901 140474388072320 learning.py:507] global step 3479: loss = 1.0689 (1.320 sec/step)\n",
            "I0919 17:47:49.952641 140474388072320 learning.py:507] global step 3480: loss = 0.8989 (1.315 sec/step)\n",
            "I0919 17:47:51.287348 140474388072320 learning.py:507] global step 3481: loss = 0.7109 (1.333 sec/step)\n",
            "I0919 17:47:52.627489 140474388072320 learning.py:507] global step 3482: loss = 0.8004 (1.338 sec/step)\n",
            "I0919 17:47:53.961990 140474388072320 learning.py:507] global step 3483: loss = 0.8095 (1.333 sec/step)\n",
            "I0919 17:47:55.274997 140474388072320 learning.py:507] global step 3484: loss = 0.8169 (1.311 sec/step)\n",
            "I0919 17:47:56.602083 140474388072320 learning.py:507] global step 3485: loss = 0.9875 (1.325 sec/step)\n",
            "I0919 17:47:57.971755 140474388072320 learning.py:507] global step 3486: loss = 0.9476 (1.368 sec/step)\n",
            "I0919 17:47:59.281938 140474388072320 learning.py:507] global step 3487: loss = 0.7426 (1.309 sec/step)\n",
            "I0919 17:48:00.565397 140474388072320 learning.py:507] global step 3488: loss = 0.7971 (1.282 sec/step)\n",
            "I0919 17:48:01.891244 140474388072320 learning.py:507] global step 3489: loss = 0.8324 (1.324 sec/step)\n",
            "I0919 17:48:03.250272 140474388072320 learning.py:507] global step 3490: loss = 0.7054 (1.357 sec/step)\n",
            "I0919 17:48:04.582998 140474388072320 learning.py:507] global step 3491: loss = 1.0842 (1.331 sec/step)\n",
            "I0919 17:48:05.917989 140474388072320 learning.py:507] global step 3492: loss = 0.8609 (1.333 sec/step)\n",
            "I0919 17:48:07.267046 140474388072320 learning.py:507] global step 3493: loss = 0.8405 (1.347 sec/step)\n",
            "I0919 17:48:08.558779 140474388072320 learning.py:507] global step 3494: loss = 0.7127 (1.290 sec/step)\n",
            "I0919 17:48:09.882968 140474388072320 learning.py:507] global step 3495: loss = 0.7144 (1.322 sec/step)\n",
            "I0919 17:48:11.172662 140474388072320 learning.py:507] global step 3496: loss = 0.7086 (1.288 sec/step)\n",
            "I0919 17:48:12.458056 140474388072320 learning.py:507] global step 3497: loss = 0.8786 (1.284 sec/step)\n",
            "I0919 17:48:13.772042 140474388072320 learning.py:507] global step 3498: loss = 0.9884 (1.312 sec/step)\n",
            "I0919 17:48:15.122817 140474388072320 learning.py:507] global step 3499: loss = 0.9295 (1.349 sec/step)\n",
            "I0919 17:48:16.445970 140474388072320 learning.py:507] global step 3500: loss = 0.9274 (1.321 sec/step)\n",
            "I0919 17:48:17.744599 140474388072320 learning.py:507] global step 3501: loss = 0.9441 (1.297 sec/step)\n",
            "I0919 17:48:19.052869 140474388072320 learning.py:507] global step 3502: loss = 0.7584 (1.306 sec/step)\n",
            "I0919 17:48:20.362289 140474388072320 learning.py:507] global step 3503: loss = 1.0231 (1.308 sec/step)\n",
            "I0919 17:48:21.672065 140474388072320 learning.py:507] global step 3504: loss = 0.9953 (1.308 sec/step)\n",
            "I0919 17:48:22.985193 140474388072320 learning.py:507] global step 3505: loss = 0.7697 (1.312 sec/step)\n",
            "I0919 17:48:24.307635 140474388072320 learning.py:507] global step 3506: loss = 0.9157 (1.321 sec/step)\n",
            "I0919 17:48:25.623254 140474388072320 learning.py:507] global step 3507: loss = 0.8203 (1.314 sec/step)\n",
            "I0919 17:48:26.966391 140474388072320 learning.py:507] global step 3508: loss = 0.9570 (1.336 sec/step)\n",
            "I0919 17:48:29.170795 140474388072320 learning.py:507] global step 3509: loss = 0.8816 (2.203 sec/step)\n",
            "I0919 17:48:29.174510 140471297337088 supervisor.py:1050] Recording summary at step 3509.\n",
            "I0919 17:48:30.493634 140474388072320 learning.py:507] global step 3510: loss = 0.8408 (1.310 sec/step)\n",
            "I0919 17:48:31.815038 140474388072320 learning.py:507] global step 3511: loss = 0.9830 (1.320 sec/step)\n",
            "I0919 17:48:33.123670 140474388072320 learning.py:507] global step 3512: loss = 1.0394 (1.307 sec/step)\n",
            "I0919 17:48:34.442018 140474388072320 learning.py:507] global step 3513: loss = 0.8801 (1.317 sec/step)\n",
            "I0919 17:48:35.748595 140474388072320 learning.py:507] global step 3514: loss = 0.8870 (1.305 sec/step)\n",
            "I0919 17:48:37.080468 140474388072320 learning.py:507] global step 3515: loss = 0.8665 (1.330 sec/step)\n",
            "I0919 17:48:38.376177 140474388072320 learning.py:507] global step 3516: loss = 1.0743 (1.294 sec/step)\n",
            "I0919 17:48:39.668421 140474388072320 learning.py:507] global step 3517: loss = 0.7042 (1.291 sec/step)\n",
            "I0919 17:48:40.970524 140474388072320 learning.py:507] global step 3518: loss = 0.9339 (1.300 sec/step)\n",
            "I0919 17:48:42.313453 140474388072320 learning.py:507] global step 3519: loss = 1.0094 (1.341 sec/step)\n",
            "I0919 17:48:43.630131 140474388072320 learning.py:507] global step 3520: loss = 1.1332 (1.315 sec/step)\n",
            "I0919 17:48:44.942476 140474388072320 learning.py:507] global step 3521: loss = 0.7473 (1.311 sec/step)\n",
            "I0919 17:48:46.310483 140474388072320 learning.py:507] global step 3522: loss = 1.0908 (1.366 sec/step)\n",
            "I0919 17:48:47.664836 140474388072320 learning.py:507] global step 3523: loss = 0.7811 (1.353 sec/step)\n",
            "I0919 17:48:49.006855 140474388072320 learning.py:507] global step 3524: loss = 0.8838 (1.340 sec/step)\n",
            "I0919 17:48:50.366997 140474388072320 learning.py:507] global step 3525: loss = 1.0071 (1.358 sec/step)\n",
            "I0919 17:48:51.648519 140474388072320 learning.py:507] global step 3526: loss = 0.8127 (1.280 sec/step)\n",
            "I0919 17:48:52.967096 140474388072320 learning.py:507] global step 3527: loss = 1.0403 (1.317 sec/step)\n",
            "I0919 17:48:54.297461 140474388072320 learning.py:507] global step 3528: loss = 0.9105 (1.329 sec/step)\n",
            "I0919 17:48:55.603826 140474388072320 learning.py:507] global step 3529: loss = 1.0530 (1.305 sec/step)\n",
            "I0919 17:48:56.896195 140474388072320 learning.py:507] global step 3530: loss = 0.9483 (1.291 sec/step)\n",
            "I0919 17:48:58.217684 140474388072320 learning.py:507] global step 3531: loss = 0.8748 (1.320 sec/step)\n",
            "I0919 17:48:59.546436 140474388072320 learning.py:507] global step 3532: loss = 0.9745 (1.327 sec/step)\n",
            "I0919 17:49:00.827747 140474388072320 learning.py:507] global step 3533: loss = 0.9708 (1.280 sec/step)\n",
            "I0919 17:49:02.144520 140474388072320 learning.py:507] global step 3534: loss = 0.9411 (1.315 sec/step)\n",
            "I0919 17:49:03.459393 140474388072320 learning.py:507] global step 3535: loss = 0.8030 (1.313 sec/step)\n",
            "I0919 17:49:04.799999 140474388072320 learning.py:507] global step 3536: loss = 0.8337 (1.339 sec/step)\n",
            "I0919 17:49:06.131232 140474388072320 learning.py:507] global step 3537: loss = 0.9783 (1.329 sec/step)\n",
            "I0919 17:49:07.453052 140474388072320 learning.py:507] global step 3538: loss = 0.8987 (1.320 sec/step)\n",
            "I0919 17:49:08.771380 140474388072320 learning.py:507] global step 3539: loss = 0.9497 (1.314 sec/step)\n",
            "I0919 17:49:10.075251 140474388072320 learning.py:507] global step 3540: loss = 1.0878 (1.302 sec/step)\n",
            "I0919 17:49:11.381505 140474388072320 learning.py:507] global step 3541: loss = 0.8611 (1.305 sec/step)\n",
            "I0919 17:49:12.720943 140474388072320 learning.py:507] global step 3542: loss = 1.0043 (1.338 sec/step)\n",
            "I0919 17:49:14.089380 140474388072320 learning.py:507] global step 3543: loss = 0.8205 (1.367 sec/step)\n",
            "I0919 17:49:15.374262 140474388072320 learning.py:507] global step 3544: loss = 0.7137 (1.283 sec/step)\n",
            "I0919 17:49:16.675587 140474388072320 learning.py:507] global step 3545: loss = 1.0773 (1.300 sec/step)\n",
            "I0919 17:49:18.005379 140474388072320 learning.py:507] global step 3546: loss = 1.1217 (1.328 sec/step)\n",
            "I0919 17:49:19.318041 140474388072320 learning.py:507] global step 3547: loss = 0.8883 (1.311 sec/step)\n",
            "I0919 17:49:20.618508 140474388072320 learning.py:507] global step 3548: loss = 0.9061 (1.299 sec/step)\n",
            "I0919 17:49:21.957525 140474388072320 learning.py:507] global step 3549: loss = 0.8866 (1.337 sec/step)\n",
            "I0919 17:49:23.263926 140474388072320 learning.py:507] global step 3550: loss = 0.7996 (1.304 sec/step)\n",
            "I0919 17:49:24.556238 140474388072320 learning.py:507] global step 3551: loss = 0.8471 (1.291 sec/step)\n",
            "I0919 17:49:25.881361 140474388072320 learning.py:507] global step 3552: loss = 0.6998 (1.324 sec/step)\n",
            "I0919 17:49:27.174485 140474388072320 learning.py:507] global step 3553: loss = 0.8941 (1.292 sec/step)\n",
            "I0919 17:49:28.523989 140474388072320 learning.py:507] global step 3554: loss = 0.7768 (1.348 sec/step)\n",
            "I0919 17:49:29.851387 140474388072320 learning.py:507] global step 3555: loss = 1.0286 (1.325 sec/step)\n",
            "I0919 17:49:31.202470 140474388072320 learning.py:507] global step 3556: loss = 1.4781 (1.350 sec/step)\n",
            "I0919 17:49:32.484376 140474388072320 learning.py:507] global step 3557: loss = 0.8247 (1.280 sec/step)\n",
            "I0919 17:49:33.778625 140474388072320 learning.py:507] global step 3558: loss = 0.9140 (1.292 sec/step)\n",
            "I0919 17:49:35.071449 140474388072320 learning.py:507] global step 3559: loss = 0.7861 (1.291 sec/step)\n",
            "I0919 17:49:36.391066 140474388072320 learning.py:507] global step 3560: loss = 0.9790 (1.318 sec/step)\n",
            "I0919 17:49:37.709918 140474388072320 learning.py:507] global step 3561: loss = 0.6466 (1.317 sec/step)\n",
            "I0919 17:49:39.005371 140474388072320 learning.py:507] global step 3562: loss = 0.7655 (1.294 sec/step)\n",
            "I0919 17:49:40.341366 140474388072320 learning.py:507] global step 3563: loss = 1.0323 (1.334 sec/step)\n",
            "I0919 17:49:41.668618 140474388072320 learning.py:507] global step 3564: loss = 0.9975 (1.325 sec/step)\n",
            "I0919 17:49:42.976002 140474388072320 learning.py:507] global step 3565: loss = 0.8599 (1.306 sec/step)\n",
            "I0919 17:49:44.249726 140474388072320 learning.py:507] global step 3566: loss = 0.9223 (1.272 sec/step)\n",
            "I0919 17:49:45.559530 140474388072320 learning.py:507] global step 3567: loss = 0.8201 (1.308 sec/step)\n",
            "I0919 17:49:46.870922 140474388072320 learning.py:507] global step 3568: loss = 1.1009 (1.310 sec/step)\n",
            "I0919 17:49:48.222664 140474388072320 learning.py:507] global step 3569: loss = 0.9545 (1.350 sec/step)\n",
            "I0919 17:49:49.550369 140474388072320 learning.py:507] global step 3570: loss = 0.9575 (1.326 sec/step)\n",
            "I0919 17:49:50.852849 140474388072320 learning.py:507] global step 3571: loss = 0.8532 (1.301 sec/step)\n",
            "I0919 17:49:52.206648 140474388072320 learning.py:507] global step 3572: loss = 1.2689 (1.348 sec/step)\n",
            "I0919 17:49:53.575200 140474388072320 learning.py:507] global step 3573: loss = 1.0199 (1.367 sec/step)\n",
            "I0919 17:49:54.871423 140474388072320 learning.py:507] global step 3574: loss = 0.9157 (1.294 sec/step)\n",
            "I0919 17:49:56.177221 140474388072320 learning.py:507] global step 3575: loss = 1.0274 (1.304 sec/step)\n",
            "I0919 17:49:57.480298 140474388072320 learning.py:507] global step 3576: loss = 1.0162 (1.301 sec/step)\n",
            "I0919 17:49:58.797883 140474388072320 learning.py:507] global step 3577: loss = 1.1752 (1.316 sec/step)\n",
            "I0919 17:50:00.128310 140474388072320 learning.py:507] global step 3578: loss = 1.0656 (1.329 sec/step)\n",
            "I0919 17:50:01.484204 140474388072320 learning.py:507] global step 3579: loss = 0.9293 (1.354 sec/step)\n",
            "I0919 17:50:02.836091 140474388072320 learning.py:507] global step 3580: loss = 0.9310 (1.350 sec/step)\n",
            "I0919 17:50:04.146159 140474388072320 learning.py:507] global step 3581: loss = 1.0454 (1.308 sec/step)\n",
            "I0919 17:50:05.450757 140474388072320 learning.py:507] global step 3582: loss = 0.6641 (1.303 sec/step)\n",
            "I0919 17:50:06.751961 140474388072320 learning.py:507] global step 3583: loss = 0.9258 (1.300 sec/step)\n",
            "I0919 17:50:08.102160 140474388072320 learning.py:507] global step 3584: loss = 1.1889 (1.349 sec/step)\n",
            "I0919 17:50:09.389659 140474388072320 learning.py:507] global step 3585: loss = 0.9069 (1.286 sec/step)\n",
            "I0919 17:50:10.686793 140474388072320 learning.py:507] global step 3586: loss = 0.8291 (1.295 sec/step)\n",
            "I0919 17:50:12.000051 140474388072320 learning.py:507] global step 3587: loss = 1.0890 (1.311 sec/step)\n",
            "I0919 17:50:13.317207 140474388072320 learning.py:507] global step 3588: loss = 0.9041 (1.315 sec/step)\n",
            "I0919 17:50:14.666313 140474388072320 learning.py:507] global step 3589: loss = 0.8849 (1.347 sec/step)\n",
            "I0919 17:50:15.958781 140474388072320 learning.py:507] global step 3590: loss = 0.8474 (1.291 sec/step)\n",
            "I0919 17:50:17.285267 140474388072320 learning.py:507] global step 3591: loss = 0.9288 (1.325 sec/step)\n",
            "I0919 17:50:18.615597 140474388072320 learning.py:507] global step 3592: loss = 1.0892 (1.329 sec/step)\n",
            "I0919 17:50:19.963570 140474388072320 learning.py:507] global step 3593: loss = 1.1121 (1.346 sec/step)\n",
            "I0919 17:50:21.299335 140474388072320 learning.py:507] global step 3594: loss = 0.6474 (1.334 sec/step)\n",
            "I0919 17:50:22.649528 140474388072320 learning.py:507] global step 3595: loss = 0.7235 (1.348 sec/step)\n",
            "I0919 17:50:23.998610 140474388072320 learning.py:507] global step 3596: loss = 0.9164 (1.347 sec/step)\n",
            "I0919 17:50:25.310534 140474388072320 learning.py:507] global step 3597: loss = 1.2505 (1.310 sec/step)\n",
            "I0919 17:50:26.604191 140474388072320 learning.py:507] global step 3598: loss = 0.8924 (1.291 sec/step)\n",
            "I0919 17:50:26.877037 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 17:50:29.503047 140474388072320 learning.py:507] global step 3599: loss = 0.9248 (2.896 sec/step)\n",
            "I0919 17:50:29.503823 140471297337088 supervisor.py:1050] Recording summary at step 3599.\n",
            "I0919 17:50:31.317188 140474388072320 learning.py:507] global step 3600: loss = 0.8961 (1.792 sec/step)\n",
            "I0919 17:50:33.322098 140474388072320 learning.py:507] global step 3601: loss = 0.9458 (1.637 sec/step)\n",
            "I0919 17:50:34.711883 140474388072320 learning.py:507] global step 3602: loss = 0.6996 (1.387 sec/step)\n",
            "I0919 17:50:36.014790 140474388072320 learning.py:507] global step 3603: loss = 0.7491 (1.301 sec/step)\n",
            "I0919 17:50:37.342464 140474388072320 learning.py:507] global step 3604: loss = 0.6873 (1.326 sec/step)\n",
            "I0919 17:50:38.684170 140474388072320 learning.py:507] global step 3605: loss = 0.8907 (1.340 sec/step)\n",
            "I0919 17:50:39.967010 140474388072320 learning.py:507] global step 3606: loss = 0.9662 (1.281 sec/step)\n",
            "I0919 17:50:41.271183 140474388072320 learning.py:507] global step 3607: loss = 0.7709 (1.302 sec/step)\n",
            "I0919 17:50:42.569927 140474388072320 learning.py:507] global step 3608: loss = 0.6635 (1.297 sec/step)\n",
            "I0919 17:50:43.858892 140474388072320 learning.py:507] global step 3609: loss = 0.7518 (1.287 sec/step)\n",
            "I0919 17:50:45.179336 140474388072320 learning.py:507] global step 3610: loss = 1.0990 (1.319 sec/step)\n",
            "I0919 17:50:46.472009 140474388072320 learning.py:507] global step 3611: loss = 0.7222 (1.291 sec/step)\n",
            "I0919 17:50:47.806313 140474388072320 learning.py:507] global step 3612: loss = 1.1471 (1.333 sec/step)\n",
            "I0919 17:50:49.124458 140474388072320 learning.py:507] global step 3613: loss = 0.9378 (1.316 sec/step)\n",
            "I0919 17:50:50.443577 140474388072320 learning.py:507] global step 3614: loss = 1.4395 (1.318 sec/step)\n",
            "I0919 17:50:51.773379 140474388072320 learning.py:507] global step 3615: loss = 0.8723 (1.328 sec/step)\n",
            "I0919 17:50:53.118503 140474388072320 learning.py:507] global step 3616: loss = 0.6841 (1.343 sec/step)\n",
            "I0919 17:50:54.458009 140474388072320 learning.py:507] global step 3617: loss = 0.9546 (1.338 sec/step)\n",
            "I0919 17:50:55.776509 140474388072320 learning.py:507] global step 3618: loss = 0.8780 (1.314 sec/step)\n",
            "I0919 17:50:57.070501 140474388072320 learning.py:507] global step 3619: loss = 0.8273 (1.289 sec/step)\n",
            "I0919 17:50:58.423237 140474388072320 learning.py:507] global step 3620: loss = 0.7034 (1.351 sec/step)\n",
            "I0919 17:50:59.748951 140474388072320 learning.py:507] global step 3621: loss = 1.0616 (1.324 sec/step)\n",
            "I0919 17:51:01.066478 140474388072320 learning.py:507] global step 3622: loss = 0.9294 (1.316 sec/step)\n",
            "I0919 17:51:02.422868 140474388072320 learning.py:507] global step 3623: loss = 0.8618 (1.355 sec/step)\n",
            "I0919 17:51:03.752811 140474388072320 learning.py:507] global step 3624: loss = 0.9167 (1.328 sec/step)\n",
            "I0919 17:51:05.038158 140474388072320 learning.py:507] global step 3625: loss = 1.0011 (1.283 sec/step)\n",
            "I0919 17:51:06.363415 140474388072320 learning.py:507] global step 3626: loss = 0.8676 (1.324 sec/step)\n",
            "I0919 17:51:07.681669 140474388072320 learning.py:507] global step 3627: loss = 0.8152 (1.317 sec/step)\n",
            "I0919 17:51:09.010200 140474388072320 learning.py:507] global step 3628: loss = 1.1676 (1.327 sec/step)\n",
            "I0919 17:51:10.331501 140474388072320 learning.py:507] global step 3629: loss = 0.9971 (1.319 sec/step)\n",
            "I0919 17:51:11.657314 140474388072320 learning.py:507] global step 3630: loss = 0.6099 (1.324 sec/step)\n",
            "I0919 17:51:12.973490 140474388072320 learning.py:507] global step 3631: loss = 0.7922 (1.314 sec/step)\n",
            "I0919 17:51:14.275928 140474388072320 learning.py:507] global step 3632: loss = 0.7757 (1.300 sec/step)\n",
            "I0919 17:51:15.595717 140474388072320 learning.py:507] global step 3633: loss = 0.8827 (1.318 sec/step)\n",
            "I0919 17:51:16.922833 140474388072320 learning.py:507] global step 3634: loss = 0.9789 (1.325 sec/step)\n",
            "I0919 17:51:18.237055 140474388072320 learning.py:507] global step 3635: loss = 0.6982 (1.312 sec/step)\n",
            "I0919 17:51:19.526103 140474388072320 learning.py:507] global step 3636: loss = 0.9882 (1.287 sec/step)\n",
            "I0919 17:51:20.799310 140474388072320 learning.py:507] global step 3637: loss = 0.7834 (1.271 sec/step)\n",
            "I0919 17:51:22.106503 140474388072320 learning.py:507] global step 3638: loss = 0.6597 (1.306 sec/step)\n",
            "I0919 17:51:23.388762 140474388072320 learning.py:507] global step 3639: loss = 0.9245 (1.280 sec/step)\n",
            "I0919 17:51:24.742357 140474388072320 learning.py:507] global step 3640: loss = 1.2109 (1.352 sec/step)\n",
            "I0919 17:51:26.100274 140474388072320 learning.py:507] global step 3641: loss = 0.8976 (1.356 sec/step)\n",
            "I0919 17:51:27.412185 140474388072320 learning.py:507] global step 3642: loss = 0.8190 (1.310 sec/step)\n",
            "I0919 17:51:28.711194 140474388072320 learning.py:507] global step 3643: loss = 0.6970 (1.297 sec/step)\n",
            "I0919 17:51:30.015634 140474388072320 learning.py:507] global step 3644: loss = 1.1378 (1.302 sec/step)\n",
            "I0919 17:51:31.358359 140474388072320 learning.py:507] global step 3645: loss = 0.6638 (1.341 sec/step)\n",
            "I0919 17:51:32.716461 140474388072320 learning.py:507] global step 3646: loss = 0.7338 (1.356 sec/step)\n",
            "I0919 17:51:34.035016 140474388072320 learning.py:507] global step 3647: loss = 0.7481 (1.317 sec/step)\n",
            "I0919 17:51:35.335284 140474388072320 learning.py:507] global step 3648: loss = 0.8991 (1.299 sec/step)\n",
            "I0919 17:51:36.623527 140474388072320 learning.py:507] global step 3649: loss = 0.8125 (1.287 sec/step)\n",
            "I0919 17:51:37.951311 140474388072320 learning.py:507] global step 3650: loss = 0.9231 (1.325 sec/step)\n",
            "I0919 17:51:39.275435 140474388072320 learning.py:507] global step 3651: loss = 0.8922 (1.321 sec/step)\n",
            "I0919 17:51:40.585498 140474388072320 learning.py:507] global step 3652: loss = 0.8586 (1.308 sec/step)\n",
            "I0919 17:51:41.888161 140474388072320 learning.py:507] global step 3653: loss = 0.7105 (1.301 sec/step)\n",
            "I0919 17:51:43.184623 140474388072320 learning.py:507] global step 3654: loss = 0.7171 (1.295 sec/step)\n",
            "I0919 17:51:44.474267 140474388072320 learning.py:507] global step 3655: loss = 0.8746 (1.288 sec/step)\n",
            "I0919 17:51:45.766628 140474388072320 learning.py:507] global step 3656: loss = 0.7132 (1.291 sec/step)\n",
            "I0919 17:51:47.076878 140474388072320 learning.py:507] global step 3657: loss = 1.0096 (1.309 sec/step)\n",
            "I0919 17:51:48.374459 140474388072320 learning.py:507] global step 3658: loss = 1.0702 (1.296 sec/step)\n",
            "I0919 17:51:49.701941 140474388072320 learning.py:507] global step 3659: loss = 1.0318 (1.325 sec/step)\n",
            "I0919 17:51:51.022746 140474388072320 learning.py:507] global step 3660: loss = 0.8892 (1.319 sec/step)\n",
            "I0919 17:51:52.335057 140474388072320 learning.py:507] global step 3661: loss = 0.8444 (1.311 sec/step)\n",
            "I0919 17:51:53.651242 140474388072320 learning.py:507] global step 3662: loss = 0.8266 (1.314 sec/step)\n",
            "I0919 17:51:54.965620 140474388072320 learning.py:507] global step 3663: loss = 0.7877 (1.312 sec/step)\n",
            "I0919 17:51:56.313024 140474388072320 learning.py:507] global step 3664: loss = 0.8804 (1.346 sec/step)\n",
            "I0919 17:51:57.620538 140474388072320 learning.py:507] global step 3665: loss = 1.1490 (1.306 sec/step)\n",
            "I0919 17:51:58.891082 140474388072320 learning.py:507] global step 3666: loss = 0.6326 (1.269 sec/step)\n",
            "I0919 17:52:00.211319 140474388072320 learning.py:507] global step 3667: loss = 1.3353 (1.318 sec/step)\n",
            "I0919 17:52:01.512872 140474388072320 learning.py:507] global step 3668: loss = 1.2014 (1.300 sec/step)\n",
            "I0919 17:52:02.810702 140474388072320 learning.py:507] global step 3669: loss = 0.9295 (1.296 sec/step)\n",
            "I0919 17:52:04.141077 140474388072320 learning.py:507] global step 3670: loss = 0.9000 (1.329 sec/step)\n",
            "I0919 17:52:05.477593 140474388072320 learning.py:507] global step 3671: loss = 0.9275 (1.335 sec/step)\n",
            "I0919 17:52:06.771816 140474388072320 learning.py:507] global step 3672: loss = 1.0766 (1.293 sec/step)\n",
            "I0919 17:52:08.083231 140474388072320 learning.py:507] global step 3673: loss = 0.8522 (1.310 sec/step)\n",
            "I0919 17:52:09.393059 140474388072320 learning.py:507] global step 3674: loss = 0.8680 (1.308 sec/step)\n",
            "I0919 17:52:10.683309 140474388072320 learning.py:507] global step 3675: loss = 0.9786 (1.288 sec/step)\n",
            "I0919 17:52:12.031804 140474388072320 learning.py:507] global step 3676: loss = 0.8827 (1.347 sec/step)\n",
            "I0919 17:52:13.334259 140474388072320 learning.py:507] global step 3677: loss = 1.2436 (1.301 sec/step)\n",
            "I0919 17:52:14.628190 140474388072320 learning.py:507] global step 3678: loss = 0.8858 (1.292 sec/step)\n",
            "I0919 17:52:15.951561 140474388072320 learning.py:507] global step 3679: loss = 0.9056 (1.321 sec/step)\n",
            "I0919 17:52:17.275314 140474388072320 learning.py:507] global step 3680: loss = 0.7524 (1.322 sec/step)\n",
            "I0919 17:52:18.595076 140474388072320 learning.py:507] global step 3681: loss = 0.9090 (1.318 sec/step)\n",
            "I0919 17:52:19.928947 140474388072320 learning.py:507] global step 3682: loss = 1.1382 (1.332 sec/step)\n",
            "I0919 17:52:21.218257 140474388072320 learning.py:507] global step 3683: loss = 0.8051 (1.288 sec/step)\n",
            "I0919 17:52:22.517261 140474388072320 learning.py:507] global step 3684: loss = 0.8450 (1.297 sec/step)\n",
            "I0919 17:52:23.860325 140474388072320 learning.py:507] global step 3685: loss = 0.9542 (1.341 sec/step)\n",
            "I0919 17:52:25.219855 140474388072320 learning.py:507] global step 3686: loss = 0.9896 (1.358 sec/step)\n",
            "I0919 17:52:26.544715 140474388072320 learning.py:507] global step 3687: loss = 1.1134 (1.323 sec/step)\n",
            "I0919 17:52:28.598620 140474388072320 learning.py:507] global step 3688: loss = 1.3623 (2.052 sec/step)\n",
            "I0919 17:52:28.627728 140471297337088 supervisor.py:1050] Recording summary at step 3688.\n",
            "I0919 17:52:29.945755 140474388072320 learning.py:507] global step 3689: loss = 0.8055 (1.345 sec/step)\n",
            "I0919 17:52:31.256841 140474388072320 learning.py:507] global step 3690: loss = 1.0509 (1.309 sec/step)\n",
            "I0919 17:52:32.610784 140474388072320 learning.py:507] global step 3691: loss = 0.9164 (1.352 sec/step)\n",
            "I0919 17:52:33.938627 140474388072320 learning.py:507] global step 3692: loss = 0.8341 (1.326 sec/step)\n",
            "I0919 17:52:35.243098 140474388072320 learning.py:507] global step 3693: loss = 0.9892 (1.303 sec/step)\n",
            "I0919 17:52:36.610637 140474388072320 learning.py:507] global step 3694: loss = 0.6485 (1.366 sec/step)\n",
            "I0919 17:52:37.904943 140474388072320 learning.py:507] global step 3695: loss = 0.8190 (1.292 sec/step)\n",
            "I0919 17:52:39.213875 140474388072320 learning.py:507] global step 3696: loss = 0.8221 (1.307 sec/step)\n",
            "I0919 17:52:40.527378 140474388072320 learning.py:507] global step 3697: loss = 0.8630 (1.312 sec/step)\n",
            "I0919 17:52:41.824352 140474388072320 learning.py:507] global step 3698: loss = 0.9112 (1.295 sec/step)\n",
            "I0919 17:52:43.160567 140474388072320 learning.py:507] global step 3699: loss = 0.9449 (1.334 sec/step)\n",
            "I0919 17:52:44.459294 140474388072320 learning.py:507] global step 3700: loss = 1.1737 (1.297 sec/step)\n",
            "I0919 17:52:45.776308 140474388072320 learning.py:507] global step 3701: loss = 0.9026 (1.315 sec/step)\n",
            "I0919 17:52:47.090140 140474388072320 learning.py:507] global step 3702: loss = 0.9074 (1.312 sec/step)\n",
            "I0919 17:52:48.425565 140474388072320 learning.py:507] global step 3703: loss = 1.1547 (1.334 sec/step)\n",
            "I0919 17:52:49.746915 140474388072320 learning.py:507] global step 3704: loss = 1.0846 (1.320 sec/step)\n",
            "I0919 17:52:51.040247 140474388072320 learning.py:507] global step 3705: loss = 0.6923 (1.292 sec/step)\n",
            "I0919 17:52:52.343693 140474388072320 learning.py:507] global step 3706: loss = 0.8282 (1.302 sec/step)\n",
            "I0919 17:52:53.678805 140474388072320 learning.py:507] global step 3707: loss = 0.8524 (1.333 sec/step)\n",
            "I0919 17:52:55.014930 140474388072320 learning.py:507] global step 3708: loss = 0.7531 (1.334 sec/step)\n",
            "I0919 17:52:56.350446 140474388072320 learning.py:507] global step 3709: loss = 0.8239 (1.334 sec/step)\n",
            "I0919 17:52:57.665980 140474388072320 learning.py:507] global step 3710: loss = 0.7984 (1.314 sec/step)\n",
            "I0919 17:52:58.963291 140474388072320 learning.py:507] global step 3711: loss = 0.9801 (1.296 sec/step)\n",
            "I0919 17:53:00.284414 140474388072320 learning.py:507] global step 3712: loss = 0.8516 (1.320 sec/step)\n",
            "I0919 17:53:01.622769 140474388072320 learning.py:507] global step 3713: loss = 0.8541 (1.337 sec/step)\n",
            "I0919 17:53:02.984206 140474388072320 learning.py:507] global step 3714: loss = 0.9079 (1.360 sec/step)\n",
            "I0919 17:53:04.280616 140474388072320 learning.py:507] global step 3715: loss = 0.8888 (1.295 sec/step)\n",
            "I0919 17:53:05.564261 140474388072320 learning.py:507] global step 3716: loss = 0.7768 (1.282 sec/step)\n",
            "I0919 17:53:06.908584 140474388072320 learning.py:507] global step 3717: loss = 0.7449 (1.343 sec/step)\n",
            "I0919 17:53:08.274945 140474388072320 learning.py:507] global step 3718: loss = 0.7923 (1.365 sec/step)\n",
            "I0919 17:53:09.584099 140474388072320 learning.py:507] global step 3719: loss = 0.9331 (1.307 sec/step)\n",
            "I0919 17:53:10.947078 140474388072320 learning.py:507] global step 3720: loss = 0.8817 (1.361 sec/step)\n",
            "I0919 17:53:12.255819 140474388072320 learning.py:507] global step 3721: loss = 0.7643 (1.307 sec/step)\n",
            "I0919 17:53:13.567452 140474388072320 learning.py:507] global step 3722: loss = 1.0673 (1.310 sec/step)\n",
            "I0919 17:53:14.873014 140474388072320 learning.py:507] global step 3723: loss = 0.8287 (1.304 sec/step)\n",
            "I0919 17:53:16.214642 140474388072320 learning.py:507] global step 3724: loss = 0.6269 (1.340 sec/step)\n",
            "I0919 17:53:17.527417 140474388072320 learning.py:507] global step 3725: loss = 0.8063 (1.311 sec/step)\n",
            "I0919 17:53:18.831376 140474388072320 learning.py:507] global step 3726: loss = 0.8484 (1.302 sec/step)\n",
            "I0919 17:53:20.120496 140474388072320 learning.py:507] global step 3727: loss = 0.6991 (1.287 sec/step)\n",
            "I0919 17:53:21.446450 140474388072320 learning.py:507] global step 3728: loss = 0.9516 (1.324 sec/step)\n",
            "I0919 17:53:22.757694 140474388072320 learning.py:507] global step 3729: loss = 0.9879 (1.310 sec/step)\n",
            "I0919 17:53:24.093074 140474388072320 learning.py:507] global step 3730: loss = 0.7272 (1.334 sec/step)\n",
            "I0919 17:53:25.409532 140474388072320 learning.py:507] global step 3731: loss = 0.8864 (1.315 sec/step)\n",
            "I0919 17:53:26.716737 140474388072320 learning.py:507] global step 3732: loss = 0.8452 (1.305 sec/step)\n",
            "I0919 17:53:28.042268 140474388072320 learning.py:507] global step 3733: loss = 0.9100 (1.324 sec/step)\n",
            "I0919 17:53:29.355866 140474388072320 learning.py:507] global step 3734: loss = 0.8083 (1.312 sec/step)\n",
            "I0919 17:53:30.715239 140474388072320 learning.py:507] global step 3735: loss = 0.7790 (1.358 sec/step)\n",
            "I0919 17:53:32.048333 140474388072320 learning.py:507] global step 3736: loss = 0.9184 (1.331 sec/step)\n",
            "I0919 17:53:33.346998 140474388072320 learning.py:507] global step 3737: loss = 0.9676 (1.297 sec/step)\n",
            "I0919 17:53:34.670542 140474388072320 learning.py:507] global step 3738: loss = 0.8101 (1.322 sec/step)\n",
            "I0919 17:53:36.007230 140474388072320 learning.py:507] global step 3739: loss = 0.9174 (1.335 sec/step)\n",
            "I0919 17:53:37.328349 140474388072320 learning.py:507] global step 3740: loss = 0.8707 (1.319 sec/step)\n",
            "I0919 17:53:38.624670 140474388072320 learning.py:507] global step 3741: loss = 0.8733 (1.295 sec/step)\n",
            "I0919 17:53:39.983171 140474388072320 learning.py:507] global step 3742: loss = 0.9498 (1.356 sec/step)\n",
            "I0919 17:53:41.311815 140474388072320 learning.py:507] global step 3743: loss = 0.8108 (1.327 sec/step)\n",
            "I0919 17:53:42.596662 140474388072320 learning.py:507] global step 3744: loss = 0.7061 (1.283 sec/step)\n",
            "I0919 17:53:43.911451 140474388072320 learning.py:507] global step 3745: loss = 1.0318 (1.313 sec/step)\n",
            "I0919 17:53:45.214154 140474388072320 learning.py:507] global step 3746: loss = 0.6967 (1.301 sec/step)\n",
            "I0919 17:53:46.555466 140474388072320 learning.py:507] global step 3747: loss = 0.9218 (1.340 sec/step)\n",
            "I0919 17:53:47.863215 140474388072320 learning.py:507] global step 3748: loss = 0.8049 (1.306 sec/step)\n",
            "I0919 17:53:49.166835 140474388072320 learning.py:507] global step 3749: loss = 0.7848 (1.302 sec/step)\n",
            "I0919 17:53:50.453056 140474388072320 learning.py:507] global step 3750: loss = 0.6669 (1.284 sec/step)\n",
            "I0919 17:53:51.770576 140474388072320 learning.py:507] global step 3751: loss = 0.8225 (1.316 sec/step)\n",
            "I0919 17:53:53.135636 140474388072320 learning.py:507] global step 3752: loss = 1.2996 (1.363 sec/step)\n",
            "I0919 17:53:54.484183 140474388072320 learning.py:507] global step 3753: loss = 0.8239 (1.347 sec/step)\n",
            "I0919 17:53:55.848608 140474388072320 learning.py:507] global step 3754: loss = 0.9539 (1.363 sec/step)\n",
            "I0919 17:53:57.164658 140474388072320 learning.py:507] global step 3755: loss = 0.9485 (1.314 sec/step)\n",
            "I0919 17:53:58.469500 140474388072320 learning.py:507] global step 3756: loss = 1.0236 (1.303 sec/step)\n",
            "I0919 17:53:59.781651 140474388072320 learning.py:507] global step 3757: loss = 0.7581 (1.310 sec/step)\n",
            "I0919 17:54:01.077682 140474388072320 learning.py:507] global step 3758: loss = 0.7867 (1.294 sec/step)\n",
            "I0919 17:54:02.380979 140474388072320 learning.py:507] global step 3759: loss = 0.8286 (1.301 sec/step)\n",
            "I0919 17:54:03.699950 140474388072320 learning.py:507] global step 3760: loss = 1.0087 (1.317 sec/step)\n",
            "I0919 17:54:05.047152 140474388072320 learning.py:507] global step 3761: loss = 0.9449 (1.346 sec/step)\n",
            "I0919 17:54:06.336946 140474388072320 learning.py:507] global step 3762: loss = 0.7769 (1.288 sec/step)\n",
            "I0919 17:54:07.715970 140474388072320 learning.py:507] global step 3763: loss = 0.7465 (1.377 sec/step)\n",
            "I0919 17:54:09.018573 140474388072320 learning.py:507] global step 3764: loss = 1.2482 (1.301 sec/step)\n",
            "I0919 17:54:10.335200 140474388072320 learning.py:507] global step 3765: loss = 1.0676 (1.315 sec/step)\n",
            "I0919 17:54:11.606980 140474388072320 learning.py:507] global step 3766: loss = 1.0288 (1.270 sec/step)\n",
            "I0919 17:54:12.921900 140474388072320 learning.py:507] global step 3767: loss = 0.6514 (1.313 sec/step)\n",
            "I0919 17:54:14.249448 140474388072320 learning.py:507] global step 3768: loss = 1.0932 (1.325 sec/step)\n",
            "I0919 17:54:15.562860 140474388072320 learning.py:507] global step 3769: loss = 0.8650 (1.312 sec/step)\n",
            "I0919 17:54:16.859843 140474388072320 learning.py:507] global step 3770: loss = 0.6594 (1.295 sec/step)\n",
            "I0919 17:54:18.156311 140474388072320 learning.py:507] global step 3771: loss = 0.8628 (1.295 sec/step)\n",
            "I0919 17:54:19.456083 140474388072320 learning.py:507] global step 3772: loss = 0.7042 (1.298 sec/step)\n",
            "I0919 17:54:20.757493 140474388072320 learning.py:507] global step 3773: loss = 0.8687 (1.299 sec/step)\n",
            "I0919 17:54:22.086293 140474388072320 learning.py:507] global step 3774: loss = 0.8730 (1.327 sec/step)\n",
            "I0919 17:54:23.395617 140474388072320 learning.py:507] global step 3775: loss = 0.7620 (1.308 sec/step)\n",
            "I0919 17:54:24.699785 140474388072320 learning.py:507] global step 3776: loss = 0.7061 (1.302 sec/step)\n",
            "I0919 17:54:25.978537 140474388072320 learning.py:507] global step 3777: loss = 0.9857 (1.277 sec/step)\n",
            "I0919 17:54:27.497178 140474388072320 learning.py:507] global step 3778: loss = 0.9314 (1.408 sec/step)\n",
            "I0919 17:54:28.428379 140471297337088 supervisor.py:1050] Recording summary at step 3778.\n",
            "I0919 17:54:29.461610 140474388072320 learning.py:507] global step 3779: loss = 0.9981 (1.920 sec/step)\n",
            "I0919 17:54:30.783725 140474388072320 learning.py:507] global step 3780: loss = 0.8845 (1.320 sec/step)\n",
            "I0919 17:54:32.082916 140474388072320 learning.py:507] global step 3781: loss = 0.8010 (1.296 sec/step)\n",
            "I0919 17:54:33.393032 140474388072320 learning.py:507] global step 3782: loss = 0.7973 (1.309 sec/step)\n",
            "I0919 17:54:34.718211 140474388072320 learning.py:507] global step 3783: loss = 0.6636 (1.323 sec/step)\n",
            "I0919 17:54:36.012298 140474388072320 learning.py:507] global step 3784: loss = 0.8628 (1.292 sec/step)\n",
            "I0919 17:54:37.328789 140474388072320 learning.py:507] global step 3785: loss = 0.8263 (1.315 sec/step)\n",
            "I0919 17:54:38.647403 140474388072320 learning.py:507] global step 3786: loss = 0.8331 (1.317 sec/step)\n",
            "I0919 17:54:39.960685 140474388072320 learning.py:507] global step 3787: loss = 1.1366 (1.311 sec/step)\n",
            "I0919 17:54:41.257740 140474388072320 learning.py:507] global step 3788: loss = 0.8097 (1.295 sec/step)\n",
            "I0919 17:54:42.557210 140474388072320 learning.py:507] global step 3789: loss = 0.7907 (1.298 sec/step)\n",
            "I0919 17:54:43.900371 140474388072320 learning.py:507] global step 3790: loss = 0.8586 (1.342 sec/step)\n",
            "I0919 17:54:45.196242 140474388072320 learning.py:507] global step 3791: loss = 0.9491 (1.294 sec/step)\n",
            "I0919 17:54:46.467225 140474388072320 learning.py:507] global step 3792: loss = 1.0879 (1.269 sec/step)\n",
            "I0919 17:54:47.772655 140474388072320 learning.py:507] global step 3793: loss = 0.7947 (1.304 sec/step)\n",
            "I0919 17:54:49.105683 140474388072320 learning.py:507] global step 3794: loss = 0.7479 (1.331 sec/step)\n",
            "I0919 17:54:50.430407 140474388072320 learning.py:507] global step 3795: loss = 0.9226 (1.323 sec/step)\n",
            "I0919 17:54:51.765103 140474388072320 learning.py:507] global step 3796: loss = 0.7264 (1.333 sec/step)\n",
            "I0919 17:54:53.067173 140474388072320 learning.py:507] global step 3797: loss = 0.9396 (1.299 sec/step)\n",
            "I0919 17:54:54.369369 140474388072320 learning.py:507] global step 3798: loss = 0.9704 (1.301 sec/step)\n",
            "I0919 17:54:55.658828 140474388072320 learning.py:507] global step 3799: loss = 0.9018 (1.288 sec/step)\n",
            "I0919 17:54:56.965010 140474388072320 learning.py:507] global step 3800: loss = 0.6974 (1.304 sec/step)\n",
            "I0919 17:54:58.288785 140474388072320 learning.py:507] global step 3801: loss = 0.7415 (1.322 sec/step)\n",
            "I0919 17:54:59.604928 140474388072320 learning.py:507] global step 3802: loss = 0.9304 (1.314 sec/step)\n",
            "I0919 17:55:00.946048 140474388072320 learning.py:507] global step 3803: loss = 1.0104 (1.339 sec/step)\n",
            "I0919 17:55:02.261058 140474388072320 learning.py:507] global step 3804: loss = 2.1023 (1.313 sec/step)\n",
            "I0919 17:55:03.627830 140474388072320 learning.py:507] global step 3805: loss = 0.8902 (1.365 sec/step)\n",
            "I0919 17:55:04.909087 140474388072320 learning.py:507] global step 3806: loss = 1.1206 (1.279 sec/step)\n",
            "I0919 17:55:06.187304 140474388072320 learning.py:507] global step 3807: loss = 0.9208 (1.277 sec/step)\n",
            "I0919 17:55:07.494187 140474388072320 learning.py:507] global step 3808: loss = 0.8369 (1.305 sec/step)\n",
            "I0919 17:55:08.792460 140474388072320 learning.py:507] global step 3809: loss = 0.8334 (1.297 sec/step)\n",
            "I0919 17:55:10.107388 140474388072320 learning.py:507] global step 3810: loss = 0.9530 (1.313 sec/step)\n",
            "I0919 17:55:11.416387 140474388072320 learning.py:507] global step 3811: loss = 0.8385 (1.307 sec/step)\n",
            "I0919 17:55:12.762271 140474388072320 learning.py:507] global step 3812: loss = 0.8921 (1.344 sec/step)\n",
            "I0919 17:55:14.082937 140474388072320 learning.py:507] global step 3813: loss = 0.8648 (1.319 sec/step)\n",
            "I0919 17:55:15.367223 140474388072320 learning.py:507] global step 3814: loss = 1.0187 (1.283 sec/step)\n",
            "I0919 17:55:16.692689 140474388072320 learning.py:507] global step 3815: loss = 0.7724 (1.324 sec/step)\n",
            "I0919 17:55:17.991074 140474388072320 learning.py:507] global step 3816: loss = 1.1464 (1.297 sec/step)\n",
            "I0919 17:55:19.274264 140474388072320 learning.py:507] global step 3817: loss = 0.8462 (1.281 sec/step)\n",
            "I0919 17:55:20.568768 140474388072320 learning.py:507] global step 3818: loss = 0.8848 (1.293 sec/step)\n",
            "I0919 17:55:21.927872 140474388072320 learning.py:507] global step 3819: loss = 0.8453 (1.357 sec/step)\n",
            "I0919 17:55:23.204751 140474388072320 learning.py:507] global step 3820: loss = 0.8084 (1.275 sec/step)\n",
            "I0919 17:55:24.532164 140474388072320 learning.py:507] global step 3821: loss = 1.1783 (1.326 sec/step)\n",
            "I0919 17:55:25.878582 140474388072320 learning.py:507] global step 3822: loss = 0.6893 (1.344 sec/step)\n",
            "I0919 17:55:27.181180 140474388072320 learning.py:507] global step 3823: loss = 0.8838 (1.301 sec/step)\n",
            "I0919 17:55:28.488095 140474388072320 learning.py:507] global step 3824: loss = 1.5417 (1.305 sec/step)\n",
            "I0919 17:55:29.792154 140474388072320 learning.py:507] global step 3825: loss = 1.0093 (1.302 sec/step)\n",
            "I0919 17:55:31.100594 140474388072320 learning.py:507] global step 3826: loss = 1.0909 (1.307 sec/step)\n",
            "I0919 17:55:32.474713 140474388072320 learning.py:507] global step 3827: loss = 0.8575 (1.372 sec/step)\n",
            "I0919 17:55:33.778851 140474388072320 learning.py:507] global step 3828: loss = 0.8761 (1.302 sec/step)\n",
            "I0919 17:55:35.110726 140474388072320 learning.py:507] global step 3829: loss = 0.8040 (1.330 sec/step)\n",
            "I0919 17:55:36.393958 140474388072320 learning.py:507] global step 3830: loss = 0.6999 (1.282 sec/step)\n",
            "I0919 17:55:37.667161 140474388072320 learning.py:507] global step 3831: loss = 0.9934 (1.272 sec/step)\n",
            "I0919 17:55:38.968772 140474388072320 learning.py:507] global step 3832: loss = 1.1234 (1.300 sec/step)\n",
            "I0919 17:55:40.264069 140474388072320 learning.py:507] global step 3833: loss = 1.1785 (1.294 sec/step)\n",
            "I0919 17:55:41.614200 140474388072320 learning.py:507] global step 3834: loss = 0.8430 (1.348 sec/step)\n",
            "I0919 17:55:42.977541 140474388072320 learning.py:507] global step 3835: loss = 0.7511 (1.362 sec/step)\n",
            "I0919 17:55:44.290687 140474388072320 learning.py:507] global step 3836: loss = 1.0633 (1.311 sec/step)\n",
            "I0919 17:55:45.615977 140474388072320 learning.py:507] global step 3837: loss = 1.0315 (1.323 sec/step)\n",
            "I0919 17:55:46.945959 140474388072320 learning.py:507] global step 3838: loss = 0.9897 (1.328 sec/step)\n",
            "I0919 17:55:48.262995 140474388072320 learning.py:507] global step 3839: loss = 0.8792 (1.315 sec/step)\n",
            "I0919 17:55:49.574284 140474388072320 learning.py:507] global step 3840: loss = 0.8070 (1.310 sec/step)\n",
            "I0919 17:55:50.925200 140474388072320 learning.py:507] global step 3841: loss = 0.7783 (1.349 sec/step)\n",
            "I0919 17:55:52.266529 140474388072320 learning.py:507] global step 3842: loss = 0.7403 (1.339 sec/step)\n",
            "I0919 17:55:53.617530 140474388072320 learning.py:507] global step 3843: loss = 0.8342 (1.349 sec/step)\n",
            "I0919 17:55:54.944298 140474388072320 learning.py:507] global step 3844: loss = 1.2096 (1.325 sec/step)\n",
            "I0919 17:55:56.275547 140474388072320 learning.py:507] global step 3845: loss = 0.7837 (1.330 sec/step)\n",
            "I0919 17:55:57.539722 140474388072320 learning.py:507] global step 3846: loss = 0.7570 (1.262 sec/step)\n",
            "I0919 17:55:58.854701 140474388072320 learning.py:507] global step 3847: loss = 1.1848 (1.313 sec/step)\n",
            "I0919 17:56:00.162269 140474388072320 learning.py:507] global step 3848: loss = 0.8290 (1.306 sec/step)\n",
            "I0919 17:56:01.465367 140474388072320 learning.py:507] global step 3849: loss = 0.7733 (1.301 sec/step)\n",
            "I0919 17:56:02.778211 140474388072320 learning.py:507] global step 3850: loss = 0.9115 (1.311 sec/step)\n",
            "I0919 17:56:04.085416 140474388072320 learning.py:507] global step 3851: loss = 0.9163 (1.306 sec/step)\n",
            "I0919 17:56:05.395761 140474388072320 learning.py:507] global step 3852: loss = 0.8960 (1.309 sec/step)\n",
            "I0919 17:56:06.692059 140474388072320 learning.py:507] global step 3853: loss = 0.7809 (1.295 sec/step)\n",
            "I0919 17:56:08.013287 140474388072320 learning.py:507] global step 3854: loss = 0.7370 (1.319 sec/step)\n",
            "I0919 17:56:09.311687 140474388072320 learning.py:507] global step 3855: loss = 0.8390 (1.296 sec/step)\n",
            "I0919 17:56:10.666944 140474388072320 learning.py:507] global step 3856: loss = 0.8692 (1.353 sec/step)\n",
            "I0919 17:56:11.977636 140474388072320 learning.py:507] global step 3857: loss = 1.0653 (1.309 sec/step)\n",
            "I0919 17:56:13.270065 140474388072320 learning.py:507] global step 3858: loss = 1.0526 (1.291 sec/step)\n",
            "I0919 17:56:14.566359 140474388072320 learning.py:507] global step 3859: loss = 0.8195 (1.294 sec/step)\n",
            "I0919 17:56:15.866095 140474388072320 learning.py:507] global step 3860: loss = 0.7866 (1.298 sec/step)\n",
            "I0919 17:56:17.190948 140474388072320 learning.py:507] global step 3861: loss = 0.9471 (1.323 sec/step)\n",
            "I0919 17:56:18.534248 140474388072320 learning.py:507] global step 3862: loss = 1.0944 (1.342 sec/step)\n",
            "I0919 17:56:19.875982 140474388072320 learning.py:507] global step 3863: loss = 0.8472 (1.340 sec/step)\n",
            "I0919 17:56:21.198440 140474388072320 learning.py:507] global step 3864: loss = 1.1564 (1.321 sec/step)\n",
            "I0919 17:56:22.514298 140474388072320 learning.py:507] global step 3865: loss = 1.0955 (1.314 sec/step)\n",
            "I0919 17:56:23.798310 140474388072320 learning.py:507] global step 3866: loss = 0.8142 (1.282 sec/step)\n",
            "I0919 17:56:25.125943 140474388072320 learning.py:507] global step 3867: loss = 0.8206 (1.326 sec/step)\n",
            "I0919 17:56:26.437685 140474388072320 learning.py:507] global step 3868: loss = 0.9323 (1.310 sec/step)\n",
            "I0919 17:56:27.947372 140474388072320 learning.py:507] global step 3869: loss = 0.6390 (1.453 sec/step)\n",
            "I0919 17:56:29.551058 140471297337088 supervisor.py:1050] Recording summary at step 3869.\n",
            "I0919 17:56:29.990745 140474388072320 learning.py:507] global step 3870: loss = 0.8589 (2.040 sec/step)\n",
            "I0919 17:56:31.311922 140474388072320 learning.py:507] global step 3871: loss = 0.9417 (1.320 sec/step)\n",
            "I0919 17:56:32.662266 140474388072320 learning.py:507] global step 3872: loss = 0.7968 (1.348 sec/step)\n",
            "I0919 17:56:34.007252 140474388072320 learning.py:507] global step 3873: loss = 0.8991 (1.343 sec/step)\n",
            "I0919 17:56:35.309670 140474388072320 learning.py:507] global step 3874: loss = 0.7194 (1.301 sec/step)\n",
            "I0919 17:56:36.626645 140474388072320 learning.py:507] global step 3875: loss = 1.2751 (1.315 sec/step)\n",
            "I0919 17:56:37.993272 140474388072320 learning.py:507] global step 3876: loss = 0.7224 (1.365 sec/step)\n",
            "I0919 17:56:39.297765 140474388072320 learning.py:507] global step 3877: loss = 1.0844 (1.303 sec/step)\n",
            "I0919 17:56:40.627623 140474388072320 learning.py:507] global step 3878: loss = 0.9261 (1.328 sec/step)\n",
            "I0919 17:56:41.940359 140474388072320 learning.py:507] global step 3879: loss = 0.7027 (1.311 sec/step)\n",
            "I0919 17:56:43.232064 140474388072320 learning.py:507] global step 3880: loss = 0.9828 (1.290 sec/step)\n",
            "I0919 17:56:44.546078 140474388072320 learning.py:507] global step 3881: loss = 0.9558 (1.312 sec/step)\n",
            "I0919 17:56:45.825784 140474388072320 learning.py:507] global step 3882: loss = 0.8694 (1.278 sec/step)\n",
            "I0919 17:56:47.156757 140474388072320 learning.py:507] global step 3883: loss = 0.7995 (1.329 sec/step)\n",
            "I0919 17:56:48.466927 140474388072320 learning.py:507] global step 3884: loss = 0.9073 (1.309 sec/step)\n",
            "I0919 17:56:49.800631 140474388072320 learning.py:507] global step 3885: loss = 0.8369 (1.332 sec/step)\n",
            "I0919 17:56:51.113215 140474388072320 learning.py:507] global step 3886: loss = 0.9696 (1.311 sec/step)\n",
            "I0919 17:56:52.432076 140474388072320 learning.py:507] global step 3887: loss = 0.7934 (1.317 sec/step)\n",
            "I0919 17:56:53.719845 140474388072320 learning.py:507] global step 3888: loss = 0.7100 (1.286 sec/step)\n",
            "I0919 17:56:55.018946 140474388072320 learning.py:507] global step 3889: loss = 0.8417 (1.297 sec/step)\n",
            "I0919 17:56:56.331557 140474388072320 learning.py:507] global step 3890: loss = 0.8555 (1.311 sec/step)\n",
            "I0919 17:56:57.692930 140474388072320 learning.py:507] global step 3891: loss = 0.8101 (1.359 sec/step)\n",
            "I0919 17:56:59.031070 140474388072320 learning.py:507] global step 3892: loss = 0.7816 (1.336 sec/step)\n",
            "I0919 17:57:00.371514 140474388072320 learning.py:507] global step 3893: loss = 0.9276 (1.339 sec/step)\n",
            "I0919 17:57:01.677649 140474388072320 learning.py:507] global step 3894: loss = 0.9718 (1.304 sec/step)\n",
            "I0919 17:57:02.981719 140474388072320 learning.py:507] global step 3895: loss = 0.9876 (1.302 sec/step)\n",
            "I0919 17:57:04.282005 140474388072320 learning.py:507] global step 3896: loss = 0.9016 (1.298 sec/step)\n",
            "I0919 17:57:05.584611 140474388072320 learning.py:507] global step 3897: loss = 0.8755 (1.301 sec/step)\n",
            "I0919 17:57:06.885282 140474388072320 learning.py:507] global step 3898: loss = 1.2430 (1.299 sec/step)\n",
            "I0919 17:57:08.230376 140474388072320 learning.py:507] global step 3899: loss = 0.7893 (1.341 sec/step)\n",
            "I0919 17:57:09.543701 140474388072320 learning.py:507] global step 3900: loss = 0.9138 (1.311 sec/step)\n",
            "I0919 17:57:10.873538 140474388072320 learning.py:507] global step 3901: loss = 1.2489 (1.328 sec/step)\n",
            "I0919 17:57:12.189564 140474388072320 learning.py:507] global step 3902: loss = 1.3684 (1.314 sec/step)\n",
            "I0919 17:57:13.498016 140474388072320 learning.py:507] global step 3903: loss = 0.9552 (1.307 sec/step)\n",
            "I0919 17:57:14.775922 140474388072320 learning.py:507] global step 3904: loss = 0.8973 (1.276 sec/step)\n",
            "I0919 17:57:16.115195 140474388072320 learning.py:507] global step 3905: loss = 1.0282 (1.338 sec/step)\n",
            "I0919 17:57:17.464710 140474388072320 learning.py:507] global step 3906: loss = 0.8529 (1.347 sec/step)\n",
            "I0919 17:57:18.846273 140474388072320 learning.py:507] global step 3907: loss = 0.7114 (1.375 sec/step)\n",
            "I0919 17:57:20.133526 140474388072320 learning.py:507] global step 3908: loss = 0.8202 (1.285 sec/step)\n",
            "I0919 17:57:21.416246 140474388072320 learning.py:507] global step 3909: loss = 0.8824 (1.281 sec/step)\n",
            "I0919 17:57:22.713227 140474388072320 learning.py:507] global step 3910: loss = 0.7709 (1.295 sec/step)\n",
            "I0919 17:57:24.054253 140474388072320 learning.py:507] global step 3911: loss = 0.8907 (1.339 sec/step)\n",
            "I0919 17:57:25.372004 140474388072320 learning.py:507] global step 3912: loss = 0.8584 (1.316 sec/step)\n",
            "I0919 17:57:26.694344 140474388072320 learning.py:507] global step 3913: loss = 0.9673 (1.321 sec/step)\n",
            "I0919 17:57:27.989764 140474388072320 learning.py:507] global step 3914: loss = 1.1472 (1.294 sec/step)\n",
            "I0919 17:57:29.286925 140474388072320 learning.py:507] global step 3915: loss = 0.8939 (1.296 sec/step)\n",
            "I0919 17:57:30.574986 140474388072320 learning.py:507] global step 3916: loss = 0.9722 (1.287 sec/step)\n",
            "I0919 17:57:31.850373 140474388072320 learning.py:507] global step 3917: loss = 0.9055 (1.274 sec/step)\n",
            "I0919 17:57:33.185616 140474388072320 learning.py:507] global step 3918: loss = 0.6908 (1.334 sec/step)\n",
            "I0919 17:57:34.500313 140474388072320 learning.py:507] global step 3919: loss = 0.8263 (1.313 sec/step)\n",
            "I0919 17:57:35.827166 140474388072320 learning.py:507] global step 3920: loss = 0.9099 (1.325 sec/step)\n",
            "I0919 17:57:37.144469 140474388072320 learning.py:507] global step 3921: loss = 0.8458 (1.316 sec/step)\n",
            "I0919 17:57:38.447626 140474388072320 learning.py:507] global step 3922: loss = 1.2764 (1.301 sec/step)\n",
            "I0919 17:57:39.731657 140474388072320 learning.py:507] global step 3923: loss = 1.1792 (1.282 sec/step)\n",
            "I0919 17:57:41.048459 140474388072320 learning.py:507] global step 3924: loss = 0.8719 (1.315 sec/step)\n",
            "I0919 17:57:42.334048 140474388072320 learning.py:507] global step 3925: loss = 0.8199 (1.284 sec/step)\n",
            "I0919 17:57:43.615923 140474388072320 learning.py:507] global step 3926: loss = 0.7953 (1.280 sec/step)\n",
            "I0919 17:57:44.945482 140474388072320 learning.py:507] global step 3927: loss = 0.8057 (1.328 sec/step)\n",
            "I0919 17:57:46.266333 140474388072320 learning.py:507] global step 3928: loss = 0.6692 (1.319 sec/step)\n",
            "I0919 17:57:47.566586 140474388072320 learning.py:507] global step 3929: loss = 0.9278 (1.299 sec/step)\n",
            "I0919 17:57:48.895735 140474388072320 learning.py:507] global step 3930: loss = 0.8135 (1.327 sec/step)\n",
            "I0919 17:57:50.212816 140474388072320 learning.py:507] global step 3931: loss = 0.9428 (1.315 sec/step)\n",
            "I0919 17:57:51.552336 140474388072320 learning.py:507] global step 3932: loss = 1.4984 (1.338 sec/step)\n",
            "I0919 17:57:52.912614 140474388072320 learning.py:507] global step 3933: loss = 0.6866 (1.359 sec/step)\n",
            "I0919 17:57:54.239388 140474388072320 learning.py:507] global step 3934: loss = 0.8360 (1.325 sec/step)\n",
            "I0919 17:57:55.532401 140474388072320 learning.py:507] global step 3935: loss = 1.2070 (1.291 sec/step)\n",
            "I0919 17:57:56.835385 140474388072320 learning.py:507] global step 3936: loss = 0.8468 (1.301 sec/step)\n",
            "I0919 17:57:58.152164 140474388072320 learning.py:507] global step 3937: loss = 0.6504 (1.315 sec/step)\n",
            "I0919 17:57:59.437810 140474388072320 learning.py:507] global step 3938: loss = 0.7799 (1.284 sec/step)\n",
            "I0919 17:58:00.791564 140474388072320 learning.py:507] global step 3939: loss = 0.7595 (1.352 sec/step)\n",
            "I0919 17:58:02.094418 140474388072320 learning.py:507] global step 3940: loss = 0.8091 (1.301 sec/step)\n",
            "I0919 17:58:03.415389 140474388072320 learning.py:507] global step 3941: loss = 1.0876 (1.319 sec/step)\n",
            "I0919 17:58:04.729515 140474388072320 learning.py:507] global step 3942: loss = 1.0041 (1.312 sec/step)\n",
            "I0919 17:58:06.039701 140474388072320 learning.py:507] global step 3943: loss = 1.0119 (1.308 sec/step)\n",
            "I0919 17:58:07.322523 140474388072320 learning.py:507] global step 3944: loss = 0.8504 (1.281 sec/step)\n",
            "I0919 17:58:08.654625 140474388072320 learning.py:507] global step 3945: loss = 0.7633 (1.330 sec/step)\n",
            "I0919 17:58:09.966012 140474388072320 learning.py:507] global step 3946: loss = 0.8099 (1.310 sec/step)\n",
            "I0919 17:58:11.236913 140474388072320 learning.py:507] global step 3947: loss = 0.8606 (1.269 sec/step)\n",
            "I0919 17:58:12.539191 140474388072320 learning.py:507] global step 3948: loss = 1.0576 (1.301 sec/step)\n",
            "I0919 17:58:13.826215 140474388072320 learning.py:507] global step 3949: loss = 0.8894 (1.285 sec/step)\n",
            "I0919 17:58:15.138883 140474388072320 learning.py:507] global step 3950: loss = 0.7408 (1.311 sec/step)\n",
            "I0919 17:58:16.399914 140474388072320 learning.py:507] global step 3951: loss = 1.1091 (1.260 sec/step)\n",
            "I0919 17:58:17.672585 140474388072320 learning.py:507] global step 3952: loss = 0.8500 (1.271 sec/step)\n",
            "I0919 17:58:18.999167 140474388072320 learning.py:507] global step 3953: loss = 0.7923 (1.325 sec/step)\n",
            "I0919 17:58:20.316239 140474388072320 learning.py:507] global step 3954: loss = 1.2144 (1.315 sec/step)\n",
            "I0919 17:58:21.593755 140474388072320 learning.py:507] global step 3955: loss = 0.9271 (1.276 sec/step)\n",
            "I0919 17:58:22.929378 140474388072320 learning.py:507] global step 3956: loss = 1.1650 (1.334 sec/step)\n",
            "I0919 17:58:24.235769 140474388072320 learning.py:507] global step 3957: loss = 0.9670 (1.305 sec/step)\n",
            "I0919 17:58:25.528095 140474388072320 learning.py:507] global step 3958: loss = 1.2269 (1.290 sec/step)\n",
            "I0919 17:58:26.832828 140474388072320 learning.py:507] global step 3959: loss = 0.9203 (1.303 sec/step)\n",
            "I0919 17:58:28.734731 140471297337088 supervisor.py:1050] Recording summary at step 3959.\n",
            "I0919 17:58:29.080075 140474388072320 learning.py:507] global step 3960: loss = 0.7753 (2.246 sec/step)\n",
            "I0919 17:58:30.440896 140474388072320 learning.py:507] global step 3961: loss = 0.7575 (1.359 sec/step)\n",
            "I0919 17:58:31.768023 140474388072320 learning.py:507] global step 3962: loss = 0.9879 (1.325 sec/step)\n",
            "I0919 17:58:33.140173 140474388072320 learning.py:507] global step 3963: loss = 1.0584 (1.370 sec/step)\n",
            "I0919 17:58:34.450706 140474388072320 learning.py:507] global step 3964: loss = 0.8210 (1.309 sec/step)\n",
            "I0919 17:58:35.822252 140474388072320 learning.py:507] global step 3965: loss = 0.9291 (1.370 sec/step)\n",
            "I0919 17:58:37.133220 140474388072320 learning.py:507] global step 3966: loss = 0.7562 (1.309 sec/step)\n",
            "I0919 17:58:38.439737 140474388072320 learning.py:507] global step 3967: loss = 0.8235 (1.305 sec/step)\n",
            "I0919 17:58:39.743972 140474388072320 learning.py:507] global step 3968: loss = 0.8635 (1.303 sec/step)\n",
            "I0919 17:58:41.054814 140474388072320 learning.py:507] global step 3969: loss = 0.9257 (1.309 sec/step)\n",
            "I0919 17:58:42.361661 140474388072320 learning.py:507] global step 3970: loss = 0.9493 (1.305 sec/step)\n",
            "I0919 17:58:43.680139 140474388072320 learning.py:507] global step 3971: loss = 0.9129 (1.317 sec/step)\n",
            "I0919 17:58:44.978215 140474388072320 learning.py:507] global step 3972: loss = 0.7306 (1.296 sec/step)\n",
            "I0919 17:58:46.290222 140474388072320 learning.py:507] global step 3973: loss = 0.7938 (1.310 sec/step)\n",
            "I0919 17:58:47.597744 140474388072320 learning.py:507] global step 3974: loss = 0.8272 (1.306 sec/step)\n",
            "I0919 17:58:48.899723 140474388072320 learning.py:507] global step 3975: loss = 0.9302 (1.300 sec/step)\n",
            "I0919 17:58:50.206049 140474388072320 learning.py:507] global step 3976: loss = 0.7518 (1.304 sec/step)\n",
            "I0919 17:58:51.504245 140474388072320 learning.py:507] global step 3977: loss = 0.7213 (1.296 sec/step)\n",
            "I0919 17:58:52.823458 140474388072320 learning.py:507] global step 3978: loss = 0.7366 (1.318 sec/step)\n",
            "I0919 17:58:54.135304 140474388072320 learning.py:507] global step 3979: loss = 0.7435 (1.310 sec/step)\n",
            "I0919 17:58:55.422171 140474388072320 learning.py:507] global step 3980: loss = 0.9832 (1.285 sec/step)\n",
            "I0919 17:58:56.743196 140474388072320 learning.py:507] global step 3981: loss = 0.6942 (1.319 sec/step)\n",
            "I0919 17:58:58.049957 140474388072320 learning.py:507] global step 3982: loss = 0.7564 (1.305 sec/step)\n",
            "I0919 17:58:59.402967 140474388072320 learning.py:507] global step 3983: loss = 0.9167 (1.351 sec/step)\n",
            "I0919 17:59:00.706643 140474388072320 learning.py:507] global step 3984: loss = 0.9882 (1.302 sec/step)\n",
            "I0919 17:59:02.061604 140474388072320 learning.py:507] global step 3985: loss = 0.7989 (1.353 sec/step)\n",
            "I0919 17:59:03.376956 140474388072320 learning.py:507] global step 3986: loss = 0.8423 (1.313 sec/step)\n",
            "I0919 17:59:04.724911 140474388072320 learning.py:507] global step 3987: loss = 0.8698 (1.346 sec/step)\n",
            "I0919 17:59:06.029906 140474388072320 learning.py:507] global step 3988: loss = 0.9564 (1.303 sec/step)\n",
            "I0919 17:59:07.324651 140474388072320 learning.py:507] global step 3989: loss = 0.7366 (1.293 sec/step)\n",
            "I0919 17:59:08.675420 140474388072320 learning.py:507] global step 3990: loss = 0.7887 (1.349 sec/step)\n",
            "I0919 17:59:09.961483 140474388072320 learning.py:507] global step 3991: loss = 0.8078 (1.285 sec/step)\n",
            "I0919 17:59:11.244385 140474388072320 learning.py:507] global step 3992: loss = 0.8439 (1.281 sec/step)\n",
            "I0919 17:59:12.540255 140474388072320 learning.py:507] global step 3993: loss = 0.6219 (1.294 sec/step)\n",
            "I0919 17:59:13.854028 140474388072320 learning.py:507] global step 3994: loss = 1.0194 (1.312 sec/step)\n",
            "I0919 17:59:15.135024 140474388072320 learning.py:507] global step 3995: loss = 1.1654 (1.279 sec/step)\n",
            "I0919 17:59:16.453814 140474388072320 learning.py:507] global step 3996: loss = 0.8866 (1.317 sec/step)\n",
            "I0919 17:59:17.730372 140474388072320 learning.py:507] global step 3997: loss = 0.8790 (1.275 sec/step)\n",
            "I0919 17:59:19.073560 140474388072320 learning.py:507] global step 3998: loss = 0.8384 (1.342 sec/step)\n",
            "I0919 17:59:20.394953 140474388072320 learning.py:507] global step 3999: loss = 0.8307 (1.320 sec/step)\n",
            "I0919 17:59:21.690618 140474388072320 learning.py:507] global step 4000: loss = 1.1298 (1.294 sec/step)\n",
            "I0919 17:59:22.997845 140474388072320 learning.py:507] global step 4001: loss = 0.8151 (1.306 sec/step)\n",
            "I0919 17:59:24.299868 140474388072320 learning.py:507] global step 4002: loss = 0.9062 (1.300 sec/step)\n",
            "I0919 17:59:25.603723 140474388072320 learning.py:507] global step 4003: loss = 0.8085 (1.302 sec/step)\n",
            "I0919 17:59:26.916232 140474388072320 learning.py:507] global step 4004: loss = 0.7921 (1.311 sec/step)\n",
            "I0919 17:59:28.186960 140474388072320 learning.py:507] global step 4005: loss = 0.8152 (1.269 sec/step)\n",
            "I0919 17:59:29.491044 140474388072320 learning.py:507] global step 4006: loss = 0.9120 (1.302 sec/step)\n",
            "I0919 17:59:30.792344 140474388072320 learning.py:507] global step 4007: loss = 0.7535 (1.300 sec/step)\n",
            "I0919 17:59:32.119395 140474388072320 learning.py:507] global step 4008: loss = 0.8544 (1.325 sec/step)\n",
            "I0919 17:59:33.426902 140474388072320 learning.py:507] global step 4009: loss = 0.9083 (1.306 sec/step)\n",
            "I0919 17:59:34.730396 140474388072320 learning.py:507] global step 4010: loss = 1.3897 (1.302 sec/step)\n",
            "I0919 17:59:36.093640 140474388072320 learning.py:507] global step 4011: loss = 0.8974 (1.362 sec/step)\n",
            "I0919 17:59:37.429636 140474388072320 learning.py:507] global step 4012: loss = 0.8755 (1.334 sec/step)\n",
            "I0919 17:59:38.744190 140474388072320 learning.py:507] global step 4013: loss = 0.7677 (1.313 sec/step)\n",
            "I0919 17:59:40.071561 140474388072320 learning.py:507] global step 4014: loss = 0.7771 (1.325 sec/step)\n",
            "I0919 17:59:41.403841 140474388072320 learning.py:507] global step 4015: loss = 0.8136 (1.330 sec/step)\n",
            "I0919 17:59:42.705614 140474388072320 learning.py:507] global step 4016: loss = 0.8745 (1.300 sec/step)\n",
            "I0919 17:59:44.030615 140474388072320 learning.py:507] global step 4017: loss = 1.0742 (1.323 sec/step)\n",
            "I0919 17:59:45.361649 140474388072320 learning.py:507] global step 4018: loss = 0.6937 (1.329 sec/step)\n",
            "I0919 17:59:46.720563 140474388072320 learning.py:507] global step 4019: loss = 0.7126 (1.357 sec/step)\n",
            "I0919 17:59:48.016253 140474388072320 learning.py:507] global step 4020: loss = 0.8055 (1.294 sec/step)\n",
            "I0919 17:59:49.334398 140474388072320 learning.py:507] global step 4021: loss = 0.8116 (1.316 sec/step)\n",
            "I0919 17:59:50.647212 140474388072320 learning.py:507] global step 4022: loss = 0.8412 (1.311 sec/step)\n",
            "I0919 17:59:51.934643 140474388072320 learning.py:507] global step 4023: loss = 0.7680 (1.285 sec/step)\n",
            "I0919 17:59:53.223431 140474388072320 learning.py:507] global step 4024: loss = 0.8475 (1.287 sec/step)\n",
            "I0919 17:59:54.541239 140474388072320 learning.py:507] global step 4025: loss = 0.8275 (1.316 sec/step)\n",
            "I0919 17:59:55.840051 140474388072320 learning.py:507] global step 4026: loss = 1.0559 (1.297 sec/step)\n",
            "I0919 17:59:57.158406 140474388072320 learning.py:507] global step 4027: loss = 0.8293 (1.316 sec/step)\n",
            "I0919 17:59:58.440527 140474388072320 learning.py:507] global step 4028: loss = 0.7106 (1.280 sec/step)\n",
            "I0919 17:59:59.774090 140474388072320 learning.py:507] global step 4029: loss = 1.0768 (1.331 sec/step)\n",
            "I0919 18:00:01.115911 140474388072320 learning.py:507] global step 4030: loss = 0.8778 (1.340 sec/step)\n",
            "I0919 18:00:02.384610 140474388072320 learning.py:507] global step 4031: loss = 1.0758 (1.267 sec/step)\n",
            "I0919 18:00:03.718621 140474388072320 learning.py:507] global step 4032: loss = 0.9800 (1.332 sec/step)\n",
            "I0919 18:00:05.028199 140474388072320 learning.py:507] global step 4033: loss = 0.7643 (1.308 sec/step)\n",
            "I0919 18:00:06.326524 140474388072320 learning.py:507] global step 4034: loss = 0.7685 (1.297 sec/step)\n",
            "I0919 18:00:07.606843 140474388072320 learning.py:507] global step 4035: loss = 1.0881 (1.278 sec/step)\n",
            "I0919 18:00:08.908083 140474388072320 learning.py:507] global step 4036: loss = 1.0098 (1.299 sec/step)\n",
            "I0919 18:00:10.216593 140474388072320 learning.py:507] global step 4037: loss = 0.7834 (1.306 sec/step)\n",
            "I0919 18:00:11.547566 140474388072320 learning.py:507] global step 4038: loss = 1.0512 (1.329 sec/step)\n",
            "I0919 18:00:12.905601 140474388072320 learning.py:507] global step 4039: loss = 1.2049 (1.356 sec/step)\n",
            "I0919 18:00:14.257034 140474388072320 learning.py:507] global step 4040: loss = 0.7010 (1.350 sec/step)\n",
            "I0919 18:00:15.545672 140474388072320 learning.py:507] global step 4041: loss = 0.8214 (1.287 sec/step)\n",
            "I0919 18:00:16.853952 140474388072320 learning.py:507] global step 4042: loss = 0.8651 (1.306 sec/step)\n",
            "I0919 18:00:18.190418 140474388072320 learning.py:507] global step 4043: loss = 0.8081 (1.335 sec/step)\n",
            "I0919 18:00:19.463363 140474388072320 learning.py:507] global step 4044: loss = 0.8492 (1.271 sec/step)\n",
            "I0919 18:00:20.760441 140474388072320 learning.py:507] global step 4045: loss = 0.7367 (1.296 sec/step)\n",
            "I0919 18:00:22.093997 140474388072320 learning.py:507] global step 4046: loss = 0.8691 (1.332 sec/step)\n",
            "I0919 18:00:23.425878 140474388072320 learning.py:507] global step 4047: loss = 0.7099 (1.330 sec/step)\n",
            "I0919 18:00:24.749538 140474388072320 learning.py:507] global step 4048: loss = 0.7077 (1.322 sec/step)\n",
            "I0919 18:00:26.037051 140474388072320 learning.py:507] global step 4049: loss = 0.9419 (1.286 sec/step)\n",
            "I0919 18:00:26.873307 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 18:00:27.656461 140474388072320 learning.py:507] global step 4050: loss = 0.8729 (1.395 sec/step)\n",
            "I0919 18:00:29.723265 140471297337088 supervisor.py:1050] Recording summary at step 4050.\n",
            "I0919 18:00:30.350182 140474388072320 learning.py:507] global step 4051: loss = 0.9525 (2.606 sec/step)\n",
            "I0919 18:00:32.305340 140474388072320 learning.py:507] global step 4052: loss = 0.7722 (1.948 sec/step)\n",
            "I0919 18:00:33.674305 140474388072320 learning.py:507] global step 4053: loss = 0.8665 (1.367 sec/step)\n",
            "I0919 18:00:34.977669 140474388072320 learning.py:507] global step 4054: loss = 0.8616 (1.300 sec/step)\n",
            "I0919 18:00:36.300426 140474388072320 learning.py:507] global step 4055: loss = 0.8637 (1.321 sec/step)\n",
            "I0919 18:00:37.621344 140474388072320 learning.py:507] global step 4056: loss = 0.9064 (1.319 sec/step)\n",
            "I0919 18:00:38.942533 140474388072320 learning.py:507] global step 4057: loss = 0.7867 (1.319 sec/step)\n",
            "I0919 18:00:40.272138 140474388072320 learning.py:507] global step 4058: loss = 0.8844 (1.328 sec/step)\n",
            "I0919 18:00:41.583234 140474388072320 learning.py:507] global step 4059: loss = 1.0396 (1.309 sec/step)\n",
            "I0919 18:00:42.903193 140474388072320 learning.py:507] global step 4060: loss = 0.7938 (1.318 sec/step)\n",
            "I0919 18:00:44.265379 140474388072320 learning.py:507] global step 4061: loss = 0.8047 (1.360 sec/step)\n",
            "I0919 18:00:45.585741 140474388072320 learning.py:507] global step 4062: loss = 0.7853 (1.319 sec/step)\n",
            "I0919 18:00:46.897660 140474388072320 learning.py:507] global step 4063: loss = 0.7381 (1.310 sec/step)\n",
            "I0919 18:00:48.187987 140474388072320 learning.py:507] global step 4064: loss = 0.8144 (1.289 sec/step)\n",
            "I0919 18:00:49.495284 140474388072320 learning.py:507] global step 4065: loss = 0.8673 (1.306 sec/step)\n",
            "I0919 18:00:50.800995 140474388072320 learning.py:507] global step 4066: loss = 0.8773 (1.304 sec/step)\n",
            "I0919 18:00:52.119339 140474388072320 learning.py:507] global step 4067: loss = 0.7806 (1.317 sec/step)\n",
            "I0919 18:00:53.401902 140474388072320 learning.py:507] global step 4068: loss = 0.7751 (1.281 sec/step)\n",
            "I0919 18:00:54.772564 140474388072320 learning.py:507] global step 4069: loss = 0.7552 (1.369 sec/step)\n",
            "I0919 18:00:56.061595 140474388072320 learning.py:507] global step 4070: loss = 0.9315 (1.287 sec/step)\n",
            "I0919 18:00:57.378010 140474388072320 learning.py:507] global step 4071: loss = 1.0755 (1.315 sec/step)\n",
            "I0919 18:00:58.701597 140474388072320 learning.py:507] global step 4072: loss = 0.8363 (1.322 sec/step)\n",
            "I0919 18:01:00.002366 140474388072320 learning.py:507] global step 4073: loss = 1.0308 (1.299 sec/step)\n",
            "I0919 18:01:01.379223 140474388072320 learning.py:507] global step 4074: loss = 0.9585 (1.375 sec/step)\n",
            "I0919 18:01:02.700350 140474388072320 learning.py:507] global step 4075: loss = 0.8156 (1.319 sec/step)\n",
            "I0919 18:01:04.000638 140474388072320 learning.py:507] global step 4076: loss = 0.9251 (1.298 sec/step)\n",
            "I0919 18:01:05.303245 140474388072320 learning.py:507] global step 4077: loss = 0.9280 (1.301 sec/step)\n",
            "I0919 18:01:06.612732 140474388072320 learning.py:507] global step 4078: loss = 1.0344 (1.308 sec/step)\n",
            "I0919 18:01:07.923048 140474388072320 learning.py:507] global step 4079: loss = 0.8052 (1.309 sec/step)\n",
            "I0919 18:01:09.223511 140474388072320 learning.py:507] global step 4080: loss = 1.0133 (1.299 sec/step)\n",
            "I0919 18:01:10.514648 140474388072320 learning.py:507] global step 4081: loss = 0.8837 (1.289 sec/step)\n",
            "I0919 18:01:11.837388 140474388072320 learning.py:507] global step 4082: loss = 1.1259 (1.321 sec/step)\n",
            "I0919 18:01:13.156193 140474388072320 learning.py:507] global step 4083: loss = 0.8217 (1.317 sec/step)\n",
            "I0919 18:01:14.436151 140474388072320 learning.py:507] global step 4084: loss = 1.0846 (1.278 sec/step)\n",
            "I0919 18:01:15.721750 140474388072320 learning.py:507] global step 4085: loss = 1.0051 (1.284 sec/step)\n",
            "I0919 18:01:17.045325 140474388072320 learning.py:507] global step 4086: loss = 0.7885 (1.322 sec/step)\n",
            "I0919 18:01:18.336328 140474388072320 learning.py:507] global step 4087: loss = 0.7359 (1.290 sec/step)\n",
            "I0919 18:01:19.619611 140474388072320 learning.py:507] global step 4088: loss = 1.1346 (1.282 sec/step)\n",
            "I0919 18:01:20.919682 140474388072320 learning.py:507] global step 4089: loss = 0.9154 (1.298 sec/step)\n",
            "I0919 18:01:22.272690 140474388072320 learning.py:507] global step 4090: loss = 1.0458 (1.351 sec/step)\n",
            "I0919 18:01:23.582696 140474388072320 learning.py:507] global step 4091: loss = 0.7733 (1.308 sec/step)\n",
            "I0919 18:01:24.896521 140474388072320 learning.py:507] global step 4092: loss = 0.8152 (1.312 sec/step)\n",
            "I0919 18:01:26.194328 140474388072320 learning.py:507] global step 4093: loss = 1.0013 (1.296 sec/step)\n",
            "I0919 18:01:27.550167 140474388072320 learning.py:507] global step 4094: loss = 0.8189 (1.354 sec/step)\n",
            "I0919 18:01:28.863173 140474388072320 learning.py:507] global step 4095: loss = 0.7768 (1.311 sec/step)\n",
            "I0919 18:01:30.144155 140474388072320 learning.py:507] global step 4096: loss = 1.0994 (1.279 sec/step)\n",
            "I0919 18:01:31.446060 140474388072320 learning.py:507] global step 4097: loss = 0.9120 (1.300 sec/step)\n",
            "I0919 18:01:32.760615 140474388072320 learning.py:507] global step 4098: loss = 0.7572 (1.313 sec/step)\n",
            "I0919 18:01:34.074007 140474388072320 learning.py:507] global step 4099: loss = 0.7074 (1.312 sec/step)\n",
            "I0919 18:01:35.390942 140474388072320 learning.py:507] global step 4100: loss = 1.1723 (1.315 sec/step)\n",
            "I0919 18:01:36.703541 140474388072320 learning.py:507] global step 4101: loss = 0.8235 (1.311 sec/step)\n",
            "I0919 18:01:38.010993 140474388072320 learning.py:507] global step 4102: loss = 0.9384 (1.306 sec/step)\n",
            "I0919 18:01:39.339745 140474388072320 learning.py:507] global step 4103: loss = 0.6805 (1.327 sec/step)\n",
            "I0919 18:01:40.657459 140474388072320 learning.py:507] global step 4104: loss = 0.9003 (1.316 sec/step)\n",
            "I0919 18:01:41.962769 140474388072320 learning.py:507] global step 4105: loss = 0.9130 (1.304 sec/step)\n",
            "I0919 18:01:43.276506 140474388072320 learning.py:507] global step 4106: loss = 1.1932 (1.312 sec/step)\n",
            "I0919 18:01:44.605225 140474388072320 learning.py:507] global step 4107: loss = 0.9395 (1.327 sec/step)\n",
            "I0919 18:01:45.901938 140474388072320 learning.py:507] global step 4108: loss = 0.8393 (1.295 sec/step)\n",
            "I0919 18:01:47.176737 140474388072320 learning.py:507] global step 4109: loss = 0.8865 (1.273 sec/step)\n",
            "I0919 18:01:48.495439 140474388072320 learning.py:507] global step 4110: loss = 0.8284 (1.317 sec/step)\n",
            "I0919 18:01:49.822566 140474388072320 learning.py:507] global step 4111: loss = 0.8647 (1.325 sec/step)\n",
            "I0919 18:01:51.120309 140474388072320 learning.py:507] global step 4112: loss = 0.9450 (1.296 sec/step)\n",
            "I0919 18:01:52.444736 140474388072320 learning.py:507] global step 4113: loss = 0.9607 (1.323 sec/step)\n",
            "I0919 18:01:53.781346 140474388072320 learning.py:507] global step 4114: loss = 1.1057 (1.335 sec/step)\n",
            "I0919 18:01:55.086076 140474388072320 learning.py:507] global step 4115: loss = 0.8164 (1.303 sec/step)\n",
            "I0919 18:01:56.380918 140474388072320 learning.py:507] global step 4116: loss = 0.9028 (1.293 sec/step)\n",
            "I0919 18:01:57.654019 140474388072320 learning.py:507] global step 4117: loss = 0.8023 (1.272 sec/step)\n",
            "I0919 18:01:58.975435 140474388072320 learning.py:507] global step 4118: loss = 0.7317 (1.320 sec/step)\n",
            "I0919 18:02:00.299395 140474388072320 learning.py:507] global step 4119: loss = 1.1620 (1.322 sec/step)\n",
            "I0919 18:02:01.612195 140474388072320 learning.py:507] global step 4120: loss = 1.0446 (1.311 sec/step)\n",
            "I0919 18:02:02.915870 140474388072320 learning.py:507] global step 4121: loss = 0.7686 (1.302 sec/step)\n",
            "I0919 18:02:04.212358 140474388072320 learning.py:507] global step 4122: loss = 0.8756 (1.295 sec/step)\n",
            "I0919 18:02:05.524851 140474388072320 learning.py:507] global step 4123: loss = 0.8678 (1.311 sec/step)\n",
            "I0919 18:02:06.868355 140474388072320 learning.py:507] global step 4124: loss = 0.8994 (1.342 sec/step)\n",
            "I0919 18:02:08.178066 140474388072320 learning.py:507] global step 4125: loss = 0.7601 (1.308 sec/step)\n",
            "I0919 18:02:09.475513 140474388072320 learning.py:507] global step 4126: loss = 0.7462 (1.295 sec/step)\n",
            "I0919 18:02:10.800384 140474388072320 learning.py:507] global step 4127: loss = 1.1190 (1.323 sec/step)\n",
            "I0919 18:02:12.115147 140474388072320 learning.py:507] global step 4128: loss = 0.9413 (1.313 sec/step)\n",
            "I0919 18:02:13.455473 140474388072320 learning.py:507] global step 4129: loss = 0.8892 (1.339 sec/step)\n",
            "I0919 18:02:14.781636 140474388072320 learning.py:507] global step 4130: loss = 0.9152 (1.324 sec/step)\n",
            "I0919 18:02:16.083379 140474388072320 learning.py:507] global step 4131: loss = 1.1739 (1.300 sec/step)\n",
            "I0919 18:02:17.386616 140474388072320 learning.py:507] global step 4132: loss = 0.7653 (1.302 sec/step)\n",
            "I0919 18:02:18.688535 140474388072320 learning.py:507] global step 4133: loss = 0.8652 (1.300 sec/step)\n",
            "I0919 18:02:19.963869 140474388072320 learning.py:507] global step 4134: loss = 0.8297 (1.274 sec/step)\n",
            "I0919 18:02:21.255857 140474388072320 learning.py:507] global step 4135: loss = 0.7509 (1.290 sec/step)\n",
            "I0919 18:02:22.547102 140474388072320 learning.py:507] global step 4136: loss = 0.7007 (1.290 sec/step)\n",
            "I0919 18:02:23.870058 140474388072320 learning.py:507] global step 4137: loss = 0.9400 (1.321 sec/step)\n",
            "I0919 18:02:25.239505 140474388072320 learning.py:507] global step 4138: loss = 0.8403 (1.368 sec/step)\n",
            "I0919 18:02:26.538779 140474388072320 learning.py:507] global step 4139: loss = 1.1404 (1.297 sec/step)\n",
            "I0919 18:02:28.188411 140474388072320 learning.py:507] global step 4140: loss = 0.8703 (1.648 sec/step)\n",
            "I0919 18:02:29.639591 140471297337088 supervisor.py:1050] Recording summary at step 4140.\n",
            "I0919 18:02:30.077599 140474388072320 learning.py:507] global step 4141: loss = 0.8731 (1.886 sec/step)\n",
            "I0919 18:02:31.414448 140474388072320 learning.py:507] global step 4142: loss = 0.7721 (1.335 sec/step)\n",
            "I0919 18:02:32.733422 140474388072320 learning.py:507] global step 4143: loss = 0.9258 (1.317 sec/step)\n",
            "I0919 18:02:34.041412 140474388072320 learning.py:507] global step 4144: loss = 0.9279 (1.306 sec/step)\n",
            "I0919 18:02:35.376709 140474388072320 learning.py:507] global step 4145: loss = 0.6877 (1.334 sec/step)\n",
            "I0919 18:02:36.710055 140474388072320 learning.py:507] global step 4146: loss = 0.8541 (1.331 sec/step)\n",
            "I0919 18:02:38.023036 140474388072320 learning.py:507] global step 4147: loss = 0.9391 (1.311 sec/step)\n",
            "I0919 18:02:39.349519 140474388072320 learning.py:507] global step 4148: loss = 1.1612 (1.325 sec/step)\n",
            "I0919 18:02:40.668179 140474388072320 learning.py:507] global step 4149: loss = 0.8612 (1.317 sec/step)\n",
            "I0919 18:02:41.960103 140474388072320 learning.py:507] global step 4150: loss = 0.7991 (1.290 sec/step)\n",
            "I0919 18:02:43.283061 140474388072320 learning.py:507] global step 4151: loss = 0.8726 (1.321 sec/step)\n",
            "I0919 18:02:44.619184 140474388072320 learning.py:507] global step 4152: loss = 1.2157 (1.335 sec/step)\n",
            "I0919 18:02:45.953155 140474388072320 learning.py:507] global step 4153: loss = 0.9918 (1.333 sec/step)\n",
            "I0919 18:02:47.268843 140474388072320 learning.py:507] global step 4154: loss = 0.8358 (1.314 sec/step)\n",
            "I0919 18:02:48.597223 140474388072320 learning.py:507] global step 4155: loss = 0.8350 (1.327 sec/step)\n",
            "I0919 18:02:49.887208 140474388072320 learning.py:507] global step 4156: loss = 0.7159 (1.288 sec/step)\n",
            "I0919 18:02:51.168329 140474388072320 learning.py:507] global step 4157: loss = 0.7983 (1.279 sec/step)\n",
            "I0919 18:02:52.466032 140474388072320 learning.py:507] global step 4158: loss = 1.1260 (1.296 sec/step)\n",
            "I0919 18:02:53.783140 140474388072320 learning.py:507] global step 4159: loss = 1.1448 (1.316 sec/step)\n",
            "I0919 18:02:55.094943 140474388072320 learning.py:507] global step 4160: loss = 0.6824 (1.310 sec/step)\n",
            "I0919 18:02:56.389143 140474388072320 learning.py:507] global step 4161: loss = 0.9176 (1.292 sec/step)\n",
            "I0919 18:02:57.675543 140474388072320 learning.py:507] global step 4162: loss = 1.1411 (1.284 sec/step)\n",
            "I0919 18:02:58.989712 140474388072320 learning.py:507] global step 4163: loss = 0.9910 (1.312 sec/step)\n",
            "I0919 18:03:00.283617 140474388072320 learning.py:507] global step 4164: loss = 0.8420 (1.292 sec/step)\n",
            "I0919 18:03:01.652240 140474388072320 learning.py:507] global step 4165: loss = 0.9385 (1.367 sec/step)\n",
            "I0919 18:03:02.986876 140474388072320 learning.py:507] global step 4166: loss = 0.9646 (1.333 sec/step)\n",
            "I0919 18:03:04.289889 140474388072320 learning.py:507] global step 4167: loss = 0.8027 (1.301 sec/step)\n",
            "I0919 18:03:05.607388 140474388072320 learning.py:507] global step 4168: loss = 0.8700 (1.316 sec/step)\n",
            "I0919 18:03:06.923754 140474388072320 learning.py:507] global step 4169: loss = 0.7007 (1.315 sec/step)\n",
            "I0919 18:03:08.206641 140474388072320 learning.py:507] global step 4170: loss = 0.9098 (1.281 sec/step)\n",
            "I0919 18:03:09.509977 140474388072320 learning.py:507] global step 4171: loss = 0.9548 (1.301 sec/step)\n",
            "I0919 18:03:10.851634 140474388072320 learning.py:507] global step 4172: loss = 0.7840 (1.340 sec/step)\n",
            "I0919 18:03:12.164099 140474388072320 learning.py:507] global step 4173: loss = 0.8875 (1.311 sec/step)\n",
            "I0919 18:03:13.472474 140474388072320 learning.py:507] global step 4174: loss = 0.9545 (1.307 sec/step)\n",
            "I0919 18:03:14.771049 140474388072320 learning.py:507] global step 4175: loss = 0.8637 (1.297 sec/step)\n",
            "I0919 18:03:16.055569 140474388072320 learning.py:507] global step 4176: loss = 0.8693 (1.283 sec/step)\n",
            "I0919 18:03:17.371092 140474388072320 learning.py:507] global step 4177: loss = 0.8046 (1.314 sec/step)\n",
            "I0919 18:03:18.665050 140474388072320 learning.py:507] global step 4178: loss = 0.8157 (1.292 sec/step)\n",
            "I0919 18:03:19.967127 140474388072320 learning.py:507] global step 4179: loss = 0.7133 (1.300 sec/step)\n",
            "I0919 18:03:21.296681 140474388072320 learning.py:507] global step 4180: loss = 1.0546 (1.328 sec/step)\n",
            "I0919 18:03:22.629863 140474388072320 learning.py:507] global step 4181: loss = 1.1774 (1.331 sec/step)\n",
            "I0919 18:03:23.939555 140474388072320 learning.py:507] global step 4182: loss = 0.9383 (1.308 sec/step)\n",
            "I0919 18:03:25.261529 140474388072320 learning.py:507] global step 4183: loss = 0.8781 (1.320 sec/step)\n",
            "I0919 18:03:26.567994 140474388072320 learning.py:507] global step 4184: loss = 1.2992 (1.305 sec/step)\n",
            "I0919 18:03:27.866174 140474388072320 learning.py:507] global step 4185: loss = 0.7288 (1.296 sec/step)\n",
            "I0919 18:03:29.192750 140474388072320 learning.py:507] global step 4186: loss = 0.8003 (1.324 sec/step)\n",
            "I0919 18:03:30.525765 140474388072320 learning.py:507] global step 4187: loss = 0.8199 (1.331 sec/step)\n",
            "I0919 18:03:31.840821 140474388072320 learning.py:507] global step 4188: loss = 0.9627 (1.313 sec/step)\n",
            "I0919 18:03:33.157576 140474388072320 learning.py:507] global step 4189: loss = 1.0502 (1.315 sec/step)\n",
            "I0919 18:03:34.474524 140474388072320 learning.py:507] global step 4190: loss = 0.8490 (1.315 sec/step)\n",
            "I0919 18:03:35.827316 140474388072320 learning.py:507] global step 4191: loss = 1.0421 (1.351 sec/step)\n",
            "I0919 18:03:37.169224 140474388072320 learning.py:507] global step 4192: loss = 1.0936 (1.340 sec/step)\n",
            "I0919 18:03:38.470381 140474388072320 learning.py:507] global step 4193: loss = 1.1597 (1.299 sec/step)\n",
            "I0919 18:03:39.815886 140474388072320 learning.py:507] global step 4194: loss = 0.7856 (1.344 sec/step)\n",
            "I0919 18:03:41.154295 140474388072320 learning.py:507] global step 4195: loss = 0.9452 (1.336 sec/step)\n",
            "I0919 18:03:42.449624 140474388072320 learning.py:507] global step 4196: loss = 0.8514 (1.294 sec/step)\n",
            "I0919 18:03:43.723074 140474388072320 learning.py:507] global step 4197: loss = 0.9229 (1.271 sec/step)\n",
            "I0919 18:03:45.034392 140474388072320 learning.py:507] global step 4198: loss = 0.8496 (1.310 sec/step)\n",
            "I0919 18:03:46.375308 140474388072320 learning.py:507] global step 4199: loss = 0.9416 (1.339 sec/step)\n",
            "I0919 18:03:47.690588 140474388072320 learning.py:507] global step 4200: loss = 0.8775 (1.313 sec/step)\n",
            "I0919 18:03:49.016447 140474388072320 learning.py:507] global step 4201: loss = 0.9580 (1.324 sec/step)\n",
            "I0919 18:03:50.336267 140474388072320 learning.py:507] global step 4202: loss = 0.6668 (1.318 sec/step)\n",
            "I0919 18:03:51.641005 140474388072320 learning.py:507] global step 4203: loss = 0.7597 (1.303 sec/step)\n",
            "I0919 18:03:52.961549 140474388072320 learning.py:507] global step 4204: loss = 0.9661 (1.319 sec/step)\n",
            "I0919 18:03:54.280575 140474388072320 learning.py:507] global step 4205: loss = 0.8587 (1.317 sec/step)\n",
            "I0919 18:03:55.613188 140474388072320 learning.py:507] global step 4206: loss = 0.7348 (1.331 sec/step)\n",
            "I0919 18:03:56.910198 140474388072320 learning.py:507] global step 4207: loss = 0.6714 (1.295 sec/step)\n",
            "I0919 18:03:58.223014 140474388072320 learning.py:507] global step 4208: loss = 1.0856 (1.311 sec/step)\n",
            "I0919 18:03:59.530556 140474388072320 learning.py:507] global step 4209: loss = 0.8576 (1.306 sec/step)\n",
            "I0919 18:04:00.888916 140474388072320 learning.py:507] global step 4210: loss = 0.7347 (1.357 sec/step)\n",
            "I0919 18:04:02.198302 140474388072320 learning.py:507] global step 4211: loss = 0.6984 (1.307 sec/step)\n",
            "I0919 18:04:03.511954 140474388072320 learning.py:507] global step 4212: loss = 0.6616 (1.312 sec/step)\n",
            "I0919 18:04:04.846591 140474388072320 learning.py:507] global step 4213: loss = 0.9506 (1.333 sec/step)\n",
            "I0919 18:04:06.167503 140474388072320 learning.py:507] global step 4214: loss = 1.0627 (1.319 sec/step)\n",
            "I0919 18:04:07.468159 140474388072320 learning.py:507] global step 4215: loss = 0.7932 (1.299 sec/step)\n",
            "I0919 18:04:08.765417 140474388072320 learning.py:507] global step 4216: loss = 0.7915 (1.295 sec/step)\n",
            "I0919 18:04:10.038074 140474388072320 learning.py:507] global step 4217: loss = 0.9313 (1.271 sec/step)\n",
            "I0919 18:04:11.337084 140474388072320 learning.py:507] global step 4218: loss = 0.9120 (1.297 sec/step)\n",
            "I0919 18:04:12.628665 140474388072320 learning.py:507] global step 4219: loss = 0.7927 (1.289 sec/step)\n",
            "I0919 18:04:13.958154 140474388072320 learning.py:507] global step 4220: loss = 0.9404 (1.328 sec/step)\n",
            "I0919 18:04:15.248589 140474388072320 learning.py:507] global step 4221: loss = 0.6882 (1.289 sec/step)\n",
            "I0919 18:04:16.608566 140474388072320 learning.py:507] global step 4222: loss = 0.8500 (1.358 sec/step)\n",
            "I0919 18:04:17.889389 140474388072320 learning.py:507] global step 4223: loss = 0.8605 (1.279 sec/step)\n",
            "I0919 18:04:19.216026 140474388072320 learning.py:507] global step 4224: loss = 0.8228 (1.325 sec/step)\n",
            "I0919 18:04:20.500797 140474388072320 learning.py:507] global step 4225: loss = 0.6123 (1.283 sec/step)\n",
            "I0919 18:04:21.821307 140474388072320 learning.py:507] global step 4226: loss = 0.7729 (1.319 sec/step)\n",
            "I0919 18:04:23.112919 140474388072320 learning.py:507] global step 4227: loss = 0.9437 (1.290 sec/step)\n",
            "I0919 18:04:24.404315 140474388072320 learning.py:507] global step 4228: loss = 0.6764 (1.290 sec/step)\n",
            "I0919 18:04:25.752686 140474388072320 learning.py:507] global step 4229: loss = 0.7853 (1.347 sec/step)\n",
            "I0919 18:04:27.074403 140474388072320 learning.py:507] global step 4230: loss = 0.7651 (1.320 sec/step)\n",
            "I0919 18:04:28.828375 140471297337088 supervisor.py:1050] Recording summary at step 4230.\n",
            "I0919 18:04:29.264636 140474388072320 learning.py:507] global step 4231: loss = 0.8231 (2.189 sec/step)\n",
            "I0919 18:04:30.553280 140474388072320 learning.py:507] global step 4232: loss = 0.8716 (1.287 sec/step)\n",
            "I0919 18:04:31.884592 140474388072320 learning.py:507] global step 4233: loss = 1.0260 (1.330 sec/step)\n",
            "I0919 18:04:33.228168 140474388072320 learning.py:507] global step 4234: loss = 0.7824 (1.342 sec/step)\n",
            "I0919 18:04:34.532551 140474388072320 learning.py:507] global step 4235: loss = 0.7508 (1.303 sec/step)\n",
            "I0919 18:04:35.857306 140474388072320 learning.py:507] global step 4236: loss = 0.8048 (1.323 sec/step)\n",
            "I0919 18:04:37.155905 140474388072320 learning.py:507] global step 4237: loss = 0.7655 (1.297 sec/step)\n",
            "I0919 18:04:38.444446 140474388072320 learning.py:507] global step 4238: loss = 0.9439 (1.287 sec/step)\n",
            "I0919 18:04:39.778269 140474388072320 learning.py:507] global step 4239: loss = 1.0369 (1.332 sec/step)\n",
            "I0919 18:04:41.124139 140474388072320 learning.py:507] global step 4240: loss = 0.9269 (1.344 sec/step)\n",
            "I0919 18:04:42.474023 140474388072320 learning.py:507] global step 4241: loss = 0.7655 (1.348 sec/step)\n",
            "I0919 18:04:43.808933 140474388072320 learning.py:507] global step 4242: loss = 0.7417 (1.333 sec/step)\n",
            "I0919 18:04:45.128529 140474388072320 learning.py:507] global step 4243: loss = 0.7545 (1.318 sec/step)\n",
            "I0919 18:04:46.468012 140474388072320 learning.py:507] global step 4244: loss = 0.9447 (1.337 sec/step)\n",
            "I0919 18:04:47.763066 140474388072320 learning.py:507] global step 4245: loss = 1.1185 (1.293 sec/step)\n",
            "I0919 18:04:49.088737 140474388072320 learning.py:507] global step 4246: loss = 0.6617 (1.324 sec/step)\n",
            "I0919 18:04:50.395875 140474388072320 learning.py:507] global step 4247: loss = 0.8025 (1.305 sec/step)\n",
            "I0919 18:04:51.695697 140474388072320 learning.py:507] global step 4248: loss = 0.8463 (1.298 sec/step)\n",
            "I0919 18:04:53.031591 140474388072320 learning.py:507] global step 4249: loss = 1.0910 (1.334 sec/step)\n",
            "I0919 18:04:54.348193 140474388072320 learning.py:507] global step 4250: loss = 0.7276 (1.315 sec/step)\n",
            "I0919 18:04:55.622936 140474388072320 learning.py:507] global step 4251: loss = 0.8502 (1.273 sec/step)\n",
            "I0919 18:04:56.920063 140474388072320 learning.py:507] global step 4252: loss = 0.7730 (1.295 sec/step)\n",
            "I0919 18:04:58.289602 140474388072320 learning.py:507] global step 4253: loss = 0.8038 (1.365 sec/step)\n",
            "I0919 18:04:59.673882 140474388072320 learning.py:507] global step 4254: loss = 0.7768 (1.381 sec/step)\n",
            "I0919 18:05:00.966985 140474388072320 learning.py:507] global step 4255: loss = 0.8437 (1.291 sec/step)\n",
            "I0919 18:05:02.270585 140474388072320 learning.py:507] global step 4256: loss = 0.6572 (1.302 sec/step)\n",
            "I0919 18:05:03.610601 140474388072320 learning.py:507] global step 4257: loss = 0.9914 (1.338 sec/step)\n",
            "I0919 18:05:04.938631 140474388072320 learning.py:507] global step 4258: loss = 0.8749 (1.326 sec/step)\n",
            "I0919 18:05:06.218691 140474388072320 learning.py:507] global step 4259: loss = 1.0252 (1.278 sec/step)\n",
            "I0919 18:05:07.545144 140474388072320 learning.py:507] global step 4260: loss = 0.7578 (1.324 sec/step)\n",
            "I0919 18:05:08.860008 140474388072320 learning.py:507] global step 4261: loss = 0.7802 (1.313 sec/step)\n",
            "I0919 18:05:10.182596 140474388072320 learning.py:507] global step 4262: loss = 0.7535 (1.321 sec/step)\n",
            "I0919 18:05:11.481888 140474388072320 learning.py:507] global step 4263: loss = 1.2455 (1.298 sec/step)\n",
            "I0919 18:05:12.777208 140474388072320 learning.py:507] global step 4264: loss = 0.8732 (1.293 sec/step)\n",
            "I0919 18:05:14.158796 140474388072320 learning.py:507] global step 4265: loss = 0.7850 (1.380 sec/step)\n",
            "I0919 18:05:15.504698 140474388072320 learning.py:507] global step 4266: loss = 0.6917 (1.344 sec/step)\n",
            "I0919 18:05:16.835649 140474388072320 learning.py:507] global step 4267: loss = 0.8700 (1.329 sec/step)\n",
            "I0919 18:05:18.124480 140474388072320 learning.py:507] global step 4268: loss = 0.7088 (1.287 sec/step)\n",
            "I0919 18:05:19.418306 140474388072320 learning.py:507] global step 4269: loss = 0.9100 (1.292 sec/step)\n",
            "I0919 18:05:20.715435 140474388072320 learning.py:507] global step 4270: loss = 0.6985 (1.295 sec/step)\n",
            "I0919 18:05:22.024156 140474388072320 learning.py:507] global step 4271: loss = 0.6394 (1.307 sec/step)\n",
            "I0919 18:05:23.355006 140474388072320 learning.py:507] global step 4272: loss = 0.7234 (1.329 sec/step)\n",
            "I0919 18:05:24.687968 140474388072320 learning.py:507] global step 4273: loss = 0.7546 (1.331 sec/step)\n",
            "I0919 18:05:26.007714 140474388072320 learning.py:507] global step 4274: loss = 1.1492 (1.318 sec/step)\n",
            "I0919 18:05:27.316092 140474388072320 learning.py:507] global step 4275: loss = 0.8753 (1.307 sec/step)\n",
            "I0919 18:05:28.678141 140474388072320 learning.py:507] global step 4276: loss = 0.7089 (1.361 sec/step)\n",
            "I0919 18:05:29.992419 140474388072320 learning.py:507] global step 4277: loss = 0.6367 (1.312 sec/step)\n",
            "I0919 18:05:31.297987 140474388072320 learning.py:507] global step 4278: loss = 0.9126 (1.304 sec/step)\n",
            "I0919 18:05:32.621994 140474388072320 learning.py:507] global step 4279: loss = 0.8587 (1.322 sec/step)\n",
            "I0919 18:05:33.958615 140474388072320 learning.py:507] global step 4280: loss = 0.9209 (1.335 sec/step)\n",
            "I0919 18:05:35.278143 140474388072320 learning.py:507] global step 4281: loss = 0.7911 (1.318 sec/step)\n",
            "I0919 18:05:36.599452 140474388072320 learning.py:507] global step 4282: loss = 1.3814 (1.320 sec/step)\n",
            "I0919 18:05:37.964462 140474388072320 learning.py:507] global step 4283: loss = 1.3051 (1.363 sec/step)\n",
            "I0919 18:05:39.323061 140474388072320 learning.py:507] global step 4284: loss = 0.9107 (1.357 sec/step)\n",
            "I0919 18:05:40.666248 140474388072320 learning.py:507] global step 4285: loss = 0.9830 (1.341 sec/step)\n",
            "I0919 18:05:41.965506 140474388072320 learning.py:507] global step 4286: loss = 0.9266 (1.297 sec/step)\n",
            "I0919 18:05:43.278129 140474388072320 learning.py:507] global step 4287: loss = 0.6984 (1.310 sec/step)\n",
            "I0919 18:05:44.591361 140474388072320 learning.py:507] global step 4288: loss = 0.8005 (1.311 sec/step)\n",
            "I0919 18:05:45.905818 140474388072320 learning.py:507] global step 4289: loss = 0.7883 (1.313 sec/step)\n",
            "I0919 18:05:47.200845 140474388072320 learning.py:507] global step 4290: loss = 0.8930 (1.293 sec/step)\n",
            "I0919 18:05:48.508066 140474388072320 learning.py:507] global step 4291: loss = 0.8463 (1.305 sec/step)\n",
            "I0919 18:05:49.791981 140474388072320 learning.py:507] global step 4292: loss = 0.6656 (1.282 sec/step)\n",
            "I0919 18:05:51.102670 140474388072320 learning.py:507] global step 4293: loss = 1.0054 (1.309 sec/step)\n",
            "I0919 18:05:52.378668 140474388072320 learning.py:507] global step 4294: loss = 0.8242 (1.274 sec/step)\n",
            "I0919 18:05:53.725608 140474388072320 learning.py:507] global step 4295: loss = 0.7354 (1.345 sec/step)\n",
            "I0919 18:05:55.084889 140474388072320 learning.py:507] global step 4296: loss = 1.1100 (1.357 sec/step)\n",
            "I0919 18:05:56.401067 140474388072320 learning.py:507] global step 4297: loss = 0.9549 (1.315 sec/step)\n",
            "I0919 18:05:57.724364 140474388072320 learning.py:507] global step 4298: loss = 0.9022 (1.321 sec/step)\n",
            "I0919 18:05:59.051593 140474388072320 learning.py:507] global step 4299: loss = 0.7649 (1.325 sec/step)\n",
            "I0919 18:06:00.345354 140474388072320 learning.py:507] global step 4300: loss = 1.0377 (1.292 sec/step)\n",
            "I0919 18:06:01.649782 140474388072320 learning.py:507] global step 4301: loss = 0.9131 (1.303 sec/step)\n",
            "I0919 18:06:02.961225 140474388072320 learning.py:507] global step 4302: loss = 1.0219 (1.309 sec/step)\n",
            "I0919 18:06:04.273883 140474388072320 learning.py:507] global step 4303: loss = 0.7916 (1.311 sec/step)\n",
            "I0919 18:06:05.624382 140474388072320 learning.py:507] global step 4304: loss = 0.7233 (1.349 sec/step)\n",
            "I0919 18:06:06.936258 140474388072320 learning.py:507] global step 4305: loss = 0.9207 (1.310 sec/step)\n",
            "I0919 18:06:08.287190 140474388072320 learning.py:507] global step 4306: loss = 1.0655 (1.349 sec/step)\n",
            "I0919 18:06:09.594322 140474388072320 learning.py:507] global step 4307: loss = 0.8598 (1.305 sec/step)\n",
            "I0919 18:06:10.904342 140474388072320 learning.py:507] global step 4308: loss = 0.7958 (1.308 sec/step)\n",
            "I0919 18:06:12.192676 140474388072320 learning.py:507] global step 4309: loss = 0.7374 (1.287 sec/step)\n",
            "I0919 18:06:13.511042 140474388072320 learning.py:507] global step 4310: loss = 0.8787 (1.316 sec/step)\n",
            "I0919 18:06:14.835832 140474388072320 learning.py:507] global step 4311: loss = 0.7140 (1.323 sec/step)\n",
            "I0919 18:06:16.162575 140474388072320 learning.py:507] global step 4312: loss = 1.0015 (1.325 sec/step)\n",
            "I0919 18:06:17.460383 140474388072320 learning.py:507] global step 4313: loss = 1.0577 (1.296 sec/step)\n",
            "I0919 18:06:18.826009 140474388072320 learning.py:507] global step 4314: loss = 0.8710 (1.364 sec/step)\n",
            "I0919 18:06:20.115135 140474388072320 learning.py:507] global step 4315: loss = 0.8496 (1.287 sec/step)\n",
            "I0919 18:06:21.435612 140474388072320 learning.py:507] global step 4316: loss = 0.9233 (1.319 sec/step)\n",
            "I0919 18:06:22.773190 140474388072320 learning.py:507] global step 4317: loss = 0.8130 (1.336 sec/step)\n",
            "I0919 18:06:24.077708 140474388072320 learning.py:507] global step 4318: loss = 0.8718 (1.302 sec/step)\n",
            "I0919 18:06:25.401339 140474388072320 learning.py:507] global step 4319: loss = 0.7477 (1.322 sec/step)\n",
            "I0919 18:06:26.734085 140474388072320 learning.py:507] global step 4320: loss = 0.8291 (1.330 sec/step)\n",
            "I0919 18:06:28.946086 140471297337088 supervisor.py:1050] Recording summary at step 4321.\n",
            "I0919 18:06:28.964622 140474388072320 learning.py:507] global step 4321: loss = 0.6627 (2.224 sec/step)\n",
            "I0919 18:06:30.264667 140474388072320 learning.py:507] global step 4322: loss = 0.9417 (1.298 sec/step)\n",
            "I0919 18:06:31.597032 140474388072320 learning.py:507] global step 4323: loss = 0.7890 (1.331 sec/step)\n",
            "I0919 18:06:32.950836 140474388072320 learning.py:507] global step 4324: loss = 0.7635 (1.352 sec/step)\n",
            "I0919 18:06:34.248316 140474388072320 learning.py:507] global step 4325: loss = 0.6870 (1.295 sec/step)\n",
            "I0919 18:06:35.561676 140474388072320 learning.py:507] global step 4326: loss = 0.8194 (1.312 sec/step)\n",
            "I0919 18:06:36.885982 140474388072320 learning.py:507] global step 4327: loss = 0.8733 (1.322 sec/step)\n",
            "I0919 18:06:38.248727 140474388072320 learning.py:507] global step 4328: loss = 0.7640 (1.361 sec/step)\n",
            "I0919 18:06:39.614940 140474388072320 learning.py:507] global step 4329: loss = 0.7737 (1.364 sec/step)\n",
            "I0919 18:06:40.920639 140474388072320 learning.py:507] global step 4330: loss = 0.8228 (1.304 sec/step)\n",
            "I0919 18:06:42.270612 140474388072320 learning.py:507] global step 4331: loss = 0.6283 (1.348 sec/step)\n",
            "I0919 18:06:43.566593 140474388072320 learning.py:507] global step 4332: loss = 0.7177 (1.294 sec/step)\n",
            "I0919 18:06:44.908732 140474388072320 learning.py:507] global step 4333: loss = 0.8699 (1.340 sec/step)\n",
            "I0919 18:06:46.288404 140474388072320 learning.py:507] global step 4334: loss = 0.9802 (1.378 sec/step)\n",
            "I0919 18:06:47.631385 140474388072320 learning.py:507] global step 4335: loss = 0.9672 (1.341 sec/step)\n",
            "I0919 18:06:48.922795 140474388072320 learning.py:507] global step 4336: loss = 0.9421 (1.290 sec/step)\n",
            "I0919 18:06:50.249639 140474388072320 learning.py:507] global step 4337: loss = 0.7894 (1.325 sec/step)\n",
            "I0919 18:06:51.587548 140474388072320 learning.py:507] global step 4338: loss = 0.8479 (1.336 sec/step)\n",
            "I0919 18:06:52.929056 140474388072320 learning.py:507] global step 4339: loss = 0.6294 (1.340 sec/step)\n",
            "I0919 18:06:54.275902 140474388072320 learning.py:507] global step 4340: loss = 0.7625 (1.345 sec/step)\n",
            "I0919 18:06:55.588085 140474388072320 learning.py:507] global step 4341: loss = 1.0019 (1.310 sec/step)\n",
            "I0919 18:06:56.904206 140474388072320 learning.py:507] global step 4342: loss = 0.6998 (1.314 sec/step)\n",
            "I0919 18:06:58.236465 140474388072320 learning.py:507] global step 4343: loss = 0.8661 (1.330 sec/step)\n",
            "I0919 18:06:59.565407 140474388072320 learning.py:507] global step 4344: loss = 0.6769 (1.327 sec/step)\n",
            "I0919 18:07:00.885102 140474388072320 learning.py:507] global step 4345: loss = 0.9341 (1.318 sec/step)\n",
            "I0919 18:07:02.220123 140474388072320 learning.py:507] global step 4346: loss = 0.8699 (1.333 sec/step)\n",
            "I0919 18:07:03.534598 140474388072320 learning.py:507] global step 4347: loss = 0.9041 (1.313 sec/step)\n",
            "I0919 18:07:04.853368 140474388072320 learning.py:507] global step 4348: loss = 0.9598 (1.317 sec/step)\n",
            "I0919 18:07:06.117841 140474388072320 learning.py:507] global step 4349: loss = 1.0447 (1.263 sec/step)\n",
            "I0919 18:07:07.427233 140474388072320 learning.py:507] global step 4350: loss = 0.8930 (1.308 sec/step)\n",
            "I0919 18:07:08.758096 140474388072320 learning.py:507] global step 4351: loss = 0.8916 (1.329 sec/step)\n",
            "I0919 18:07:10.108180 140474388072320 learning.py:507] global step 4352: loss = 0.9107 (1.348 sec/step)\n",
            "I0919 18:07:11.419953 140474388072320 learning.py:507] global step 4353: loss = 1.0004 (1.310 sec/step)\n",
            "I0919 18:07:12.696962 140474388072320 learning.py:507] global step 4354: loss = 0.9483 (1.275 sec/step)\n",
            "I0919 18:07:14.057523 140474388072320 learning.py:507] global step 4355: loss = 0.6868 (1.359 sec/step)\n",
            "I0919 18:07:15.362684 140474388072320 learning.py:507] global step 4356: loss = 0.8945 (1.303 sec/step)\n",
            "I0919 18:07:16.724370 140474388072320 learning.py:507] global step 4357: loss = 0.7538 (1.360 sec/step)\n",
            "I0919 18:07:18.049743 140474388072320 learning.py:507] global step 4358: loss = 0.7956 (1.324 sec/step)\n",
            "I0919 18:07:19.340955 140474388072320 learning.py:507] global step 4359: loss = 0.9252 (1.290 sec/step)\n",
            "I0919 18:07:20.649746 140474388072320 learning.py:507] global step 4360: loss = 0.8520 (1.307 sec/step)\n",
            "I0919 18:07:21.981204 140474388072320 learning.py:507] global step 4361: loss = 1.0635 (1.329 sec/step)\n",
            "I0919 18:07:23.286602 140474388072320 learning.py:507] global step 4362: loss = 0.6939 (1.304 sec/step)\n",
            "I0919 18:07:24.608957 140474388072320 learning.py:507] global step 4363: loss = 0.7991 (1.321 sec/step)\n",
            "I0919 18:07:25.957045 140474388072320 learning.py:507] global step 4364: loss = 0.9228 (1.346 sec/step)\n",
            "I0919 18:07:27.276981 140474388072320 learning.py:507] global step 4365: loss = 0.9033 (1.319 sec/step)\n",
            "I0919 18:07:28.600565 140474388072320 learning.py:507] global step 4366: loss = 0.9088 (1.322 sec/step)\n",
            "I0919 18:07:29.918519 140474388072320 learning.py:507] global step 4367: loss = 0.6984 (1.316 sec/step)\n",
            "I0919 18:07:31.254733 140474388072320 learning.py:507] global step 4368: loss = 0.6606 (1.334 sec/step)\n",
            "I0919 18:07:32.611485 140474388072320 learning.py:507] global step 4369: loss = 0.8368 (1.355 sec/step)\n",
            "I0919 18:07:33.942559 140474388072320 learning.py:507] global step 4370: loss = 1.1134 (1.329 sec/step)\n",
            "I0919 18:07:35.293400 140474388072320 learning.py:507] global step 4371: loss = 0.8226 (1.349 sec/step)\n",
            "I0919 18:07:36.608685 140474388072320 learning.py:507] global step 4372: loss = 0.7442 (1.314 sec/step)\n",
            "I0919 18:07:37.921439 140474388072320 learning.py:507] global step 4373: loss = 0.9105 (1.311 sec/step)\n",
            "I0919 18:07:39.255850 140474388072320 learning.py:507] global step 4374: loss = 0.8728 (1.333 sec/step)\n",
            "I0919 18:07:40.550348 140474388072320 learning.py:507] global step 4375: loss = 0.8488 (1.293 sec/step)\n",
            "I0919 18:07:41.869193 140474388072320 learning.py:507] global step 4376: loss = 0.7251 (1.317 sec/step)\n",
            "I0919 18:07:43.193905 140474388072320 learning.py:507] global step 4377: loss = 0.8752 (1.323 sec/step)\n",
            "I0919 18:07:44.496288 140474388072320 learning.py:507] global step 4378: loss = 1.0432 (1.301 sec/step)\n",
            "I0919 18:07:45.841001 140474388072320 learning.py:507] global step 4379: loss = 0.9034 (1.343 sec/step)\n",
            "I0919 18:07:47.136055 140474388072320 learning.py:507] global step 4380: loss = 0.9734 (1.293 sec/step)\n",
            "I0919 18:07:48.425668 140474388072320 learning.py:507] global step 4381: loss = 0.7727 (1.288 sec/step)\n",
            "I0919 18:07:49.741327 140474388072320 learning.py:507] global step 4382: loss = 0.7492 (1.314 sec/step)\n",
            "I0919 18:07:51.057775 140474388072320 learning.py:507] global step 4383: loss = 0.8420 (1.315 sec/step)\n",
            "I0919 18:07:52.373197 140474388072320 learning.py:507] global step 4384: loss = 0.9752 (1.314 sec/step)\n",
            "I0919 18:07:53.741415 140474388072320 learning.py:507] global step 4385: loss = 0.7204 (1.366 sec/step)\n",
            "I0919 18:07:55.091929 140474388072320 learning.py:507] global step 4386: loss = 0.8292 (1.349 sec/step)\n",
            "I0919 18:07:56.407202 140474388072320 learning.py:507] global step 4387: loss = 0.7754 (1.313 sec/step)\n",
            "I0919 18:07:57.708679 140474388072320 learning.py:507] global step 4388: loss = 1.2777 (1.300 sec/step)\n",
            "I0919 18:07:59.049552 140474388072320 learning.py:507] global step 4389: loss = 1.0696 (1.339 sec/step)\n",
            "I0919 18:08:00.364897 140474388072320 learning.py:507] global step 4390: loss = 0.7822 (1.314 sec/step)\n",
            "I0919 18:08:01.689929 140474388072320 learning.py:507] global step 4391: loss = 0.8373 (1.321 sec/step)\n",
            "I0919 18:08:03.004897 140474388072320 learning.py:507] global step 4392: loss = 0.7519 (1.313 sec/step)\n",
            "I0919 18:08:04.332102 140474388072320 learning.py:507] global step 4393: loss = 0.8347 (1.326 sec/step)\n",
            "I0919 18:08:05.653389 140474388072320 learning.py:507] global step 4394: loss = 0.8495 (1.319 sec/step)\n",
            "I0919 18:08:06.948096 140474388072320 learning.py:507] global step 4395: loss = 1.1289 (1.293 sec/step)\n",
            "I0919 18:08:08.251195 140474388072320 learning.py:507] global step 4396: loss = 0.8235 (1.302 sec/step)\n",
            "I0919 18:08:09.558176 140474388072320 learning.py:507] global step 4397: loss = 0.8223 (1.305 sec/step)\n",
            "I0919 18:08:10.872356 140474388072320 learning.py:507] global step 4398: loss = 0.6789 (1.313 sec/step)\n",
            "I0919 18:08:12.182459 140474388072320 learning.py:507] global step 4399: loss = 0.8171 (1.308 sec/step)\n",
            "I0919 18:08:13.506174 140474388072320 learning.py:507] global step 4400: loss = 0.8503 (1.322 sec/step)\n",
            "I0919 18:08:14.805146 140474388072320 learning.py:507] global step 4401: loss = 0.8149 (1.297 sec/step)\n",
            "I0919 18:08:16.109749 140474388072320 learning.py:507] global step 4402: loss = 0.9883 (1.303 sec/step)\n",
            "I0919 18:08:17.403973 140474388072320 learning.py:507] global step 4403: loss = 0.8993 (1.292 sec/step)\n",
            "I0919 18:08:18.721602 140474388072320 learning.py:507] global step 4404: loss = 0.7406 (1.316 sec/step)\n",
            "I0919 18:08:20.052124 140474388072320 learning.py:507] global step 4405: loss = 1.5805 (1.329 sec/step)\n",
            "I0919 18:08:21.367553 140474388072320 learning.py:507] global step 4406: loss = 1.0122 (1.314 sec/step)\n",
            "I0919 18:08:22.698590 140474388072320 learning.py:507] global step 4407: loss = 1.0311 (1.329 sec/step)\n",
            "I0919 18:08:23.986644 140474388072320 learning.py:507] global step 4408: loss = 0.5997 (1.286 sec/step)\n",
            "I0919 18:08:25.300888 140474388072320 learning.py:507] global step 4409: loss = 1.0268 (1.313 sec/step)\n",
            "I0919 18:08:26.594440 140474388072320 learning.py:507] global step 4410: loss = 0.9217 (1.292 sec/step)\n",
            "I0919 18:08:28.843034 140471297337088 supervisor.py:1050] Recording summary at step 4411.\n",
            "I0919 18:08:28.848069 140474388072320 learning.py:507] global step 4411: loss = 0.9033 (2.252 sec/step)\n",
            "I0919 18:08:30.172873 140474388072320 learning.py:507] global step 4412: loss = 0.6586 (1.319 sec/step)\n",
            "I0919 18:08:31.478936 140474388072320 learning.py:507] global step 4413: loss = 0.7325 (1.304 sec/step)\n",
            "I0919 18:08:32.778460 140474388072320 learning.py:507] global step 4414: loss = 0.8152 (1.298 sec/step)\n",
            "I0919 18:08:34.106852 140474388072320 learning.py:507] global step 4415: loss = 0.8760 (1.327 sec/step)\n",
            "I0919 18:08:35.457387 140474388072320 learning.py:507] global step 4416: loss = 1.0489 (1.349 sec/step)\n",
            "I0919 18:08:36.749901 140474388072320 learning.py:507] global step 4417: loss = 0.7244 (1.291 sec/step)\n",
            "I0919 18:08:38.088046 140474388072320 learning.py:507] global step 4418: loss = 0.7159 (1.337 sec/step)\n",
            "I0919 18:08:39.422581 140474388072320 learning.py:507] global step 4419: loss = 0.7663 (1.333 sec/step)\n",
            "I0919 18:08:40.756967 140474388072320 learning.py:507] global step 4420: loss = 0.9107 (1.332 sec/step)\n",
            "I0919 18:08:42.063484 140474388072320 learning.py:507] global step 4421: loss = 1.0039 (1.305 sec/step)\n",
            "I0919 18:08:43.427726 140474388072320 learning.py:507] global step 4422: loss = 0.8008 (1.362 sec/step)\n",
            "I0919 18:08:44.780206 140474388072320 learning.py:507] global step 4423: loss = 0.7238 (1.350 sec/step)\n",
            "I0919 18:08:46.139352 140474388072320 learning.py:507] global step 4424: loss = 0.9937 (1.357 sec/step)\n",
            "I0919 18:08:47.472177 140474388072320 learning.py:507] global step 4425: loss = 0.7624 (1.331 sec/step)\n",
            "I0919 18:08:48.800427 140474388072320 learning.py:507] global step 4426: loss = 0.8016 (1.326 sec/step)\n",
            "I0919 18:08:50.100004 140474388072320 learning.py:507] global step 4427: loss = 0.9030 (1.298 sec/step)\n",
            "I0919 18:08:51.396533 140474388072320 learning.py:507] global step 4428: loss = 0.8370 (1.295 sec/step)\n",
            "I0919 18:08:52.720077 140474388072320 learning.py:507] global step 4429: loss = 1.4850 (1.322 sec/step)\n",
            "I0919 18:08:54.030236 140474388072320 learning.py:507] global step 4430: loss = 0.9624 (1.308 sec/step)\n",
            "I0919 18:08:55.334650 140474388072320 learning.py:507] global step 4431: loss = 0.9652 (1.303 sec/step)\n",
            "I0919 18:08:56.653584 140474388072320 learning.py:507] global step 4432: loss = 0.9367 (1.317 sec/step)\n",
            "I0919 18:08:57.953557 140474388072320 learning.py:507] global step 4433: loss = 0.6950 (1.298 sec/step)\n",
            "I0919 18:08:59.260438 140474388072320 learning.py:507] global step 4434: loss = 0.8375 (1.305 sec/step)\n",
            "I0919 18:09:00.618521 140474388072320 learning.py:507] global step 4435: loss = 0.7903 (1.356 sec/step)\n",
            "I0919 18:09:01.954286 140474388072320 learning.py:507] global step 4436: loss = 0.8853 (1.334 sec/step)\n",
            "I0919 18:09:03.260616 140474388072320 learning.py:507] global step 4437: loss = 1.1457 (1.305 sec/step)\n",
            "I0919 18:09:04.566647 140474388072320 learning.py:507] global step 4438: loss = 0.6638 (1.304 sec/step)\n",
            "I0919 18:09:05.910993 140474388072320 learning.py:507] global step 4439: loss = 0.8587 (1.343 sec/step)\n",
            "I0919 18:09:07.248702 140474388072320 learning.py:507] global step 4440: loss = 0.9321 (1.336 sec/step)\n",
            "I0919 18:09:08.545541 140474388072320 learning.py:507] global step 4441: loss = 0.7044 (1.295 sec/step)\n",
            "I0919 18:09:09.904407 140474388072320 learning.py:507] global step 4442: loss = 0.9238 (1.357 sec/step)\n",
            "I0919 18:09:11.237808 140474388072320 learning.py:507] global step 4443: loss = 0.8781 (1.332 sec/step)\n",
            "I0919 18:09:12.547626 140474388072320 learning.py:507] global step 4444: loss = 0.7005 (1.308 sec/step)\n",
            "I0919 18:09:13.867300 140474388072320 learning.py:507] global step 4445: loss = 0.7768 (1.318 sec/step)\n",
            "I0919 18:09:15.200705 140474388072320 learning.py:507] global step 4446: loss = 0.8046 (1.332 sec/step)\n",
            "I0919 18:09:16.531698 140474388072320 learning.py:507] global step 4447: loss = 0.9510 (1.329 sec/step)\n",
            "I0919 18:09:17.907316 140474388072320 learning.py:507] global step 4448: loss = 0.5829 (1.374 sec/step)\n",
            "I0919 18:09:19.227091 140474388072320 learning.py:507] global step 4449: loss = 0.8277 (1.318 sec/step)\n",
            "I0919 18:09:20.511274 140474388072320 learning.py:507] global step 4450: loss = 0.7890 (1.282 sec/step)\n",
            "I0919 18:09:21.784441 140474388072320 learning.py:507] global step 4451: loss = 1.4478 (1.271 sec/step)\n",
            "I0919 18:09:23.091782 140474388072320 learning.py:507] global step 4452: loss = 0.7727 (1.306 sec/step)\n",
            "I0919 18:09:24.410828 140474388072320 learning.py:507] global step 4453: loss = 0.8893 (1.317 sec/step)\n",
            "I0919 18:09:25.730237 140474388072320 learning.py:507] global step 4454: loss = 1.0247 (1.317 sec/step)\n",
            "I0919 18:09:27.031357 140474388072320 learning.py:507] global step 4455: loss = 0.8919 (1.299 sec/step)\n",
            "I0919 18:09:28.340069 140474388072320 learning.py:507] global step 4456: loss = 0.6385 (1.307 sec/step)\n",
            "I0919 18:09:29.650964 140474388072320 learning.py:507] global step 4457: loss = 0.8663 (1.309 sec/step)\n",
            "I0919 18:09:31.021460 140474388072320 learning.py:507] global step 4458: loss = 0.8239 (1.369 sec/step)\n",
            "I0919 18:09:32.293207 140474388072320 learning.py:507] global step 4459: loss = 0.9601 (1.270 sec/step)\n",
            "I0919 18:09:33.650504 140474388072320 learning.py:507] global step 4460: loss = 0.9063 (1.356 sec/step)\n",
            "I0919 18:09:34.955418 140474388072320 learning.py:507] global step 4461: loss = 0.7863 (1.303 sec/step)\n",
            "I0919 18:09:36.292810 140474388072320 learning.py:507] global step 4462: loss = 0.7726 (1.336 sec/step)\n",
            "I0919 18:09:37.596574 140474388072320 learning.py:507] global step 4463: loss = 0.8392 (1.302 sec/step)\n",
            "I0919 18:09:38.911092 140474388072320 learning.py:507] global step 4464: loss = 0.7880 (1.312 sec/step)\n",
            "I0919 18:09:40.278980 140474388072320 learning.py:507] global step 4465: loss = 0.9543 (1.366 sec/step)\n",
            "I0919 18:09:41.595210 140474388072320 learning.py:507] global step 4466: loss = 0.8607 (1.315 sec/step)\n",
            "I0919 18:09:42.929374 140474388072320 learning.py:507] global step 4467: loss = 0.8611 (1.332 sec/step)\n",
            "I0919 18:09:44.248846 140474388072320 learning.py:507] global step 4468: loss = 0.6985 (1.318 sec/step)\n",
            "I0919 18:09:45.538235 140474388072320 learning.py:507] global step 4469: loss = 0.9125 (1.288 sec/step)\n",
            "I0919 18:09:46.853224 140474388072320 learning.py:507] global step 4470: loss = 1.0354 (1.313 sec/step)\n",
            "I0919 18:09:48.172691 140474388072320 learning.py:507] global step 4471: loss = 0.7098 (1.318 sec/step)\n",
            "I0919 18:09:49.506099 140474388072320 learning.py:507] global step 4472: loss = 0.8156 (1.328 sec/step)\n",
            "I0919 18:09:50.863221 140474388072320 learning.py:507] global step 4473: loss = 0.7106 (1.353 sec/step)\n",
            "I0919 18:09:52.167222 140474388072320 learning.py:507] global step 4474: loss = 0.7625 (1.302 sec/step)\n",
            "I0919 18:09:53.475980 140474388072320 learning.py:507] global step 4475: loss = 0.8762 (1.307 sec/step)\n",
            "I0919 18:09:54.765986 140474388072320 learning.py:507] global step 4476: loss = 0.8350 (1.288 sec/step)\n",
            "I0919 18:09:56.048013 140474388072320 learning.py:507] global step 4477: loss = 0.8819 (1.280 sec/step)\n",
            "I0919 18:09:57.354316 140474388072320 learning.py:507] global step 4478: loss = 0.7612 (1.305 sec/step)\n",
            "I0919 18:09:58.693351 140474388072320 learning.py:507] global step 4479: loss = 1.0240 (1.337 sec/step)\n",
            "I0919 18:10:00.010966 140474388072320 learning.py:507] global step 4480: loss = 0.9462 (1.316 sec/step)\n",
            "I0919 18:10:01.367940 140474388072320 learning.py:507] global step 4481: loss = 0.6747 (1.355 sec/step)\n",
            "I0919 18:10:02.661484 140474388072320 learning.py:507] global step 4482: loss = 0.8411 (1.292 sec/step)\n",
            "I0919 18:10:03.959280 140474388072320 learning.py:507] global step 4483: loss = 1.1700 (1.296 sec/step)\n",
            "I0919 18:10:05.270253 140474388072320 learning.py:507] global step 4484: loss = 0.6419 (1.309 sec/step)\n",
            "I0919 18:10:06.563189 140474388072320 learning.py:507] global step 4485: loss = 1.0327 (1.291 sec/step)\n",
            "I0919 18:10:07.864221 140474388072320 learning.py:507] global step 4486: loss = 0.6894 (1.299 sec/step)\n",
            "I0919 18:10:09.180169 140474388072320 learning.py:507] global step 4487: loss = 0.7398 (1.314 sec/step)\n",
            "I0919 18:10:10.474252 140474388072320 learning.py:507] global step 4488: loss = 0.6288 (1.292 sec/step)\n",
            "I0919 18:10:11.799438 140474388072320 learning.py:507] global step 4489: loss = 0.9470 (1.324 sec/step)\n",
            "I0919 18:10:13.122589 140474388072320 learning.py:507] global step 4490: loss = 0.7248 (1.321 sec/step)\n",
            "I0919 18:10:14.457978 140474388072320 learning.py:507] global step 4491: loss = 0.8661 (1.333 sec/step)\n",
            "I0919 18:10:15.770606 140474388072320 learning.py:507] global step 4492: loss = 0.9355 (1.311 sec/step)\n",
            "I0919 18:10:17.083282 140474388072320 learning.py:507] global step 4493: loss = 0.8058 (1.311 sec/step)\n",
            "I0919 18:10:18.401534 140474388072320 learning.py:507] global step 4494: loss = 0.7303 (1.316 sec/step)\n",
            "I0919 18:10:19.699130 140474388072320 learning.py:507] global step 4495: loss = 1.0057 (1.296 sec/step)\n",
            "I0919 18:10:21.011315 140474388072320 learning.py:507] global step 4496: loss = 0.7639 (1.310 sec/step)\n",
            "I0919 18:10:22.309617 140474388072320 learning.py:507] global step 4497: loss = 0.8051 (1.296 sec/step)\n",
            "I0919 18:10:23.664261 140474388072320 learning.py:507] global step 4498: loss = 0.8694 (1.353 sec/step)\n",
            "I0919 18:10:25.023468 140474388072320 learning.py:507] global step 4499: loss = 0.7551 (1.357 sec/step)\n",
            "I0919 18:10:26.323904 140474388072320 learning.py:507] global step 4500: loss = 0.7026 (1.299 sec/step)\n",
            "I0919 18:10:26.872524 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 18:10:27.962985 140474388072320 learning.py:507] global step 4501: loss = 0.5987 (1.524 sec/step)\n",
            "I0919 18:10:30.262892 140471297337088 supervisor.py:1050] Recording summary at step 4501.\n",
            "I0919 18:10:30.625333 140474388072320 learning.py:507] global step 4502: loss = 0.7033 (2.552 sec/step)\n",
            "I0919 18:10:32.688839 140474388072320 learning.py:507] global step 4503: loss = 0.8537 (2.058 sec/step)\n",
            "I0919 18:10:34.295464 140474388072320 learning.py:507] global step 4504: loss = 0.7159 (1.605 sec/step)\n",
            "I0919 18:10:35.600744 140474388072320 learning.py:507] global step 4505: loss = 0.8730 (1.303 sec/step)\n",
            "I0919 18:10:36.921540 140474388072320 learning.py:507] global step 4506: loss = 0.8111 (1.319 sec/step)\n",
            "I0919 18:10:38.217664 140474388072320 learning.py:507] global step 4507: loss = 0.8044 (1.294 sec/step)\n",
            "I0919 18:10:39.545250 140474388072320 learning.py:507] global step 4508: loss = 0.7275 (1.326 sec/step)\n",
            "I0919 18:10:40.884578 140474388072320 learning.py:507] global step 4509: loss = 0.7349 (1.337 sec/step)\n",
            "I0919 18:10:42.198236 140474388072320 learning.py:507] global step 4510: loss = 0.9855 (1.312 sec/step)\n",
            "I0919 18:10:43.489845 140474388072320 learning.py:507] global step 4511: loss = 0.9786 (1.290 sec/step)\n",
            "I0919 18:10:44.808095 140474388072320 learning.py:507] global step 4512: loss = 0.8428 (1.316 sec/step)\n",
            "I0919 18:10:46.130497 140474388072320 learning.py:507] global step 4513: loss = 1.0624 (1.320 sec/step)\n",
            "I0919 18:10:47.483272 140474388072320 learning.py:507] global step 4514: loss = 0.7866 (1.351 sec/step)\n",
            "I0919 18:10:48.775271 140474388072320 learning.py:507] global step 4515: loss = 0.8940 (1.290 sec/step)\n",
            "I0919 18:10:50.067065 140474388072320 learning.py:507] global step 4516: loss = 1.1150 (1.290 sec/step)\n",
            "I0919 18:10:51.374191 140474388072320 learning.py:507] global step 4517: loss = 0.8654 (1.305 sec/step)\n",
            "I0919 18:10:52.714082 140474388072320 learning.py:507] global step 4518: loss = 0.7845 (1.338 sec/step)\n",
            "I0919 18:10:54.016587 140474388072320 learning.py:507] global step 4519: loss = 0.8098 (1.301 sec/step)\n",
            "I0919 18:10:55.379544 140474388072320 learning.py:507] global step 4520: loss = 0.9448 (1.361 sec/step)\n",
            "I0919 18:10:56.693001 140474388072320 learning.py:507] global step 4521: loss = 0.7002 (1.312 sec/step)\n",
            "I0919 18:10:57.999855 140474388072320 learning.py:507] global step 4522: loss = 0.8532 (1.305 sec/step)\n",
            "I0919 18:10:59.302647 140474388072320 learning.py:507] global step 4523: loss = 0.6978 (1.301 sec/step)\n",
            "I0919 18:11:00.636805 140474388072320 learning.py:507] global step 4524: loss = 1.1525 (1.332 sec/step)\n",
            "I0919 18:11:01.954256 140474388072320 learning.py:507] global step 4525: loss = 0.7668 (1.315 sec/step)\n",
            "I0919 18:11:03.234566 140474388072320 learning.py:507] global step 4526: loss = 0.6243 (1.279 sec/step)\n",
            "I0919 18:11:04.553272 140474388072320 learning.py:507] global step 4527: loss = 0.9657 (1.317 sec/step)\n",
            "I0919 18:11:05.869220 140474388072320 learning.py:507] global step 4528: loss = 0.6516 (1.314 sec/step)\n",
            "I0919 18:11:07.173362 140474388072320 learning.py:507] global step 4529: loss = 0.8083 (1.302 sec/step)\n",
            "I0919 18:11:08.502646 140474388072320 learning.py:507] global step 4530: loss = 1.0067 (1.328 sec/step)\n",
            "I0919 18:11:09.821064 140474388072320 learning.py:507] global step 4531: loss = 0.8832 (1.317 sec/step)\n",
            "I0919 18:11:11.178044 140474388072320 learning.py:507] global step 4532: loss = 0.8107 (1.355 sec/step)\n",
            "I0919 18:11:12.511598 140474388072320 learning.py:507] global step 4533: loss = 0.8209 (1.332 sec/step)\n",
            "I0919 18:11:13.795378 140474388072320 learning.py:507] global step 4534: loss = 0.7124 (1.282 sec/step)\n",
            "I0919 18:11:15.107028 140474388072320 learning.py:507] global step 4535: loss = 0.6213 (1.310 sec/step)\n",
            "I0919 18:11:16.430603 140474388072320 learning.py:507] global step 4536: loss = 0.7513 (1.322 sec/step)\n",
            "I0919 18:11:17.764904 140474388072320 learning.py:507] global step 4537: loss = 0.7519 (1.333 sec/step)\n",
            "I0919 18:11:19.112598 140474388072320 learning.py:507] global step 4538: loss = 0.9357 (1.344 sec/step)\n",
            "I0919 18:11:20.413866 140474388072320 learning.py:507] global step 4539: loss = 0.7905 (1.298 sec/step)\n",
            "I0919 18:11:21.750199 140474388072320 learning.py:507] global step 4540: loss = 0.8726 (1.334 sec/step)\n",
            "I0919 18:11:23.104820 140474388072320 learning.py:507] global step 4541: loss = 0.8249 (1.353 sec/step)\n",
            "I0919 18:11:24.403441 140474388072320 learning.py:507] global step 4542: loss = 0.8629 (1.297 sec/step)\n",
            "I0919 18:11:25.744334 140474388072320 learning.py:507] global step 4543: loss = 0.8702 (1.339 sec/step)\n",
            "I0919 18:11:27.063879 140474388072320 learning.py:507] global step 4544: loss = 1.0949 (1.318 sec/step)\n",
            "I0919 18:11:28.377312 140474388072320 learning.py:507] global step 4545: loss = 0.7842 (1.309 sec/step)\n",
            "I0919 18:11:29.695215 140474388072320 learning.py:507] global step 4546: loss = 0.7139 (1.316 sec/step)\n",
            "I0919 18:11:30.996812 140474388072320 learning.py:507] global step 4547: loss = 1.2747 (1.300 sec/step)\n",
            "I0919 18:11:32.290426 140474388072320 learning.py:507] global step 4548: loss = 0.8795 (1.292 sec/step)\n",
            "I0919 18:11:33.608571 140474388072320 learning.py:507] global step 4549: loss = 0.8769 (1.316 sec/step)\n",
            "I0919 18:11:34.959888 140474388072320 learning.py:507] global step 4550: loss = 0.7561 (1.350 sec/step)\n",
            "I0919 18:11:36.281516 140474388072320 learning.py:507] global step 4551: loss = 0.8413 (1.320 sec/step)\n",
            "I0919 18:11:37.574382 140474388072320 learning.py:507] global step 4552: loss = 0.8406 (1.291 sec/step)\n",
            "I0919 18:11:38.859235 140474388072320 learning.py:507] global step 4553: loss = 0.7774 (1.283 sec/step)\n",
            "I0919 18:11:40.174786 140474388072320 learning.py:507] global step 4554: loss = 0.8206 (1.314 sec/step)\n",
            "I0919 18:11:41.550894 140474388072320 learning.py:507] global step 4555: loss = 0.9656 (1.374 sec/step)\n",
            "I0919 18:11:42.885715 140474388072320 learning.py:507] global step 4556: loss = 0.8343 (1.333 sec/step)\n",
            "I0919 18:11:44.201006 140474388072320 learning.py:507] global step 4557: loss = 1.1056 (1.313 sec/step)\n",
            "I0919 18:11:45.522054 140474388072320 learning.py:507] global step 4558: loss = 0.7942 (1.319 sec/step)\n",
            "I0919 18:11:46.812864 140474388072320 learning.py:507] global step 4559: loss = 0.6974 (1.289 sec/step)\n",
            "I0919 18:11:48.165251 140474388072320 learning.py:507] global step 4560: loss = 0.7785 (1.351 sec/step)\n",
            "I0919 18:11:49.524568 140474388072320 learning.py:507] global step 4561: loss = 0.7611 (1.357 sec/step)\n",
            "I0919 18:11:50.826083 140474388072320 learning.py:507] global step 4562: loss = 1.1972 (1.300 sec/step)\n",
            "I0919 18:11:52.120524 140474388072320 learning.py:507] global step 4563: loss = 0.9907 (1.293 sec/step)\n",
            "I0919 18:11:53.426783 140474388072320 learning.py:507] global step 4564: loss = 1.1022 (1.305 sec/step)\n",
            "I0919 18:11:54.788028 140474388072320 learning.py:507] global step 4565: loss = 0.7026 (1.360 sec/step)\n",
            "I0919 18:11:56.116303 140474388072320 learning.py:507] global step 4566: loss = 0.8724 (1.327 sec/step)\n",
            "I0919 18:11:57.418654 140474388072320 learning.py:507] global step 4567: loss = 0.7752 (1.301 sec/step)\n",
            "I0919 18:11:58.743697 140474388072320 learning.py:507] global step 4568: loss = 0.8439 (1.324 sec/step)\n",
            "I0919 18:12:00.081183 140474388072320 learning.py:507] global step 4569: loss = 0.8182 (1.335 sec/step)\n",
            "I0919 18:12:01.406965 140474388072320 learning.py:507] global step 4570: loss = 1.0259 (1.324 sec/step)\n",
            "I0919 18:12:02.728213 140474388072320 learning.py:507] global step 4571: loss = 0.7676 (1.320 sec/step)\n",
            "I0919 18:12:04.039511 140474388072320 learning.py:507] global step 4572: loss = 0.8272 (1.310 sec/step)\n",
            "I0919 18:12:05.354650 140474388072320 learning.py:507] global step 4573: loss = 0.7818 (1.314 sec/step)\n",
            "I0919 18:12:06.647737 140474388072320 learning.py:507] global step 4574: loss = 0.9368 (1.291 sec/step)\n",
            "I0919 18:12:07.972584 140474388072320 learning.py:507] global step 4575: loss = 0.9986 (1.323 sec/step)\n",
            "I0919 18:12:09.291186 140474388072320 learning.py:507] global step 4576: loss = 0.6706 (1.317 sec/step)\n",
            "I0919 18:12:10.599037 140474388072320 learning.py:507] global step 4577: loss = 0.7232 (1.306 sec/step)\n",
            "I0919 18:12:11.904956 140474388072320 learning.py:507] global step 4578: loss = 0.8760 (1.304 sec/step)\n",
            "I0919 18:12:13.216454 140474388072320 learning.py:507] global step 4579: loss = 0.8300 (1.310 sec/step)\n",
            "I0919 18:12:14.523603 140474388072320 learning.py:507] global step 4580: loss = 0.7710 (1.305 sec/step)\n",
            "I0919 18:12:15.833364 140474388072320 learning.py:507] global step 4581: loss = 0.7969 (1.308 sec/step)\n",
            "I0919 18:12:17.159070 140474388072320 learning.py:507] global step 4582: loss = 0.8400 (1.324 sec/step)\n",
            "I0919 18:12:18.478873 140474388072320 learning.py:507] global step 4583: loss = 0.7812 (1.318 sec/step)\n",
            "I0919 18:12:19.784296 140474388072320 learning.py:507] global step 4584: loss = 0.9504 (1.304 sec/step)\n",
            "I0919 18:12:21.088419 140474388072320 learning.py:507] global step 4585: loss = 0.8310 (1.302 sec/step)\n",
            "I0919 18:12:22.394038 140474388072320 learning.py:507] global step 4586: loss = 0.8240 (1.304 sec/step)\n",
            "I0919 18:12:23.722954 140474388072320 learning.py:507] global step 4587: loss = 0.8375 (1.327 sec/step)\n",
            "I0919 18:12:25.031776 140474388072320 learning.py:507] global step 4588: loss = 0.8419 (1.307 sec/step)\n",
            "I0919 18:12:26.353394 140474388072320 learning.py:507] global step 4589: loss = 0.9192 (1.320 sec/step)\n",
            "I0919 18:12:28.429831 140474388072320 learning.py:507] global step 4590: loss = 0.8348 (2.072 sec/step)\n",
            "I0919 18:12:28.457185 140471297337088 supervisor.py:1050] Recording summary at step 4590.\n",
            "I0919 18:12:29.796078 140474388072320 learning.py:507] global step 4591: loss = 0.8926 (1.364 sec/step)\n",
            "I0919 18:12:31.096790 140474388072320 learning.py:507] global step 4592: loss = 1.0088 (1.299 sec/step)\n",
            "I0919 18:12:32.456034 140474388072320 learning.py:507] global step 4593: loss = 0.8465 (1.357 sec/step)\n",
            "I0919 18:12:33.759934 140474388072320 learning.py:507] global step 4594: loss = 1.0529 (1.302 sec/step)\n",
            "I0919 18:12:35.068156 140474388072320 learning.py:507] global step 4595: loss = 0.8027 (1.307 sec/step)\n",
            "I0919 18:12:36.418343 140474388072320 learning.py:507] global step 4596: loss = 1.0103 (1.349 sec/step)\n",
            "I0919 18:12:37.721887 140474388072320 learning.py:507] global step 4597: loss = 0.8345 (1.301 sec/step)\n",
            "I0919 18:12:39.106539 140474388072320 learning.py:507] global step 4598: loss = 0.8767 (1.383 sec/step)\n",
            "I0919 18:12:40.437072 140474388072320 learning.py:507] global step 4599: loss = 0.9280 (1.329 sec/step)\n",
            "I0919 18:12:41.749624 140474388072320 learning.py:507] global step 4600: loss = 0.8502 (1.310 sec/step)\n",
            "I0919 18:12:43.059839 140474388072320 learning.py:507] global step 4601: loss = 0.7092 (1.308 sec/step)\n",
            "I0919 18:12:44.444960 140474388072320 learning.py:507] global step 4602: loss = 0.9433 (1.383 sec/step)\n",
            "I0919 18:12:45.740668 140474388072320 learning.py:507] global step 4603: loss = 0.8164 (1.294 sec/step)\n",
            "I0919 18:12:47.087774 140474388072320 learning.py:507] global step 4604: loss = 0.7312 (1.345 sec/step)\n",
            "I0919 18:12:48.447974 140474388072320 learning.py:507] global step 4605: loss = 0.9673 (1.358 sec/step)\n",
            "I0919 18:12:49.758848 140474388072320 learning.py:507] global step 4606: loss = 0.6475 (1.309 sec/step)\n",
            "I0919 18:12:51.040256 140474388072320 learning.py:507] global step 4607: loss = 0.9763 (1.279 sec/step)\n",
            "I0919 18:12:52.353703 140474388072320 learning.py:507] global step 4608: loss = 0.6882 (1.312 sec/step)\n",
            "I0919 18:12:53.660090 140474388072320 learning.py:507] global step 4609: loss = 0.8310 (1.304 sec/step)\n",
            "I0919 18:12:54.955096 140474388072320 learning.py:507] global step 4610: loss = 0.9122 (1.293 sec/step)\n",
            "I0919 18:12:56.248342 140474388072320 learning.py:507] global step 4611: loss = 1.2625 (1.291 sec/step)\n",
            "I0919 18:12:57.591444 140474388072320 learning.py:507] global step 4612: loss = 0.8818 (1.341 sec/step)\n",
            "I0919 18:12:58.911097 140474388072320 learning.py:507] global step 4613: loss = 0.8026 (1.318 sec/step)\n",
            "I0919 18:13:00.251297 140474388072320 learning.py:507] global step 4614: loss = 0.7240 (1.338 sec/step)\n",
            "I0919 18:13:01.572233 140474388072320 learning.py:507] global step 4615: loss = 0.9068 (1.319 sec/step)\n",
            "I0919 18:13:02.877501 140474388072320 learning.py:507] global step 4616: loss = 0.9662 (1.304 sec/step)\n",
            "I0919 18:13:04.220577 140474388072320 learning.py:507] global step 4617: loss = 0.7293 (1.341 sec/step)\n",
            "I0919 18:13:05.562939 140474388072320 learning.py:507] global step 4618: loss = 0.7184 (1.341 sec/step)\n",
            "I0919 18:13:06.931196 140474388072320 learning.py:507] global step 4619: loss = 1.0184 (1.367 sec/step)\n",
            "I0919 18:13:08.302001 140474388072320 learning.py:507] global step 4620: loss = 0.9986 (1.369 sec/step)\n",
            "I0919 18:13:09.630656 140474388072320 learning.py:507] global step 4621: loss = 0.8561 (1.327 sec/step)\n",
            "I0919 18:13:10.944062 140474388072320 learning.py:507] global step 4622: loss = 0.7963 (1.308 sec/step)\n",
            "I0919 18:13:12.306016 140474388072320 learning.py:507] global step 4623: loss = 0.6675 (1.357 sec/step)\n",
            "I0919 18:13:13.611920 140474388072320 learning.py:507] global step 4624: loss = 0.7771 (1.303 sec/step)\n",
            "I0919 18:13:14.934226 140474388072320 learning.py:507] global step 4625: loss = 0.9372 (1.320 sec/step)\n",
            "I0919 18:13:16.221798 140474388072320 learning.py:507] global step 4626: loss = 0.7971 (1.286 sec/step)\n",
            "I0919 18:13:17.512444 140474388072320 learning.py:507] global step 4627: loss = 0.7003 (1.289 sec/step)\n",
            "I0919 18:13:18.820302 140474388072320 learning.py:507] global step 4628: loss = 1.0795 (1.306 sec/step)\n",
            "I0919 18:13:20.137007 140474388072320 learning.py:507] global step 4629: loss = 0.7456 (1.315 sec/step)\n",
            "I0919 18:13:21.450992 140474388072320 learning.py:507] global step 4630: loss = 0.8524 (1.312 sec/step)\n",
            "I0919 18:13:22.760920 140474388072320 learning.py:507] global step 4631: loss = 0.9047 (1.308 sec/step)\n",
            "I0919 18:13:24.069231 140474388072320 learning.py:507] global step 4632: loss = 0.6910 (1.307 sec/step)\n",
            "I0919 18:13:25.397412 140474388072320 learning.py:507] global step 4633: loss = 0.9754 (1.327 sec/step)\n",
            "I0919 18:13:26.774152 140474388072320 learning.py:507] global step 4634: loss = 0.8874 (1.375 sec/step)\n",
            "I0919 18:13:28.104603 140474388072320 learning.py:507] global step 4635: loss = 0.6903 (1.329 sec/step)\n",
            "I0919 18:13:29.438586 140474388072320 learning.py:507] global step 4636: loss = 1.0635 (1.332 sec/step)\n",
            "I0919 18:13:30.760493 140474388072320 learning.py:507] global step 4637: loss = 0.9767 (1.320 sec/step)\n",
            "I0919 18:13:32.080846 140474388072320 learning.py:507] global step 4638: loss = 0.8922 (1.318 sec/step)\n",
            "I0919 18:13:33.405801 140474388072320 learning.py:507] global step 4639: loss = 0.6798 (1.323 sec/step)\n",
            "I0919 18:13:34.764813 140474388072320 learning.py:507] global step 4640: loss = 0.8938 (1.357 sec/step)\n",
            "I0919 18:13:36.085500 140474388072320 learning.py:507] global step 4641: loss = 0.6785 (1.318 sec/step)\n",
            "I0919 18:13:37.428244 140474388072320 learning.py:507] global step 4642: loss = 0.7921 (1.341 sec/step)\n",
            "I0919 18:13:38.731631 140474388072320 learning.py:507] global step 4643: loss = 0.7813 (1.301 sec/step)\n",
            "I0919 18:13:40.045555 140474388072320 learning.py:507] global step 4644: loss = 1.1154 (1.312 sec/step)\n",
            "I0919 18:13:41.370282 140474388072320 learning.py:507] global step 4645: loss = 0.6304 (1.323 sec/step)\n",
            "I0919 18:13:42.685811 140474388072320 learning.py:507] global step 4646: loss = 0.7658 (1.314 sec/step)\n",
            "I0919 18:13:44.034753 140474388072320 learning.py:507] global step 4647: loss = 0.7197 (1.347 sec/step)\n",
            "I0919 18:13:45.382031 140474388072320 learning.py:507] global step 4648: loss = 0.7934 (1.345 sec/step)\n",
            "I0919 18:13:46.704906 140474388072320 learning.py:507] global step 4649: loss = 0.8123 (1.321 sec/step)\n",
            "I0919 18:13:47.992603 140474388072320 learning.py:507] global step 4650: loss = 0.7628 (1.286 sec/step)\n",
            "I0919 18:13:49.307199 140474388072320 learning.py:507] global step 4651: loss = 1.4882 (1.313 sec/step)\n",
            "I0919 18:13:50.612773 140474388072320 learning.py:507] global step 4652: loss = 0.7965 (1.304 sec/step)\n",
            "I0919 18:13:51.934209 140474388072320 learning.py:507] global step 4653: loss = 0.8509 (1.319 sec/step)\n",
            "I0919 18:13:53.244355 140474388072320 learning.py:507] global step 4654: loss = 0.9291 (1.308 sec/step)\n",
            "I0919 18:13:54.563411 140474388072320 learning.py:507] global step 4655: loss = 0.9898 (1.317 sec/step)\n",
            "I0919 18:13:55.923979 140474388072320 learning.py:507] global step 4656: loss = 0.7662 (1.358 sec/step)\n",
            "I0919 18:13:57.250406 140474388072320 learning.py:507] global step 4657: loss = 0.6633 (1.324 sec/step)\n",
            "I0919 18:13:58.601090 140474388072320 learning.py:507] global step 4658: loss = 0.8146 (1.349 sec/step)\n",
            "I0919 18:13:59.913821 140474388072320 learning.py:507] global step 4659: loss = 0.9809 (1.311 sec/step)\n",
            "I0919 18:14:01.233007 140474388072320 learning.py:507] global step 4660: loss = 0.8678 (1.317 sec/step)\n",
            "I0919 18:14:02.542273 140474388072320 learning.py:507] global step 4661: loss = 0.7788 (1.308 sec/step)\n",
            "I0919 18:14:03.885852 140474388072320 learning.py:507] global step 4662: loss = 0.7888 (1.342 sec/step)\n",
            "I0919 18:14:05.181519 140474388072320 learning.py:507] global step 4663: loss = 0.8833 (1.294 sec/step)\n",
            "I0919 18:14:06.530060 140474388072320 learning.py:507] global step 4664: loss = 0.9139 (1.347 sec/step)\n",
            "I0919 18:14:07.861630 140474388072320 learning.py:507] global step 4665: loss = 0.9287 (1.330 sec/step)\n",
            "I0919 18:14:09.152103 140474388072320 learning.py:507] global step 4666: loss = 0.9331 (1.288 sec/step)\n",
            "I0919 18:14:10.481705 140474388072320 learning.py:507] global step 4667: loss = 1.0568 (1.328 sec/step)\n",
            "I0919 18:14:11.805994 140474388072320 learning.py:507] global step 4668: loss = 0.8566 (1.322 sec/step)\n",
            "I0919 18:14:13.100286 140474388072320 learning.py:507] global step 4669: loss = 0.8188 (1.293 sec/step)\n",
            "I0919 18:14:14.438872 140474388072320 learning.py:507] global step 4670: loss = 1.2772 (1.337 sec/step)\n",
            "I0919 18:14:15.749579 140474388072320 learning.py:507] global step 4671: loss = 0.8479 (1.309 sec/step)\n",
            "I0919 18:14:17.030414 140474388072320 learning.py:507] global step 4672: loss = 0.9723 (1.279 sec/step)\n",
            "I0919 18:14:18.356187 140474388072320 learning.py:507] global step 4673: loss = 0.7866 (1.324 sec/step)\n",
            "I0919 18:14:19.676666 140474388072320 learning.py:507] global step 4674: loss = 0.7274 (1.319 sec/step)\n",
            "I0919 18:14:20.988368 140474388072320 learning.py:507] global step 4675: loss = 0.8067 (1.310 sec/step)\n",
            "I0919 18:14:22.346354 140474388072320 learning.py:507] global step 4676: loss = 1.1075 (1.356 sec/step)\n",
            "I0919 18:14:23.661051 140474388072320 learning.py:507] global step 4677: loss = 0.9477 (1.313 sec/step)\n",
            "I0919 18:14:24.986979 140474388072320 learning.py:507] global step 4678: loss = 0.7238 (1.324 sec/step)\n",
            "I0919 18:14:26.310596 140474388072320 learning.py:507] global step 4679: loss = 0.7999 (1.322 sec/step)\n",
            "I0919 18:14:28.483575 140474388072320 learning.py:507] global step 4680: loss = 0.6797 (2.170 sec/step)\n",
            "I0919 18:14:28.506180 140471297337088 supervisor.py:1050] Recording summary at step 4680.\n",
            "I0919 18:14:29.818034 140474388072320 learning.py:507] global step 4681: loss = 0.8301 (1.333 sec/step)\n",
            "I0919 18:14:31.136502 140474388072320 learning.py:507] global step 4682: loss = 0.6764 (1.317 sec/step)\n",
            "I0919 18:14:32.430984 140474388072320 learning.py:507] global step 4683: loss = 1.1848 (1.293 sec/step)\n",
            "I0919 18:14:33.727965 140474388072320 learning.py:507] global step 4684: loss = 0.8275 (1.295 sec/step)\n",
            "I0919 18:14:35.066406 140474388072320 learning.py:507] global step 4685: loss = 0.7509 (1.337 sec/step)\n",
            "I0919 18:14:36.372889 140474388072320 learning.py:507] global step 4686: loss = 0.7610 (1.305 sec/step)\n",
            "I0919 18:14:37.687364 140474388072320 learning.py:507] global step 4687: loss = 0.7577 (1.313 sec/step)\n",
            "I0919 18:14:38.988558 140474388072320 learning.py:507] global step 4688: loss = 0.7973 (1.300 sec/step)\n",
            "I0919 18:14:40.290673 140474388072320 learning.py:507] global step 4689: loss = 1.0701 (1.300 sec/step)\n",
            "I0919 18:14:41.637562 140474388072320 learning.py:507] global step 4690: loss = 0.8415 (1.345 sec/step)\n",
            "I0919 18:14:42.964746 140474388072320 learning.py:507] global step 4691: loss = 0.8852 (1.325 sec/step)\n",
            "I0919 18:14:44.252319 140474388072320 learning.py:507] global step 4692: loss = 0.6892 (1.286 sec/step)\n",
            "I0919 18:14:45.596836 140474388072320 learning.py:507] global step 4693: loss = 0.7218 (1.343 sec/step)\n",
            "I0919 18:14:46.957742 140474388072320 learning.py:507] global step 4694: loss = 0.7139 (1.359 sec/step)\n",
            "I0919 18:14:48.244740 140474388072320 learning.py:507] global step 4695: loss = 0.8165 (1.285 sec/step)\n",
            "I0919 18:14:49.612062 140474388072320 learning.py:507] global step 4696: loss = 0.9480 (1.366 sec/step)\n",
            "I0919 18:14:50.920058 140474388072320 learning.py:507] global step 4697: loss = 0.7590 (1.306 sec/step)\n",
            "I0919 18:14:52.260020 140474388072320 learning.py:507] global step 4698: loss = 0.8341 (1.338 sec/step)\n",
            "I0919 18:14:53.577821 140474388072320 learning.py:507] global step 4699: loss = 0.8863 (1.316 sec/step)\n",
            "I0919 18:14:54.875447 140474388072320 learning.py:507] global step 4700: loss = 0.8523 (1.296 sec/step)\n",
            "I0919 18:14:56.195652 140474388072320 learning.py:507] global step 4701: loss = 0.8862 (1.318 sec/step)\n",
            "I0919 18:14:57.525509 140474388072320 learning.py:507] global step 4702: loss = 0.8455 (1.328 sec/step)\n",
            "I0919 18:14:58.832030 140474388072320 learning.py:507] global step 4703: loss = 1.0280 (1.305 sec/step)\n",
            "I0919 18:15:00.138705 140474388072320 learning.py:507] global step 4704: loss = 0.8533 (1.304 sec/step)\n",
            "I0919 18:15:01.501682 140474388072320 learning.py:507] global step 4705: loss = 0.9600 (1.361 sec/step)\n",
            "I0919 18:15:02.820900 140474388072320 learning.py:507] global step 4706: loss = 0.7667 (1.317 sec/step)\n",
            "I0919 18:15:04.150836 140474388072320 learning.py:507] global step 4707: loss = 0.9358 (1.328 sec/step)\n",
            "I0919 18:15:05.466824 140474388072320 learning.py:507] global step 4708: loss = 0.8754 (1.314 sec/step)\n",
            "I0919 18:15:06.832897 140474388072320 learning.py:507] global step 4709: loss = 0.7376 (1.364 sec/step)\n",
            "I0919 18:15:08.125884 140474388072320 learning.py:507] global step 4710: loss = 0.9811 (1.291 sec/step)\n",
            "I0919 18:15:09.470002 140474388072320 learning.py:507] global step 4711: loss = 0.7601 (1.342 sec/step)\n",
            "I0919 18:15:10.764436 140474388072320 learning.py:507] global step 4712: loss = 0.6855 (1.293 sec/step)\n",
            "I0919 18:15:12.045471 140474388072320 learning.py:507] global step 4713: loss = 0.8083 (1.279 sec/step)\n",
            "I0919 18:15:13.334640 140474388072320 learning.py:507] global step 4714: loss = 0.8954 (1.287 sec/step)\n",
            "I0919 18:15:14.627250 140474388072320 learning.py:507] global step 4715: loss = 1.0167 (1.291 sec/step)\n",
            "I0919 18:15:15.969826 140474388072320 learning.py:507] global step 4716: loss = 1.4177 (1.341 sec/step)\n",
            "I0919 18:15:17.314728 140474388072320 learning.py:507] global step 4717: loss = 0.9425 (1.343 sec/step)\n",
            "I0919 18:15:18.633194 140474388072320 learning.py:507] global step 4718: loss = 0.7109 (1.317 sec/step)\n",
            "I0919 18:15:19.955024 140474388072320 learning.py:507] global step 4719: loss = 0.8406 (1.316 sec/step)\n",
            "I0919 18:15:21.278522 140474388072320 learning.py:507] global step 4720: loss = 1.1641 (1.322 sec/step)\n",
            "I0919 18:15:22.569027 140474388072320 learning.py:507] global step 4721: loss = 1.1494 (1.289 sec/step)\n",
            "I0919 18:15:23.868367 140474388072320 learning.py:507] global step 4722: loss = 0.9234 (1.298 sec/step)\n",
            "I0919 18:15:25.196422 140474388072320 learning.py:507] global step 4723: loss = 0.7622 (1.326 sec/step)\n",
            "I0919 18:15:26.525907 140474388072320 learning.py:507] global step 4724: loss = 0.7950 (1.328 sec/step)\n",
            "I0919 18:15:27.844006 140474388072320 learning.py:507] global step 4725: loss = 0.7673 (1.316 sec/step)\n",
            "I0919 18:15:29.144618 140474388072320 learning.py:507] global step 4726: loss = 0.7724 (1.299 sec/step)\n",
            "I0919 18:15:30.428570 140474388072320 learning.py:507] global step 4727: loss = 0.8540 (1.282 sec/step)\n",
            "I0919 18:15:31.781866 140474388072320 learning.py:507] global step 4728: loss = 0.8420 (1.351 sec/step)\n",
            "I0919 18:15:33.092643 140474388072320 learning.py:507] global step 4729: loss = 0.8553 (1.309 sec/step)\n",
            "I0919 18:15:34.387274 140474388072320 learning.py:507] global step 4730: loss = 0.7823 (1.293 sec/step)\n",
            "I0919 18:15:35.682162 140474388072320 learning.py:507] global step 4731: loss = 0.7145 (1.293 sec/step)\n",
            "I0919 18:15:37.008933 140474388072320 learning.py:507] global step 4732: loss = 0.7230 (1.325 sec/step)\n",
            "I0919 18:15:38.297157 140474388072320 learning.py:507] global step 4733: loss = 0.7946 (1.286 sec/step)\n",
            "I0919 18:15:39.620151 140474388072320 learning.py:507] global step 4734: loss = 0.9308 (1.321 sec/step)\n",
            "I0919 18:15:40.904840 140474388072320 learning.py:507] global step 4735: loss = 1.0006 (1.283 sec/step)\n",
            "I0919 18:15:42.232018 140474388072320 learning.py:507] global step 4736: loss = 1.0130 (1.325 sec/step)\n",
            "I0919 18:15:43.536323 140474388072320 learning.py:507] global step 4737: loss = 0.8242 (1.302 sec/step)\n",
            "I0919 18:15:44.872826 140474388072320 learning.py:507] global step 4738: loss = 0.7935 (1.335 sec/step)\n",
            "I0919 18:15:46.199615 140474388072320 learning.py:507] global step 4739: loss = 1.0681 (1.325 sec/step)\n",
            "I0919 18:15:47.524337 140474388072320 learning.py:507] global step 4740: loss = 0.6693 (1.323 sec/step)\n",
            "I0919 18:15:48.854404 140474388072320 learning.py:507] global step 4741: loss = 0.7132 (1.328 sec/step)\n",
            "I0919 18:15:50.174154 140474388072320 learning.py:507] global step 4742: loss = 0.7810 (1.318 sec/step)\n",
            "I0919 18:15:51.484735 140474388072320 learning.py:507] global step 4743: loss = 1.0149 (1.309 sec/step)\n",
            "I0919 18:15:52.836047 140474388072320 learning.py:507] global step 4744: loss = 1.1185 (1.350 sec/step)\n",
            "I0919 18:15:54.172668 140474388072320 learning.py:507] global step 4745: loss = 0.7328 (1.334 sec/step)\n",
            "I0919 18:15:55.493524 140474388072320 learning.py:507] global step 4746: loss = 0.8131 (1.319 sec/step)\n",
            "I0919 18:15:56.812462 140474388072320 learning.py:507] global step 4747: loss = 0.8960 (1.317 sec/step)\n",
            "I0919 18:15:58.144826 140474388072320 learning.py:507] global step 4748: loss = 0.8490 (1.330 sec/step)\n",
            "I0919 18:15:59.458782 140474388072320 learning.py:507] global step 4749: loss = 0.7722 (1.312 sec/step)\n",
            "I0919 18:16:00.781943 140474388072320 learning.py:507] global step 4750: loss = 0.8085 (1.322 sec/step)\n",
            "I0919 18:16:02.084806 140474388072320 learning.py:507] global step 4751: loss = 0.7876 (1.301 sec/step)\n",
            "I0919 18:16:03.422259 140474388072320 learning.py:507] global step 4752: loss = 1.2100 (1.335 sec/step)\n",
            "I0919 18:16:04.757457 140474388072320 learning.py:507] global step 4753: loss = 1.0730 (1.333 sec/step)\n",
            "I0919 18:16:06.066999 140474388072320 learning.py:507] global step 4754: loss = 1.1085 (1.308 sec/step)\n",
            "I0919 18:16:07.380466 140474388072320 learning.py:507] global step 4755: loss = 0.7352 (1.312 sec/step)\n",
            "I0919 18:16:08.735275 140474388072320 learning.py:507] global step 4756: loss = 0.9011 (1.353 sec/step)\n",
            "I0919 18:16:10.062572 140474388072320 learning.py:507] global step 4757: loss = 0.9486 (1.325 sec/step)\n",
            "I0919 18:16:11.368218 140474388072320 learning.py:507] global step 4758: loss = 0.6245 (1.304 sec/step)\n",
            "I0919 18:16:12.704854 140474388072320 learning.py:507] global step 4759: loss = 0.7747 (1.335 sec/step)\n",
            "I0919 18:16:14.031661 140474388072320 learning.py:507] global step 4760: loss = 0.8301 (1.325 sec/step)\n",
            "I0919 18:16:15.351216 140474388072320 learning.py:507] global step 4761: loss = 0.9060 (1.318 sec/step)\n",
            "I0919 18:16:16.658601 140474388072320 learning.py:507] global step 4762: loss = 0.7508 (1.306 sec/step)\n",
            "I0919 18:16:17.977315 140474388072320 learning.py:507] global step 4763: loss = 0.8595 (1.317 sec/step)\n",
            "I0919 18:16:19.266046 140474388072320 learning.py:507] global step 4764: loss = 1.0201 (1.287 sec/step)\n",
            "I0919 18:16:20.545469 140474388072320 learning.py:507] global step 4765: loss = 0.9117 (1.278 sec/step)\n",
            "I0919 18:16:21.849081 140474388072320 learning.py:507] global step 4766: loss = 1.0523 (1.302 sec/step)\n",
            "I0919 18:16:23.150494 140474388072320 learning.py:507] global step 4767: loss = 0.7787 (1.300 sec/step)\n",
            "I0919 18:16:24.457684 140474388072320 learning.py:507] global step 4768: loss = 0.9045 (1.306 sec/step)\n",
            "I0919 18:16:25.765081 140474388072320 learning.py:507] global step 4769: loss = 0.7663 (1.306 sec/step)\n",
            "I0919 18:16:27.086692 140474388072320 learning.py:507] global step 4770: loss = 0.8787 (1.320 sec/step)\n",
            "I0919 18:16:28.841794 140471297337088 supervisor.py:1050] Recording summary at step 4770.\n",
            "I0919 18:16:29.308782 140474388072320 learning.py:507] global step 4771: loss = 0.7013 (2.219 sec/step)\n",
            "I0919 18:16:30.686274 140474388072320 learning.py:507] global step 4772: loss = 0.7914 (1.376 sec/step)\n",
            "I0919 18:16:31.983041 140474388072320 learning.py:507] global step 4773: loss = 0.8325 (1.295 sec/step)\n",
            "I0919 18:16:33.272743 140474388072320 learning.py:507] global step 4774: loss = 0.9605 (1.287 sec/step)\n",
            "I0919 18:16:34.571317 140474388072320 learning.py:507] global step 4775: loss = 0.7917 (1.296 sec/step)\n",
            "I0919 18:16:35.899775 140474388072320 learning.py:507] global step 4776: loss = 0.8553 (1.327 sec/step)\n",
            "I0919 18:16:37.215470 140474388072320 learning.py:507] global step 4777: loss = 0.6505 (1.314 sec/step)\n",
            "I0919 18:16:38.527539 140474388072320 learning.py:507] global step 4778: loss = 0.7782 (1.310 sec/step)\n",
            "I0919 18:16:39.907768 140474388072320 learning.py:507] global step 4779: loss = 0.7111 (1.378 sec/step)\n",
            "I0919 18:16:41.263881 140474388072320 learning.py:507] global step 4780: loss = 0.8192 (1.354 sec/step)\n",
            "I0919 18:16:42.569288 140474388072320 learning.py:507] global step 4781: loss = 0.8445 (1.304 sec/step)\n",
            "I0919 18:16:43.880079 140474388072320 learning.py:507] global step 4782: loss = 0.8531 (1.309 sec/step)\n",
            "I0919 18:16:45.215989 140474388072320 learning.py:507] global step 4783: loss = 0.8993 (1.334 sec/step)\n",
            "I0919 18:16:46.530881 140474388072320 learning.py:507] global step 4784: loss = 0.6750 (1.313 sec/step)\n",
            "I0919 18:16:47.865696 140474388072320 learning.py:507] global step 4785: loss = 0.9111 (1.333 sec/step)\n",
            "I0919 18:16:49.172346 140474388072320 learning.py:507] global step 4786: loss = 0.9197 (1.305 sec/step)\n",
            "I0919 18:16:50.500936 140474388072320 learning.py:507] global step 4787: loss = 1.0870 (1.327 sec/step)\n",
            "I0919 18:16:51.824625 140474388072320 learning.py:507] global step 4788: loss = 0.7644 (1.322 sec/step)\n",
            "I0919 18:16:53.181444 140474388072320 learning.py:507] global step 4789: loss = 0.7771 (1.355 sec/step)\n",
            "I0919 18:16:54.500606 140474388072320 learning.py:507] global step 4790: loss = 0.7552 (1.317 sec/step)\n",
            "I0919 18:16:55.809684 140474388072320 learning.py:507] global step 4791: loss = 0.7134 (1.307 sec/step)\n",
            "I0919 18:16:57.105647 140474388072320 learning.py:507] global step 4792: loss = 0.8977 (1.294 sec/step)\n",
            "I0919 18:16:58.437711 140474388072320 learning.py:507] global step 4793: loss = 0.7126 (1.330 sec/step)\n",
            "I0919 18:16:59.775559 140474388072320 learning.py:507] global step 4794: loss = 0.8019 (1.336 sec/step)\n",
            "I0919 18:17:01.116604 140474388072320 learning.py:507] global step 4795: loss = 0.8119 (1.339 sec/step)\n",
            "I0919 18:17:02.424911 140474388072320 learning.py:507] global step 4796: loss = 0.7138 (1.306 sec/step)\n",
            "I0919 18:17:03.735673 140474388072320 learning.py:507] global step 4797: loss = 1.6659 (1.309 sec/step)\n",
            "I0919 18:17:05.054368 140474388072320 learning.py:507] global step 4798: loss = 0.8931 (1.317 sec/step)\n",
            "I0919 18:17:06.353395 140474388072320 learning.py:507] global step 4799: loss = 0.7934 (1.297 sec/step)\n",
            "I0919 18:17:07.652731 140474388072320 learning.py:507] global step 4800: loss = 1.0002 (1.298 sec/step)\n",
            "I0919 18:17:08.968978 140474388072320 learning.py:507] global step 4801: loss = 0.9985 (1.315 sec/step)\n",
            "I0919 18:17:10.292737 140474388072320 learning.py:507] global step 4802: loss = 0.7358 (1.322 sec/step)\n",
            "I0919 18:17:11.643806 140474388072320 learning.py:507] global step 4803: loss = 1.0849 (1.349 sec/step)\n",
            "I0919 18:17:12.933651 140474388072320 learning.py:507] global step 4804: loss = 0.8416 (1.288 sec/step)\n",
            "I0919 18:17:14.297552 140474388072320 learning.py:507] global step 4805: loss = 0.7064 (1.362 sec/step)\n",
            "I0919 18:17:15.636703 140474388072320 learning.py:507] global step 4806: loss = 0.7576 (1.337 sec/step)\n",
            "I0919 18:17:16.944510 140474388072320 learning.py:507] global step 4807: loss = 0.8224 (1.306 sec/step)\n",
            "I0919 18:17:18.285774 140474388072320 learning.py:507] global step 4808: loss = 0.9937 (1.339 sec/step)\n",
            "I0919 18:17:19.572679 140474388072320 learning.py:507] global step 4809: loss = 0.6906 (1.285 sec/step)\n",
            "I0919 18:17:20.874715 140474388072320 learning.py:507] global step 4810: loss = 1.0390 (1.300 sec/step)\n",
            "I0919 18:17:22.201375 140474388072320 learning.py:507] global step 4811: loss = 0.9154 (1.325 sec/step)\n",
            "I0919 18:17:23.499102 140474388072320 learning.py:507] global step 4812: loss = 0.9173 (1.296 sec/step)\n",
            "I0919 18:17:24.814696 140474388072320 learning.py:507] global step 4813: loss = 0.7238 (1.314 sec/step)\n",
            "I0919 18:17:26.133618 140474388072320 learning.py:507] global step 4814: loss = 0.7484 (1.317 sec/step)\n",
            "I0919 18:17:27.475347 140474388072320 learning.py:507] global step 4815: loss = 0.8152 (1.340 sec/step)\n",
            "I0919 18:17:28.807759 140474388072320 learning.py:507] global step 4816: loss = 1.1233 (1.331 sec/step)\n",
            "I0919 18:17:30.134526 140474388072320 learning.py:507] global step 4817: loss = 0.9233 (1.325 sec/step)\n",
            "I0919 18:17:31.457885 140474388072320 learning.py:507] global step 4818: loss = 0.8326 (1.322 sec/step)\n",
            "I0919 18:17:32.772229 140474388072320 learning.py:507] global step 4819: loss = 0.6551 (1.313 sec/step)\n",
            "I0919 18:17:34.080039 140474388072320 learning.py:507] global step 4820: loss = 0.7315 (1.306 sec/step)\n",
            "I0919 18:17:35.415617 140474388072320 learning.py:507] global step 4821: loss = 0.6674 (1.334 sec/step)\n",
            "I0919 18:17:36.738290 140474388072320 learning.py:507] global step 4822: loss = 0.6930 (1.320 sec/step)\n",
            "I0919 18:17:38.052830 140474388072320 learning.py:507] global step 4823: loss = 0.9787 (1.312 sec/step)\n",
            "I0919 18:17:39.355925 140474388072320 learning.py:507] global step 4824: loss = 1.0964 (1.301 sec/step)\n",
            "I0919 18:17:40.670896 140474388072320 learning.py:507] global step 4825: loss = 0.9366 (1.313 sec/step)\n",
            "I0919 18:17:42.001698 140474388072320 learning.py:507] global step 4826: loss = 0.7364 (1.329 sec/step)\n",
            "I0919 18:17:43.336643 140474388072320 learning.py:507] global step 4827: loss = 0.9908 (1.333 sec/step)\n",
            "I0919 18:17:44.666223 140474388072320 learning.py:507] global step 4828: loss = 0.7464 (1.328 sec/step)\n",
            "I0919 18:17:46.004700 140474388072320 learning.py:507] global step 4829: loss = 0.7342 (1.337 sec/step)\n",
            "I0919 18:17:47.330611 140474388072320 learning.py:507] global step 4830: loss = 0.8446 (1.324 sec/step)\n",
            "I0919 18:17:48.646945 140474388072320 learning.py:507] global step 4831: loss = 0.7788 (1.315 sec/step)\n",
            "I0919 18:17:49.955554 140474388072320 learning.py:507] global step 4832: loss = 0.7748 (1.307 sec/step)\n",
            "I0919 18:17:51.270432 140474388072320 learning.py:507] global step 4833: loss = 0.7301 (1.313 sec/step)\n",
            "I0919 18:17:52.559082 140474388072320 learning.py:507] global step 4834: loss = 0.9427 (1.286 sec/step)\n",
            "I0919 18:17:53.868505 140474388072320 learning.py:507] global step 4835: loss = 0.7677 (1.307 sec/step)\n",
            "I0919 18:17:55.144334 140474388072320 learning.py:507] global step 4836: loss = 0.7724 (1.274 sec/step)\n",
            "I0919 18:17:56.449553 140474388072320 learning.py:507] global step 4837: loss = 0.7712 (1.304 sec/step)\n",
            "I0919 18:17:57.780594 140474388072320 learning.py:507] global step 4838: loss = 0.7143 (1.329 sec/step)\n",
            "I0919 18:17:59.098967 140474388072320 learning.py:507] global step 4839: loss = 1.0562 (1.316 sec/step)\n",
            "I0919 18:18:00.396893 140474388072320 learning.py:507] global step 4840: loss = 0.8062 (1.296 sec/step)\n",
            "I0919 18:18:01.713198 140474388072320 learning.py:507] global step 4841: loss = 0.9655 (1.315 sec/step)\n",
            "I0919 18:18:03.022712 140474388072320 learning.py:507] global step 4842: loss = 0.7896 (1.307 sec/step)\n",
            "I0919 18:18:04.317770 140474388072320 learning.py:507] global step 4843: loss = 0.7406 (1.291 sec/step)\n",
            "I0919 18:18:05.618943 140474388072320 learning.py:507] global step 4844: loss = 0.6640 (1.299 sec/step)\n",
            "I0919 18:18:06.920064 140474388072320 learning.py:507] global step 4845: loss = 0.9509 (1.299 sec/step)\n",
            "I0919 18:18:08.258486 140474388072320 learning.py:507] global step 4846: loss = 0.5775 (1.337 sec/step)\n",
            "I0919 18:18:09.572274 140474388072320 learning.py:507] global step 4847: loss = 0.8899 (1.312 sec/step)\n",
            "I0919 18:18:10.919097 140474388072320 learning.py:507] global step 4848: loss = 0.8098 (1.345 sec/step)\n",
            "I0919 18:18:12.206976 140474388072320 learning.py:507] global step 4849: loss = 0.8069 (1.286 sec/step)\n",
            "I0919 18:18:13.544739 140474388072320 learning.py:507] global step 4850: loss = 0.7538 (1.336 sec/step)\n",
            "I0919 18:18:14.902256 140474388072320 learning.py:507] global step 4851: loss = 1.1793 (1.356 sec/step)\n",
            "I0919 18:18:16.209222 140474388072320 learning.py:507] global step 4852: loss = 0.8950 (1.305 sec/step)\n",
            "I0919 18:18:17.522269 140474388072320 learning.py:507] global step 4853: loss = 0.7813 (1.311 sec/step)\n",
            "I0919 18:18:18.816237 140474388072320 learning.py:507] global step 4854: loss = 0.8529 (1.292 sec/step)\n",
            "I0919 18:18:20.166168 140474388072320 learning.py:507] global step 4855: loss = 0.7585 (1.348 sec/step)\n",
            "I0919 18:18:21.454186 140474388072320 learning.py:507] global step 4856: loss = 0.7291 (1.286 sec/step)\n",
            "I0919 18:18:22.760277 140474388072320 learning.py:507] global step 4857: loss = 0.9239 (1.304 sec/step)\n",
            "I0919 18:18:24.086941 140474388072320 learning.py:507] global step 4858: loss = 0.7914 (1.325 sec/step)\n",
            "I0919 18:18:25.447005 140474388072320 learning.py:507] global step 4859: loss = 0.6370 (1.358 sec/step)\n",
            "I0919 18:18:26.754353 140474388072320 learning.py:507] global step 4860: loss = 0.8311 (1.306 sec/step)\n",
            "I0919 18:18:28.592463 140471297337088 supervisor.py:1050] Recording summary at step 4860.\n",
            "I0919 18:18:29.017210 140474388072320 learning.py:507] global step 4861: loss = 1.0289 (2.261 sec/step)\n",
            "I0919 18:18:30.323048 140474388072320 learning.py:507] global step 4862: loss = 0.7485 (1.304 sec/step)\n",
            "I0919 18:18:31.609854 140474388072320 learning.py:507] global step 4863: loss = 0.9255 (1.285 sec/step)\n",
            "I0919 18:18:32.960021 140474388072320 learning.py:507] global step 4864: loss = 0.8319 (1.348 sec/step)\n",
            "I0919 18:18:34.321259 140474388072320 learning.py:507] global step 4865: loss = 0.7797 (1.359 sec/step)\n",
            "I0919 18:18:35.669331 140474388072320 learning.py:507] global step 4866: loss = 0.7423 (1.347 sec/step)\n",
            "I0919 18:18:37.022777 140474388072320 learning.py:507] global step 4867: loss = 0.7861 (1.352 sec/step)\n",
            "I0919 18:18:38.315860 140474388072320 learning.py:507] global step 4868: loss = 0.7258 (1.291 sec/step)\n",
            "I0919 18:18:39.628384 140474388072320 learning.py:507] global step 4869: loss = 0.7759 (1.311 sec/step)\n",
            "I0919 18:18:40.921741 140474388072320 learning.py:507] global step 4870: loss = 0.7934 (1.292 sec/step)\n",
            "I0919 18:18:42.248382 140474388072320 learning.py:507] global step 4871: loss = 0.7967 (1.325 sec/step)\n",
            "I0919 18:18:43.538436 140474388072320 learning.py:507] global step 4872: loss = 1.2324 (1.288 sec/step)\n",
            "I0919 18:18:44.866673 140474388072320 learning.py:507] global step 4873: loss = 0.8419 (1.327 sec/step)\n",
            "I0919 18:18:46.212578 140474388072320 learning.py:507] global step 4874: loss = 0.7382 (1.343 sec/step)\n",
            "I0919 18:18:47.570365 140474388072320 learning.py:507] global step 4875: loss = 0.9024 (1.356 sec/step)\n",
            "I0919 18:18:48.907782 140474388072320 learning.py:507] global step 4876: loss = 0.7261 (1.335 sec/step)\n",
            "I0919 18:18:50.229778 140474388072320 learning.py:507] global step 4877: loss = 0.6029 (1.320 sec/step)\n",
            "I0919 18:18:51.545713 140474388072320 learning.py:507] global step 4878: loss = 0.8979 (1.314 sec/step)\n",
            "I0919 18:18:52.864501 140474388072320 learning.py:507] global step 4879: loss = 0.8222 (1.317 sec/step)\n",
            "I0919 18:18:54.171734 140474388072320 learning.py:507] global step 4880: loss = 0.7395 (1.305 sec/step)\n",
            "I0919 18:18:55.496237 140474388072320 learning.py:507] global step 4881: loss = 0.8342 (1.323 sec/step)\n",
            "I0919 18:18:56.826319 140474388072320 learning.py:507] global step 4882: loss = 0.9584 (1.328 sec/step)\n",
            "I0919 18:18:58.139904 140474388072320 learning.py:507] global step 4883: loss = 0.7673 (1.312 sec/step)\n",
            "I0919 18:18:59.442572 140474388072320 learning.py:507] global step 4884: loss = 0.8845 (1.301 sec/step)\n",
            "I0919 18:19:00.744378 140474388072320 learning.py:507] global step 4885: loss = 0.8044 (1.300 sec/step)\n",
            "I0919 18:19:02.055284 140474388072320 learning.py:507] global step 4886: loss = 0.8422 (1.309 sec/step)\n",
            "I0919 18:19:03.362432 140474388072320 learning.py:507] global step 4887: loss = 0.7169 (1.305 sec/step)\n",
            "I0919 18:19:04.675532 140474388072320 learning.py:507] global step 4888: loss = 0.8183 (1.311 sec/step)\n",
            "I0919 18:19:05.993987 140474388072320 learning.py:507] global step 4889: loss = 1.0547 (1.317 sec/step)\n",
            "I0919 18:19:07.293000 140474388072320 learning.py:507] global step 4890: loss = 0.8895 (1.297 sec/step)\n",
            "I0919 18:19:08.601164 140474388072320 learning.py:507] global step 4891: loss = 0.9682 (1.306 sec/step)\n",
            "I0919 18:19:09.916884 140474388072320 learning.py:507] global step 4892: loss = 0.8489 (1.314 sec/step)\n",
            "I0919 18:19:11.304542 140474388072320 learning.py:507] global step 4893: loss = 0.9785 (1.386 sec/step)\n",
            "I0919 18:19:12.616565 140474388072320 learning.py:507] global step 4894: loss = 0.8203 (1.310 sec/step)\n",
            "I0919 18:19:13.986676 140474388072320 learning.py:507] global step 4895: loss = 1.0676 (1.368 sec/step)\n",
            "I0919 18:19:15.331254 140474388072320 learning.py:507] global step 4896: loss = 0.8365 (1.343 sec/step)\n",
            "I0919 18:19:16.703322 140474388072320 learning.py:507] global step 4897: loss = 0.7041 (1.370 sec/step)\n",
            "I0919 18:19:18.027760 140474388072320 learning.py:507] global step 4898: loss = 1.5462 (1.323 sec/step)\n",
            "I0919 18:19:19.360039 140474388072320 learning.py:507] global step 4899: loss = 0.7609 (1.331 sec/step)\n",
            "I0919 18:19:20.675266 140474388072320 learning.py:507] global step 4900: loss = 0.8130 (1.314 sec/step)\n",
            "I0919 18:19:21.974843 140474388072320 learning.py:507] global step 4901: loss = 0.9007 (1.298 sec/step)\n",
            "I0919 18:19:23.282958 140474388072320 learning.py:507] global step 4902: loss = 0.6626 (1.306 sec/step)\n",
            "I0919 18:19:24.610832 140474388072320 learning.py:507] global step 4903: loss = 0.7013 (1.326 sec/step)\n",
            "I0919 18:19:25.928702 140474388072320 learning.py:507] global step 4904: loss = 0.7536 (1.316 sec/step)\n",
            "I0919 18:19:27.260653 140474388072320 learning.py:507] global step 4905: loss = 0.9002 (1.330 sec/step)\n",
            "I0919 18:19:28.566504 140474388072320 learning.py:507] global step 4906: loss = 0.8395 (1.304 sec/step)\n",
            "I0919 18:19:29.880388 140474388072320 learning.py:507] global step 4907: loss = 1.1721 (1.312 sec/step)\n",
            "I0919 18:19:31.243077 140474388072320 learning.py:507] global step 4908: loss = 1.0261 (1.361 sec/step)\n",
            "I0919 18:19:32.544604 140474388072320 learning.py:507] global step 4909: loss = 0.7929 (1.300 sec/step)\n",
            "I0919 18:19:33.888616 140474388072320 learning.py:507] global step 4910: loss = 0.6130 (1.342 sec/step)\n",
            "I0919 18:19:35.232661 140474388072320 learning.py:507] global step 4911: loss = 0.5842 (1.342 sec/step)\n",
            "I0919 18:19:36.556307 140474388072320 learning.py:507] global step 4912: loss = 0.6042 (1.322 sec/step)\n",
            "I0919 18:19:37.875844 140474388072320 learning.py:507] global step 4913: loss = 0.6590 (1.318 sec/step)\n",
            "I0919 18:19:39.223849 140474388072320 learning.py:507] global step 4914: loss = 0.7708 (1.346 sec/step)\n",
            "I0919 18:19:40.547936 140474388072320 learning.py:507] global step 4915: loss = 0.8824 (1.323 sec/step)\n",
            "I0919 18:19:41.845508 140474388072320 learning.py:507] global step 4916: loss = 0.7762 (1.296 sec/step)\n",
            "I0919 18:19:43.155471 140474388072320 learning.py:507] global step 4917: loss = 1.0979 (1.308 sec/step)\n",
            "I0919 18:19:44.460203 140474388072320 learning.py:507] global step 4918: loss = 0.7767 (1.303 sec/step)\n",
            "I0919 18:19:45.768784 140474388072320 learning.py:507] global step 4919: loss = 0.9018 (1.307 sec/step)\n",
            "I0919 18:19:47.134852 140474388072320 learning.py:507] global step 4920: loss = 0.7655 (1.364 sec/step)\n",
            "I0919 18:19:48.511460 140474388072320 learning.py:507] global step 4921: loss = 0.8310 (1.375 sec/step)\n",
            "I0919 18:19:49.835515 140474388072320 learning.py:507] global step 4922: loss = 0.8230 (1.322 sec/step)\n",
            "I0919 18:19:51.143723 140474388072320 learning.py:507] global step 4923: loss = 0.8561 (1.307 sec/step)\n",
            "I0919 18:19:52.483002 140474388072320 learning.py:507] global step 4924: loss = 1.0084 (1.338 sec/step)\n",
            "I0919 18:19:53.768595 140474388072320 learning.py:507] global step 4925: loss = 1.3133 (1.284 sec/step)\n",
            "I0919 18:19:55.079730 140474388072320 learning.py:507] global step 4926: loss = 0.8189 (1.309 sec/step)\n",
            "I0919 18:19:56.393256 140474388072320 learning.py:507] global step 4927: loss = 0.9799 (1.312 sec/step)\n",
            "I0919 18:19:57.684442 140474388072320 learning.py:507] global step 4928: loss = 0.9213 (1.290 sec/step)\n",
            "I0919 18:19:59.003268 140474388072320 learning.py:507] global step 4929: loss = 0.8916 (1.316 sec/step)\n",
            "I0919 18:20:00.292397 140474388072320 learning.py:507] global step 4930: loss = 1.0948 (1.287 sec/step)\n",
            "I0919 18:20:01.597876 140474388072320 learning.py:507] global step 4931: loss = 0.9951 (1.304 sec/step)\n",
            "I0919 18:20:02.930026 140474388072320 learning.py:507] global step 4932: loss = 0.7611 (1.330 sec/step)\n",
            "I0919 18:20:04.257028 140474388072320 learning.py:507] global step 4933: loss = 0.7612 (1.325 sec/step)\n",
            "I0919 18:20:05.591824 140474388072320 learning.py:507] global step 4934: loss = 1.5893 (1.333 sec/step)\n",
            "I0919 18:20:06.894165 140474388072320 learning.py:507] global step 4935: loss = 1.0457 (1.300 sec/step)\n",
            "I0919 18:20:08.222198 140474388072320 learning.py:507] global step 4936: loss = 0.9124 (1.326 sec/step)\n",
            "I0919 18:20:09.521152 140474388072320 learning.py:507] global step 4937: loss = 0.7946 (1.297 sec/step)\n",
            "I0919 18:20:10.837688 140474388072320 learning.py:507] global step 4938: loss = 1.1135 (1.315 sec/step)\n",
            "I0919 18:20:12.168706 140474388072320 learning.py:507] global step 4939: loss = 0.7617 (1.329 sec/step)\n",
            "I0919 18:20:13.481234 140474388072320 learning.py:507] global step 4940: loss = 0.8325 (1.311 sec/step)\n",
            "I0919 18:20:14.814172 140474388072320 learning.py:507] global step 4941: loss = 0.6750 (1.331 sec/step)\n",
            "I0919 18:20:16.130054 140474388072320 learning.py:507] global step 4942: loss = 1.1242 (1.314 sec/step)\n",
            "I0919 18:20:17.458823 140474388072320 learning.py:507] global step 4943: loss = 0.9994 (1.327 sec/step)\n",
            "I0919 18:20:18.789916 140474388072320 learning.py:507] global step 4944: loss = 0.8451 (1.327 sec/step)\n",
            "I0919 18:20:20.100923 140474388072320 learning.py:507] global step 4945: loss = 0.8789 (1.309 sec/step)\n",
            "I0919 18:20:21.398304 140474388072320 learning.py:507] global step 4946: loss = 0.9933 (1.296 sec/step)\n",
            "I0919 18:20:22.716042 140474388072320 learning.py:507] global step 4947: loss = 0.8521 (1.316 sec/step)\n",
            "I0919 18:20:24.023916 140474388072320 learning.py:507] global step 4948: loss = 0.7550 (1.306 sec/step)\n",
            "I0919 18:20:25.342758 140474388072320 learning.py:507] global step 4949: loss = 0.8434 (1.317 sec/step)\n",
            "I0919 18:20:26.668330 140474388072320 learning.py:507] global step 4950: loss = 1.0897 (1.324 sec/step)\n",
            "I0919 18:20:26.872942 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 18:20:29.427346 140474388072320 learning.py:507] global step 4951: loss = 0.9586 (2.079 sec/step)\n",
            "I0919 18:20:30.412668 140471297337088 supervisor.py:1050] Recording summary at step 4951.\n",
            "I0919 18:20:31.612473 140474388072320 learning.py:507] global step 4952: loss = 0.7253 (2.135 sec/step)\n",
            "I0919 18:20:33.239095 140474388072320 learning.py:507] global step 4953: loss = 0.8522 (1.624 sec/step)\n",
            "I0919 18:20:34.525803 140474388072320 learning.py:507] global step 4954: loss = 0.9027 (1.285 sec/step)\n",
            "I0919 18:20:35.842300 140474388072320 learning.py:507] global step 4955: loss = 0.8367 (1.315 sec/step)\n",
            "I0919 18:20:37.136673 140474388072320 learning.py:507] global step 4956: loss = 0.8329 (1.292 sec/step)\n",
            "I0919 18:20:38.475291 140474388072320 learning.py:507] global step 4957: loss = 0.6893 (1.337 sec/step)\n",
            "I0919 18:20:39.765955 140474388072320 learning.py:507] global step 4958: loss = 0.7872 (1.289 sec/step)\n",
            "I0919 18:20:41.069442 140474388072320 learning.py:507] global step 4959: loss = 0.8035 (1.301 sec/step)\n",
            "I0919 18:20:42.423577 140474388072320 learning.py:507] global step 4960: loss = 1.0234 (1.352 sec/step)\n",
            "I0919 18:20:43.756308 140474388072320 learning.py:507] global step 4961: loss = 0.7570 (1.331 sec/step)\n",
            "I0919 18:20:45.044404 140474388072320 learning.py:507] global step 4962: loss = 1.0734 (1.286 sec/step)\n",
            "I0919 18:20:46.358659 140474388072320 learning.py:507] global step 4963: loss = 0.9195 (1.312 sec/step)\n",
            "I0919 18:20:47.681266 140474388072320 learning.py:507] global step 4964: loss = 0.7414 (1.321 sec/step)\n",
            "I0919 18:20:48.968857 140474388072320 learning.py:507] global step 4965: loss = 0.7447 (1.286 sec/step)\n",
            "I0919 18:20:50.282905 140474388072320 learning.py:507] global step 4966: loss = 0.7479 (1.312 sec/step)\n",
            "I0919 18:20:51.584908 140474388072320 learning.py:507] global step 4967: loss = 0.7895 (1.300 sec/step)\n",
            "I0919 18:20:52.922076 140474388072320 learning.py:507] global step 4968: loss = 0.6987 (1.335 sec/step)\n",
            "I0919 18:20:54.256545 140474388072320 learning.py:507] global step 4969: loss = 0.9332 (1.333 sec/step)\n",
            "I0919 18:20:55.568237 140474388072320 learning.py:507] global step 4970: loss = 0.7543 (1.309 sec/step)\n",
            "I0919 18:20:56.854459 140474388072320 learning.py:507] global step 4971: loss = 0.8720 (1.284 sec/step)\n",
            "I0919 18:20:58.171384 140474388072320 learning.py:507] global step 4972: loss = 0.8454 (1.315 sec/step)\n",
            "I0919 18:20:59.458945 140474388072320 learning.py:507] global step 4973: loss = 0.6833 (1.286 sec/step)\n",
            "I0919 18:21:00.756068 140474388072320 learning.py:507] global step 4974: loss = 0.7360 (1.296 sec/step)\n",
            "I0919 18:21:02.093569 140474388072320 learning.py:507] global step 4975: loss = 0.8164 (1.336 sec/step)\n",
            "I0919 18:21:03.405778 140474388072320 learning.py:507] global step 4976: loss = 0.8735 (1.310 sec/step)\n",
            "I0919 18:21:04.710462 140474388072320 learning.py:507] global step 4977: loss = 0.7906 (1.303 sec/step)\n",
            "I0919 18:21:06.028841 140474388072320 learning.py:507] global step 4978: loss = 0.6491 (1.317 sec/step)\n",
            "I0919 18:21:07.357172 140474388072320 learning.py:507] global step 4979: loss = 0.8852 (1.327 sec/step)\n",
            "I0919 18:21:08.663383 140474388072320 learning.py:507] global step 4980: loss = 0.8577 (1.304 sec/step)\n",
            "I0919 18:21:09.984623 140474388072320 learning.py:507] global step 4981: loss = 0.9213 (1.318 sec/step)\n",
            "I0919 18:21:11.316361 140474388072320 learning.py:507] global step 4982: loss = 0.6184 (1.330 sec/step)\n",
            "I0919 18:21:12.641071 140474388072320 learning.py:507] global step 4983: loss = 0.8637 (1.323 sec/step)\n",
            "I0919 18:21:13.927761 140474388072320 learning.py:507] global step 4984: loss = 1.1103 (1.285 sec/step)\n",
            "I0919 18:21:15.227925 140474388072320 learning.py:507] global step 4985: loss = 0.7753 (1.298 sec/step)\n",
            "I0919 18:21:16.564665 140474388072320 learning.py:507] global step 4986: loss = 0.6566 (1.335 sec/step)\n",
            "I0919 18:21:17.855979 140474388072320 learning.py:507] global step 4987: loss = 0.7638 (1.290 sec/step)\n",
            "I0919 18:21:19.161125 140474388072320 learning.py:507] global step 4988: loss = 0.9428 (1.303 sec/step)\n",
            "I0919 18:21:20.478889 140474388072320 learning.py:507] global step 4989: loss = 1.1120 (1.316 sec/step)\n",
            "I0919 18:21:21.787158 140474388072320 learning.py:507] global step 4990: loss = 1.4121 (1.306 sec/step)\n",
            "I0919 18:21:23.106564 140474388072320 learning.py:507] global step 4991: loss = 0.8420 (1.318 sec/step)\n",
            "I0919 18:21:24.420576 140474388072320 learning.py:507] global step 4992: loss = 0.7906 (1.312 sec/step)\n",
            "I0919 18:21:25.728893 140474388072320 learning.py:507] global step 4993: loss = 0.8811 (1.306 sec/step)\n",
            "I0919 18:21:27.043680 140474388072320 learning.py:507] global step 4994: loss = 0.7690 (1.313 sec/step)\n",
            "I0919 18:21:28.342454 140474388072320 learning.py:507] global step 4995: loss = 0.8420 (1.297 sec/step)\n",
            "I0919 18:21:29.659233 140474388072320 learning.py:507] global step 4996: loss = 0.7803 (1.315 sec/step)\n",
            "I0919 18:21:30.967214 140474388072320 learning.py:507] global step 4997: loss = 0.9388 (1.306 sec/step)\n",
            "I0919 18:21:32.256400 140474388072320 learning.py:507] global step 4998: loss = 0.7022 (1.287 sec/step)\n",
            "I0919 18:21:33.541259 140474388072320 learning.py:507] global step 4999: loss = 0.8474 (1.283 sec/step)\n",
            "I0919 18:21:34.860095 140474388072320 learning.py:507] global step 5000: loss = 1.0044 (1.317 sec/step)\n",
            "I0919 18:21:36.158843 140474388072320 learning.py:507] global step 5001: loss = 0.8641 (1.297 sec/step)\n",
            "I0919 18:21:37.449790 140474388072320 learning.py:507] global step 5002: loss = 0.8642 (1.289 sec/step)\n",
            "I0919 18:21:38.768152 140474388072320 learning.py:507] global step 5003: loss = 0.8386 (1.315 sec/step)\n",
            "I0919 18:21:40.103481 140474388072320 learning.py:507] global step 5004: loss = 0.7760 (1.333 sec/step)\n",
            "I0919 18:21:41.451895 140474388072320 learning.py:507] global step 5005: loss = 0.8424 (1.347 sec/step)\n",
            "I0919 18:21:42.806723 140474388072320 learning.py:507] global step 5006: loss = 0.7594 (1.353 sec/step)\n",
            "I0919 18:21:44.137823 140474388072320 learning.py:507] global step 5007: loss = 0.8125 (1.329 sec/step)\n",
            "I0919 18:21:45.446815 140474388072320 learning.py:507] global step 5008: loss = 0.8401 (1.307 sec/step)\n",
            "I0919 18:21:46.756484 140474388072320 learning.py:507] global step 5009: loss = 0.6071 (1.308 sec/step)\n",
            "I0919 18:21:48.081330 140474388072320 learning.py:507] global step 5010: loss = 0.9160 (1.323 sec/step)\n",
            "I0919 18:21:49.410974 140474388072320 learning.py:507] global step 5011: loss = 0.7505 (1.328 sec/step)\n",
            "I0919 18:21:50.751406 140474388072320 learning.py:507] global step 5012: loss = 0.8541 (1.338 sec/step)\n",
            "I0919 18:21:52.064404 140474388072320 learning.py:507] global step 5013: loss = 0.8309 (1.311 sec/step)\n",
            "I0919 18:21:53.387486 140474388072320 learning.py:507] global step 5014: loss = 0.6806 (1.322 sec/step)\n",
            "I0919 18:21:54.699853 140474388072320 learning.py:507] global step 5015: loss = 0.7478 (1.311 sec/step)\n",
            "I0919 18:21:55.994760 140474388072320 learning.py:507] global step 5016: loss = 0.7598 (1.293 sec/step)\n",
            "I0919 18:21:57.326500 140474388072320 learning.py:507] global step 5017: loss = 0.5910 (1.330 sec/step)\n",
            "I0919 18:21:58.609357 140474388072320 learning.py:507] global step 5018: loss = 0.5741 (1.280 sec/step)\n",
            "I0919 18:21:59.913489 140474388072320 learning.py:507] global step 5019: loss = 0.8656 (1.302 sec/step)\n",
            "I0919 18:22:01.224917 140474388072320 learning.py:507] global step 5020: loss = 0.8091 (1.310 sec/step)\n",
            "I0919 18:22:02.561738 140474388072320 learning.py:507] global step 5021: loss = 0.8296 (1.335 sec/step)\n",
            "I0919 18:22:03.891788 140474388072320 learning.py:507] global step 5022: loss = 0.7457 (1.328 sec/step)\n",
            "I0919 18:22:05.232491 140474388072320 learning.py:507] global step 5023: loss = 0.7778 (1.339 sec/step)\n",
            "I0919 18:22:06.528283 140474388072320 learning.py:507] global step 5024: loss = 0.8520 (1.294 sec/step)\n",
            "I0919 18:22:07.844778 140474388072320 learning.py:507] global step 5025: loss = 0.7377 (1.315 sec/step)\n",
            "I0919 18:22:09.158092 140474388072320 learning.py:507] global step 5026: loss = 0.8991 (1.312 sec/step)\n",
            "I0919 18:22:10.512988 140474388072320 learning.py:507] global step 5027: loss = 0.8874 (1.353 sec/step)\n",
            "I0919 18:22:11.833064 140474388072320 learning.py:507] global step 5028: loss = 0.5401 (1.318 sec/step)\n",
            "I0919 18:22:13.102540 140474388072320 learning.py:507] global step 5029: loss = 0.8181 (1.268 sec/step)\n",
            "I0919 18:22:14.413132 140474388072320 learning.py:507] global step 5030: loss = 0.6860 (1.309 sec/step)\n",
            "I0919 18:22:15.727674 140474388072320 learning.py:507] global step 5031: loss = 0.7123 (1.313 sec/step)\n",
            "I0919 18:22:17.059545 140474388072320 learning.py:507] global step 5032: loss = 1.0379 (1.330 sec/step)\n",
            "I0919 18:22:18.347181 140474388072320 learning.py:507] global step 5033: loss = 0.7112 (1.286 sec/step)\n",
            "I0919 18:22:19.640594 140474388072320 learning.py:507] global step 5034: loss = 0.7633 (1.292 sec/step)\n",
            "I0919 18:22:20.960820 140474388072320 learning.py:507] global step 5035: loss = 1.0110 (1.319 sec/step)\n",
            "I0919 18:22:22.265688 140474388072320 learning.py:507] global step 5036: loss = 0.8266 (1.303 sec/step)\n",
            "I0919 18:22:23.593090 140474388072320 learning.py:507] global step 5037: loss = 0.7154 (1.326 sec/step)\n",
            "I0919 18:22:24.879257 140474388072320 learning.py:507] global step 5038: loss = 0.7780 (1.285 sec/step)\n",
            "I0919 18:22:26.196974 140474388072320 learning.py:507] global step 5039: loss = 0.7517 (1.316 sec/step)\n",
            "I0919 18:22:27.447269 140474388072320 learning.py:507] global step 5040: loss = 0.6256 (1.248 sec/step)\n",
            "I0919 18:22:29.236681 140471297337088 supervisor.py:1050] Recording summary at step 5040.\n",
            "I0919 18:22:29.743021 140474388072320 learning.py:507] global step 5041: loss = 0.8923 (2.177 sec/step)\n",
            "I0919 18:22:31.119347 140474388072320 learning.py:507] global step 5042: loss = 0.8886 (1.374 sec/step)\n",
            "I0919 18:22:32.436027 140474388072320 learning.py:507] global step 5043: loss = 0.7335 (1.315 sec/step)\n",
            "I0919 18:22:33.745341 140474388072320 learning.py:507] global step 5044: loss = 0.7878 (1.307 sec/step)\n",
            "I0919 18:22:35.070926 140474388072320 learning.py:507] global step 5045: loss = 1.0690 (1.324 sec/step)\n",
            "I0919 18:22:36.378912 140474388072320 learning.py:507] global step 5046: loss = 0.9559 (1.306 sec/step)\n",
            "I0919 18:22:37.688520 140474388072320 learning.py:507] global step 5047: loss = 0.9035 (1.308 sec/step)\n",
            "I0919 18:22:39.042893 140474388072320 learning.py:507] global step 5048: loss = 0.6627 (1.353 sec/step)\n",
            "I0919 18:22:40.341634 140474388072320 learning.py:507] global step 5049: loss = 0.7202 (1.297 sec/step)\n",
            "I0919 18:22:41.694025 140474388072320 learning.py:507] global step 5050: loss = 0.8938 (1.351 sec/step)\n",
            "I0919 18:22:43.006052 140474388072320 learning.py:507] global step 5051: loss = 0.7003 (1.310 sec/step)\n",
            "I0919 18:22:44.316308 140474388072320 learning.py:507] global step 5052: loss = 1.0279 (1.309 sec/step)\n",
            "I0919 18:22:45.656418 140474388072320 learning.py:507] global step 5053: loss = 0.9940 (1.338 sec/step)\n",
            "I0919 18:22:46.969758 140474388072320 learning.py:507] global step 5054: loss = 0.8211 (1.312 sec/step)\n",
            "I0919 18:22:48.296871 140474388072320 learning.py:507] global step 5055: loss = 0.8838 (1.326 sec/step)\n",
            "I0919 18:22:49.687638 140474388072320 learning.py:507] global step 5056: loss = 0.7764 (1.389 sec/step)\n",
            "I0919 18:22:51.012084 140474388072320 learning.py:507] global step 5057: loss = 0.8234 (1.322 sec/step)\n",
            "I0919 18:22:52.323474 140474388072320 learning.py:507] global step 5058: loss = 0.7539 (1.309 sec/step)\n",
            "I0919 18:22:53.636997 140474388072320 learning.py:507] global step 5059: loss = 0.9424 (1.312 sec/step)\n",
            "I0919 18:22:54.934037 140474388072320 learning.py:507] global step 5060: loss = 0.7450 (1.295 sec/step)\n",
            "I0919 18:22:56.327577 140474388072320 learning.py:507] global step 5061: loss = 1.0260 (1.392 sec/step)\n",
            "I0919 18:22:57.622328 140474388072320 learning.py:507] global step 5062: loss = 0.7283 (1.293 sec/step)\n",
            "I0919 18:22:58.977420 140474388072320 learning.py:507] global step 5063: loss = 0.7380 (1.354 sec/step)\n",
            "I0919 18:23:00.269539 140474388072320 learning.py:507] global step 5064: loss = 0.7491 (1.290 sec/step)\n",
            "I0919 18:23:01.562663 140474388072320 learning.py:507] global step 5065: loss = 1.1767 (1.292 sec/step)\n",
            "I0919 18:23:02.868866 140474388072320 learning.py:507] global step 5066: loss = 0.9402 (1.305 sec/step)\n",
            "I0919 18:23:04.230345 140474388072320 learning.py:507] global step 5067: loss = 1.0502 (1.360 sec/step)\n",
            "I0919 18:23:05.588943 140474388072320 learning.py:507] global step 5068: loss = 0.8027 (1.357 sec/step)\n",
            "I0919 18:23:06.963519 140474388072320 learning.py:507] global step 5069: loss = 0.7757 (1.373 sec/step)\n",
            "I0919 18:23:08.299046 140474388072320 learning.py:507] global step 5070: loss = 0.9631 (1.334 sec/step)\n",
            "I0919 18:23:09.605015 140474388072320 learning.py:507] global step 5071: loss = 0.8821 (1.304 sec/step)\n",
            "I0919 18:23:10.889305 140474388072320 learning.py:507] global step 5072: loss = 0.8351 (1.282 sec/step)\n",
            "I0919 18:23:12.183886 140474388072320 learning.py:507] global step 5073: loss = 0.7231 (1.293 sec/step)\n",
            "I0919 18:23:13.507030 140474388072320 learning.py:507] global step 5074: loss = 1.2344 (1.322 sec/step)\n",
            "I0919 18:23:14.801923 140474388072320 learning.py:507] global step 5075: loss = 0.7366 (1.293 sec/step)\n",
            "I0919 18:23:16.134454 140474388072320 learning.py:507] global step 5076: loss = 1.1205 (1.331 sec/step)\n",
            "I0919 18:23:17.466720 140474388072320 learning.py:507] global step 5077: loss = 0.8146 (1.330 sec/step)\n",
            "I0919 18:23:18.785362 140474388072320 learning.py:507] global step 5078: loss = 0.9834 (1.317 sec/step)\n",
            "I0919 18:23:20.129998 140474388072320 learning.py:507] global step 5079: loss = 0.9026 (1.343 sec/step)\n",
            "I0919 18:23:21.447331 140474388072320 learning.py:507] global step 5080: loss = 0.9888 (1.316 sec/step)\n",
            "I0919 18:23:22.739729 140474388072320 learning.py:507] global step 5081: loss = 0.9147 (1.291 sec/step)\n",
            "I0919 18:23:24.020618 140474388072320 learning.py:507] global step 5082: loss = 0.6969 (1.279 sec/step)\n",
            "I0919 18:23:25.332750 140474388072320 learning.py:507] global step 5083: loss = 0.9970 (1.310 sec/step)\n",
            "I0919 18:23:26.630146 140474388072320 learning.py:507] global step 5084: loss = 0.6799 (1.296 sec/step)\n",
            "I0919 18:23:27.963990 140474388072320 learning.py:507] global step 5085: loss = 0.7776 (1.332 sec/step)\n",
            "I0919 18:23:29.335907 140474388072320 learning.py:507] global step 5086: loss = 0.6863 (1.370 sec/step)\n",
            "I0919 18:23:30.624602 140474388072320 learning.py:507] global step 5087: loss = 1.0067 (1.287 sec/step)\n",
            "I0919 18:23:31.944457 140474388072320 learning.py:507] global step 5088: loss = 0.8524 (1.318 sec/step)\n",
            "I0919 18:23:33.266746 140474388072320 learning.py:507] global step 5089: loss = 0.6654 (1.321 sec/step)\n",
            "I0919 18:23:34.595628 140474388072320 learning.py:507] global step 5090: loss = 0.9018 (1.327 sec/step)\n",
            "I0919 18:23:35.938536 140474388072320 learning.py:507] global step 5091: loss = 0.8165 (1.341 sec/step)\n",
            "I0919 18:23:37.274396 140474388072320 learning.py:507] global step 5092: loss = 0.7796 (1.334 sec/step)\n",
            "I0919 18:23:38.597183 140474388072320 learning.py:507] global step 5093: loss = 0.6079 (1.321 sec/step)\n",
            "I0919 18:23:39.905502 140474388072320 learning.py:507] global step 5094: loss = 0.8742 (1.307 sec/step)\n",
            "I0919 18:23:41.206965 140474388072320 learning.py:507] global step 5095: loss = 0.9872 (1.300 sec/step)\n",
            "I0919 18:23:42.520468 140474388072320 learning.py:507] global step 5096: loss = 0.9313 (1.312 sec/step)\n",
            "I0919 18:23:43.816832 140474388072320 learning.py:507] global step 5097: loss = 0.8165 (1.295 sec/step)\n",
            "I0919 18:23:45.136059 140474388072320 learning.py:507] global step 5098: loss = 0.9038 (1.317 sec/step)\n",
            "I0919 18:23:46.498421 140474388072320 learning.py:507] global step 5099: loss = 0.9271 (1.361 sec/step)\n",
            "I0919 18:23:47.840888 140474388072320 learning.py:507] global step 5100: loss = 0.7284 (1.341 sec/step)\n",
            "I0919 18:23:49.158771 140474388072320 learning.py:507] global step 5101: loss = 0.7008 (1.316 sec/step)\n",
            "I0919 18:23:50.465912 140474388072320 learning.py:507] global step 5102: loss = 0.8985 (1.305 sec/step)\n",
            "I0919 18:23:51.765645 140474388072320 learning.py:507] global step 5103: loss = 0.8506 (1.298 sec/step)\n",
            "I0919 18:23:53.096467 140474388072320 learning.py:507] global step 5104: loss = 0.9915 (1.329 sec/step)\n",
            "I0919 18:23:54.389379 140474388072320 learning.py:507] global step 5105: loss = 0.7005 (1.291 sec/step)\n",
            "I0919 18:23:55.746228 140474388072320 learning.py:507] global step 5106: loss = 0.8462 (1.355 sec/step)\n",
            "I0919 18:23:57.029407 140474388072320 learning.py:507] global step 5107: loss = 0.9771 (1.281 sec/step)\n",
            "I0919 18:23:58.331178 140474388072320 learning.py:507] global step 5108: loss = 1.0886 (1.300 sec/step)\n",
            "I0919 18:23:59.672847 140474388072320 learning.py:507] global step 5109: loss = 0.7284 (1.340 sec/step)\n",
            "I0919 18:24:01.008300 140474388072320 learning.py:507] global step 5110: loss = 0.9434 (1.333 sec/step)\n",
            "I0919 18:24:02.314441 140474388072320 learning.py:507] global step 5111: loss = 0.6954 (1.304 sec/step)\n",
            "I0919 18:24:03.617491 140474388072320 learning.py:507] global step 5112: loss = 1.0837 (1.301 sec/step)\n",
            "I0919 18:24:04.945966 140474388072320 learning.py:507] global step 5113: loss = 0.6436 (1.327 sec/step)\n",
            "I0919 18:24:06.284575 140474388072320 learning.py:507] global step 5114: loss = 0.8715 (1.337 sec/step)\n",
            "I0919 18:24:07.581491 140474388072320 learning.py:507] global step 5115: loss = 0.8111 (1.295 sec/step)\n",
            "I0919 18:24:08.923585 140474388072320 learning.py:507] global step 5116: loss = 0.7421 (1.340 sec/step)\n",
            "I0919 18:24:10.237708 140474388072320 learning.py:507] global step 5117: loss = 0.7800 (1.311 sec/step)\n",
            "I0919 18:24:11.562473 140474388072320 learning.py:507] global step 5118: loss = 0.9360 (1.319 sec/step)\n",
            "I0919 18:24:12.867592 140474388072320 learning.py:507] global step 5119: loss = 0.8601 (1.303 sec/step)\n",
            "I0919 18:24:14.170660 140474388072320 learning.py:507] global step 5120: loss = 0.7104 (1.301 sec/step)\n",
            "I0919 18:24:15.488464 140474388072320 learning.py:507] global step 5121: loss = 0.7158 (1.316 sec/step)\n",
            "I0919 18:24:16.852260 140474388072320 learning.py:507] global step 5122: loss = 0.7663 (1.362 sec/step)\n",
            "I0919 18:24:18.162065 140474388072320 learning.py:507] global step 5123: loss = 0.7848 (1.308 sec/step)\n",
            "I0919 18:24:19.487804 140474388072320 learning.py:507] global step 5124: loss = 0.7162 (1.322 sec/step)\n",
            "I0919 18:24:20.813685 140474388072320 learning.py:507] global step 5125: loss = 0.6608 (1.324 sec/step)\n",
            "I0919 18:24:22.143959 140474388072320 learning.py:507] global step 5126: loss = 0.9429 (1.329 sec/step)\n",
            "I0919 18:24:23.489676 140474388072320 learning.py:507] global step 5127: loss = 0.8444 (1.344 sec/step)\n",
            "I0919 18:24:24.795454 140474388072320 learning.py:507] global step 5128: loss = 0.7961 (1.304 sec/step)\n",
            "I0919 18:24:26.150552 140474388072320 learning.py:507] global step 5129: loss = 0.7745 (1.353 sec/step)\n",
            "I0919 18:24:27.705508 140474388072320 learning.py:507] global step 5130: loss = 0.7665 (1.349 sec/step)\n",
            "I0919 18:24:29.204649 140471297337088 supervisor.py:1050] Recording summary at step 5130.\n",
            "I0919 18:24:29.677925 140474388072320 learning.py:507] global step 5131: loss = 0.7258 (1.923 sec/step)\n",
            "I0919 18:24:31.042781 140474388072320 learning.py:507] global step 5132: loss = 0.9612 (1.363 sec/step)\n",
            "I0919 18:24:32.352768 140474388072320 learning.py:507] global step 5133: loss = 0.7838 (1.308 sec/step)\n",
            "I0919 18:24:33.657339 140474388072320 learning.py:507] global step 5134: loss = 0.8635 (1.303 sec/step)\n",
            "I0919 18:24:34.955569 140474388072320 learning.py:507] global step 5135: loss = 0.6345 (1.297 sec/step)\n",
            "I0919 18:24:36.243807 140474388072320 learning.py:507] global step 5136: loss = 0.9114 (1.286 sec/step)\n",
            "I0919 18:24:37.556998 140474388072320 learning.py:507] global step 5137: loss = 1.1179 (1.311 sec/step)\n",
            "I0919 18:24:38.903172 140474388072320 learning.py:507] global step 5138: loss = 1.5451 (1.344 sec/step)\n",
            "I0919 18:24:40.207819 140474388072320 learning.py:507] global step 5139: loss = 1.2272 (1.303 sec/step)\n",
            "I0919 18:24:41.572907 140474388072320 learning.py:507] global step 5140: loss = 0.7983 (1.364 sec/step)\n",
            "I0919 18:24:42.879442 140474388072320 learning.py:507] global step 5141: loss = 0.7774 (1.304 sec/step)\n",
            "I0919 18:24:44.232558 140474388072320 learning.py:507] global step 5142: loss = 0.5602 (1.351 sec/step)\n",
            "I0919 18:24:45.523807 140474388072320 learning.py:507] global step 5143: loss = 0.6387 (1.289 sec/step)\n",
            "I0919 18:24:46.796267 140474388072320 learning.py:507] global step 5144: loss = 0.7152 (1.271 sec/step)\n",
            "I0919 18:24:48.152460 140474388072320 learning.py:507] global step 5145: loss = 0.8012 (1.354 sec/step)\n",
            "I0919 18:24:49.453444 140474388072320 learning.py:507] global step 5146: loss = 0.8526 (1.299 sec/step)\n",
            "I0919 18:24:50.765655 140474388072320 learning.py:507] global step 5147: loss = 0.7269 (1.311 sec/step)\n",
            "I0919 18:24:52.098246 140474388072320 learning.py:507] global step 5148: loss = 0.9341 (1.331 sec/step)\n",
            "I0919 18:24:53.428239 140474388072320 learning.py:507] global step 5149: loss = 0.8603 (1.328 sec/step)\n",
            "I0919 18:24:54.725142 140474388072320 learning.py:507] global step 5150: loss = 0.7941 (1.295 sec/step)\n",
            "I0919 18:24:56.074187 140474388072320 learning.py:507] global step 5151: loss = 0.9089 (1.347 sec/step)\n",
            "I0919 18:24:57.359247 140474388072320 learning.py:507] global step 5152: loss = 0.8890 (1.283 sec/step)\n",
            "I0919 18:24:58.688487 140474388072320 learning.py:507] global step 5153: loss = 0.8261 (1.327 sec/step)\n",
            "I0919 18:24:59.984652 140474388072320 learning.py:507] global step 5154: loss = 0.9853 (1.293 sec/step)\n",
            "I0919 18:25:01.294682 140474388072320 learning.py:507] global step 5155: loss = 0.8828 (1.308 sec/step)\n",
            "I0919 18:25:02.603671 140474388072320 learning.py:507] global step 5156: loss = 0.8876 (1.307 sec/step)\n",
            "I0919 18:25:03.953435 140474388072320 learning.py:507] global step 5157: loss = 0.7784 (1.348 sec/step)\n",
            "I0919 18:25:05.250818 140474388072320 learning.py:507] global step 5158: loss = 1.0759 (1.296 sec/step)\n",
            "I0919 18:25:06.559400 140474388072320 learning.py:507] global step 5159: loss = 0.9072 (1.307 sec/step)\n",
            "I0919 18:25:07.896666 140474388072320 learning.py:507] global step 5160: loss = 0.7032 (1.335 sec/step)\n",
            "I0919 18:25:09.255914 140474388072320 learning.py:507] global step 5161: loss = 0.7429 (1.357 sec/step)\n",
            "I0919 18:25:10.591487 140474388072320 learning.py:507] global step 5162: loss = 1.0224 (1.333 sec/step)\n",
            "I0919 18:25:11.974011 140474388072320 learning.py:507] global step 5163: loss = 0.9750 (1.380 sec/step)\n",
            "I0919 18:25:13.300257 140474388072320 learning.py:507] global step 5164: loss = 0.7239 (1.324 sec/step)\n",
            "I0919 18:25:14.592886 140474388072320 learning.py:507] global step 5165: loss = 0.8132 (1.291 sec/step)\n",
            "I0919 18:25:15.906929 140474388072320 learning.py:507] global step 5166: loss = 0.8739 (1.312 sec/step)\n",
            "I0919 18:25:17.215791 140474388072320 learning.py:507] global step 5167: loss = 0.6451 (1.307 sec/step)\n",
            "I0919 18:25:18.522191 140474388072320 learning.py:507] global step 5168: loss = 1.0873 (1.305 sec/step)\n",
            "I0919 18:25:19.815757 140474388072320 learning.py:507] global step 5169: loss = 0.8812 (1.292 sec/step)\n",
            "I0919 18:25:21.144377 140474388072320 learning.py:507] global step 5170: loss = 0.6008 (1.327 sec/step)\n",
            "I0919 18:25:22.466588 140474388072320 learning.py:507] global step 5171: loss = 0.8932 (1.321 sec/step)\n",
            "I0919 18:25:23.805289 140474388072320 learning.py:507] global step 5172: loss = 0.9220 (1.337 sec/step)\n",
            "I0919 18:25:25.124658 140474388072320 learning.py:507] global step 5173: loss = 0.6572 (1.318 sec/step)\n",
            "I0919 18:25:26.414989 140474388072320 learning.py:507] global step 5174: loss = 0.6896 (1.289 sec/step)\n",
            "I0919 18:25:27.716979 140474388072320 learning.py:507] global step 5175: loss = 0.9548 (1.300 sec/step)\n",
            "I0919 18:25:29.006644 140474388072320 learning.py:507] global step 5176: loss = 0.8123 (1.288 sec/step)\n",
            "I0919 18:25:30.322892 140474388072320 learning.py:507] global step 5177: loss = 0.9498 (1.315 sec/step)\n",
            "I0919 18:25:31.636190 140474388072320 learning.py:507] global step 5178: loss = 0.9081 (1.312 sec/step)\n",
            "I0919 18:25:32.959577 140474388072320 learning.py:507] global step 5179: loss = 0.7624 (1.322 sec/step)\n",
            "I0919 18:25:34.263885 140474388072320 learning.py:507] global step 5180: loss = 0.8965 (1.303 sec/step)\n",
            "I0919 18:25:35.632431 140474388072320 learning.py:507] global step 5181: loss = 0.8588 (1.367 sec/step)\n",
            "I0919 18:25:36.962604 140474388072320 learning.py:507] global step 5182: loss = 0.8574 (1.328 sec/step)\n",
            "I0919 18:25:38.295958 140474388072320 learning.py:507] global step 5183: loss = 0.6954 (1.332 sec/step)\n",
            "I0919 18:25:39.604444 140474388072320 learning.py:507] global step 5184: loss = 0.8108 (1.307 sec/step)\n",
            "I0919 18:25:40.903134 140474388072320 learning.py:507] global step 5185: loss = 0.6663 (1.297 sec/step)\n",
            "I0919 18:25:42.187308 140474388072320 learning.py:507] global step 5186: loss = 0.6959 (1.282 sec/step)\n",
            "I0919 18:25:43.503957 140474388072320 learning.py:507] global step 5187: loss = 0.6860 (1.315 sec/step)\n",
            "I0919 18:25:44.844013 140474388072320 learning.py:507] global step 5188: loss = 1.0714 (1.338 sec/step)\n",
            "I0919 18:25:46.165838 140474388072320 learning.py:507] global step 5189: loss = 0.6840 (1.320 sec/step)\n",
            "I0919 18:25:47.469511 140474388072320 learning.py:507] global step 5190: loss = 0.7501 (1.300 sec/step)\n",
            "I0919 18:25:48.814789 140474388072320 learning.py:507] global step 5191: loss = 0.7781 (1.343 sec/step)\n",
            "I0919 18:25:50.157457 140474388072320 learning.py:507] global step 5192: loss = 0.7762 (1.341 sec/step)\n",
            "I0919 18:25:51.476140 140474388072320 learning.py:507] global step 5193: loss = 0.7862 (1.317 sec/step)\n",
            "I0919 18:25:52.833781 140474388072320 learning.py:507] global step 5194: loss = 0.7701 (1.356 sec/step)\n",
            "I0919 18:25:54.132756 140474388072320 learning.py:507] global step 5195: loss = 0.8767 (1.297 sec/step)\n",
            "I0919 18:25:55.444605 140474388072320 learning.py:507] global step 5196: loss = 0.8863 (1.310 sec/step)\n",
            "I0919 18:25:56.756190 140474388072320 learning.py:507] global step 5197: loss = 0.7906 (1.310 sec/step)\n",
            "I0919 18:25:58.114521 140474388072320 learning.py:507] global step 5198: loss = 0.6988 (1.357 sec/step)\n",
            "I0919 18:25:59.409795 140474388072320 learning.py:507] global step 5199: loss = 0.7679 (1.293 sec/step)\n",
            "I0919 18:26:00.687436 140474388072320 learning.py:507] global step 5200: loss = 0.6598 (1.276 sec/step)\n",
            "I0919 18:26:02.006808 140474388072320 learning.py:507] global step 5201: loss = 0.6843 (1.318 sec/step)\n",
            "I0919 18:26:03.312057 140474388072320 learning.py:507] global step 5202: loss = 0.7183 (1.303 sec/step)\n",
            "I0919 18:26:04.664433 140474388072320 learning.py:507] global step 5203: loss = 1.1155 (1.351 sec/step)\n",
            "I0919 18:26:05.980906 140474388072320 learning.py:507] global step 5204: loss = 0.7661 (1.315 sec/step)\n",
            "I0919 18:26:07.278929 140474388072320 learning.py:507] global step 5205: loss = 0.6885 (1.296 sec/step)\n",
            "I0919 18:26:08.567320 140474388072320 learning.py:507] global step 5206: loss = 0.8830 (1.287 sec/step)\n",
            "I0919 18:26:09.880991 140474388072320 learning.py:507] global step 5207: loss = 1.0092 (1.312 sec/step)\n",
            "I0919 18:26:11.184967 140474388072320 learning.py:507] global step 5208: loss = 0.8897 (1.302 sec/step)\n",
            "I0919 18:26:12.490619 140474388072320 learning.py:507] global step 5209: loss = 0.8412 (1.304 sec/step)\n",
            "I0919 18:26:13.781996 140474388072320 learning.py:507] global step 5210: loss = 0.6704 (1.290 sec/step)\n",
            "I0919 18:26:15.061368 140474388072320 learning.py:507] global step 5211: loss = 0.8386 (1.278 sec/step)\n",
            "I0919 18:26:16.369570 140474388072320 learning.py:507] global step 5212: loss = 0.6992 (1.307 sec/step)\n",
            "I0919 18:26:17.666467 140474388072320 learning.py:507] global step 5213: loss = 0.7282 (1.295 sec/step)\n",
            "I0919 18:26:18.974870 140474388072320 learning.py:507] global step 5214: loss = 0.7718 (1.307 sec/step)\n",
            "I0919 18:26:20.276293 140474388072320 learning.py:507] global step 5215: loss = 0.8303 (1.300 sec/step)\n",
            "I0919 18:26:21.593465 140474388072320 learning.py:507] global step 5216: loss = 0.6207 (1.316 sec/step)\n",
            "I0919 18:26:22.895195 140474388072320 learning.py:507] global step 5217: loss = 0.8382 (1.300 sec/step)\n",
            "I0919 18:26:24.198395 140474388072320 learning.py:507] global step 5218: loss = 0.7020 (1.301 sec/step)\n",
            "I0919 18:26:25.531322 140474388072320 learning.py:507] global step 5219: loss = 0.6616 (1.331 sec/step)\n",
            "I0919 18:26:26.865406 140474388072320 learning.py:507] global step 5220: loss = 0.6366 (1.332 sec/step)\n",
            "I0919 18:26:28.978548 140474388072320 learning.py:507] global step 5221: loss = 0.6851 (2.110 sec/step)\n",
            "I0919 18:26:28.991084 140471297337088 supervisor.py:1050] Recording summary at step 5221.\n",
            "I0919 18:26:30.296237 140474388072320 learning.py:507] global step 5222: loss = 0.8091 (1.315 sec/step)\n",
            "I0919 18:26:31.601869 140474388072320 learning.py:507] global step 5223: loss = 0.7494 (1.304 sec/step)\n",
            "I0919 18:26:32.887017 140474388072320 learning.py:507] global step 5224: loss = 0.8265 (1.284 sec/step)\n",
            "I0919 18:26:34.195713 140474388072320 learning.py:507] global step 5225: loss = 0.7547 (1.307 sec/step)\n",
            "I0919 18:26:35.504615 140474388072320 learning.py:507] global step 5226: loss = 0.9802 (1.307 sec/step)\n",
            "I0919 18:26:36.838466 140474388072320 learning.py:507] global step 5227: loss = 0.7010 (1.332 sec/step)\n",
            "I0919 18:26:38.129059 140474388072320 learning.py:507] global step 5228: loss = 0.8414 (1.289 sec/step)\n",
            "I0919 18:26:39.487509 140474388072320 learning.py:507] global step 5229: loss = 0.8117 (1.357 sec/step)\n",
            "I0919 18:26:40.812067 140474388072320 learning.py:507] global step 5230: loss = 0.9576 (1.323 sec/step)\n",
            "I0919 18:26:42.122081 140474388072320 learning.py:507] global step 5231: loss = 0.7432 (1.308 sec/step)\n",
            "I0919 18:26:43.410872 140474388072320 learning.py:507] global step 5232: loss = 0.9265 (1.287 sec/step)\n",
            "I0919 18:26:44.722600 140474388072320 learning.py:507] global step 5233: loss = 0.9543 (1.310 sec/step)\n",
            "I0919 18:26:46.032675 140474388072320 learning.py:507] global step 5234: loss = 1.0499 (1.308 sec/step)\n",
            "I0919 18:26:47.359447 140474388072320 learning.py:507] global step 5235: loss = 0.7555 (1.325 sec/step)\n",
            "I0919 18:26:48.652156 140474388072320 learning.py:507] global step 5236: loss = 0.8287 (1.291 sec/step)\n",
            "I0919 18:26:49.961420 140474388072320 learning.py:507] global step 5237: loss = 0.6820 (1.307 sec/step)\n",
            "I0919 18:26:51.326786 140474388072320 learning.py:507] global step 5238: loss = 0.8991 (1.364 sec/step)\n",
            "I0919 18:26:52.675230 140474388072320 learning.py:507] global step 5239: loss = 0.6140 (1.347 sec/step)\n",
            "I0919 18:26:53.980397 140474388072320 learning.py:507] global step 5240: loss = 0.8118 (1.303 sec/step)\n",
            "I0919 18:26:55.265022 140474388072320 learning.py:507] global step 5241: loss = 1.0028 (1.283 sec/step)\n",
            "I0919 18:26:56.567467 140474388072320 learning.py:507] global step 5242: loss = 0.8239 (1.300 sec/step)\n",
            "I0919 18:26:57.889311 140474388072320 learning.py:507] global step 5243: loss = 1.0918 (1.320 sec/step)\n",
            "I0919 18:26:59.204447 140474388072320 learning.py:507] global step 5244: loss = 0.7927 (1.313 sec/step)\n",
            "I0919 18:27:00.568691 140474388072320 learning.py:507] global step 5245: loss = 0.6888 (1.363 sec/step)\n",
            "I0919 18:27:01.907013 140474388072320 learning.py:507] global step 5246: loss = 0.7300 (1.337 sec/step)\n",
            "I0919 18:27:03.229553 140474388072320 learning.py:507] global step 5247: loss = 0.7756 (1.321 sec/step)\n",
            "I0919 18:27:04.554708 140474388072320 learning.py:507] global step 5248: loss = 0.8450 (1.323 sec/step)\n",
            "I0919 18:27:05.853446 140474388072320 learning.py:507] global step 5249: loss = 0.7084 (1.297 sec/step)\n",
            "I0919 18:27:07.150773 140474388072320 learning.py:507] global step 5250: loss = 1.0360 (1.296 sec/step)\n",
            "I0919 18:27:08.441545 140474388072320 learning.py:507] global step 5251: loss = 0.7170 (1.289 sec/step)\n",
            "I0919 18:27:09.746745 140474388072320 learning.py:507] global step 5252: loss = 0.7966 (1.303 sec/step)\n",
            "I0919 18:27:11.081231 140474388072320 learning.py:507] global step 5253: loss = 0.6641 (1.333 sec/step)\n",
            "I0919 18:27:12.373279 140474388072320 learning.py:507] global step 5254: loss = 0.9948 (1.290 sec/step)\n",
            "I0919 18:27:13.674491 140474388072320 learning.py:507] global step 5255: loss = 0.8559 (1.300 sec/step)\n",
            "I0919 18:27:15.031603 140474388072320 learning.py:507] global step 5256: loss = 0.7103 (1.356 sec/step)\n",
            "I0919 18:27:16.336802 140474388072320 learning.py:507] global step 5257: loss = 0.9277 (1.303 sec/step)\n",
            "I0919 18:27:17.646471 140474388072320 learning.py:507] global step 5258: loss = 0.7952 (1.308 sec/step)\n",
            "I0919 18:27:18.986629 140474388072320 learning.py:507] global step 5259: loss = 0.6568 (1.338 sec/step)\n",
            "I0919 18:27:20.286242 140474388072320 learning.py:507] global step 5260: loss = 0.8555 (1.298 sec/step)\n",
            "I0919 18:27:21.650983 140474388072320 learning.py:507] global step 5261: loss = 0.8488 (1.363 sec/step)\n",
            "I0919 18:27:22.944221 140474388072320 learning.py:507] global step 5262: loss = 0.8031 (1.291 sec/step)\n",
            "I0919 18:27:24.316174 140474388072320 learning.py:507] global step 5263: loss = 0.8234 (1.370 sec/step)\n",
            "I0919 18:27:25.613249 140474388072320 learning.py:507] global step 5264: loss = 0.7638 (1.295 sec/step)\n",
            "I0919 18:27:26.943825 140474388072320 learning.py:507] global step 5265: loss = 0.9173 (1.329 sec/step)\n",
            "I0919 18:27:28.240000 140474388072320 learning.py:507] global step 5266: loss = 0.8859 (1.295 sec/step)\n",
            "I0919 18:27:29.561101 140474388072320 learning.py:507] global step 5267: loss = 0.8279 (1.319 sec/step)\n",
            "I0919 18:27:30.875477 140474388072320 learning.py:507] global step 5268: loss = 1.0261 (1.313 sec/step)\n",
            "I0919 18:27:32.203950 140474388072320 learning.py:507] global step 5269: loss = 0.7962 (1.326 sec/step)\n",
            "I0919 18:27:33.543927 140474388072320 learning.py:507] global step 5270: loss = 0.7771 (1.338 sec/step)\n",
            "I0919 18:27:34.851881 140474388072320 learning.py:507] global step 5271: loss = 0.7490 (1.306 sec/step)\n",
            "I0919 18:27:36.180686 140474388072320 learning.py:507] global step 5272: loss = 0.7767 (1.327 sec/step)\n",
            "I0919 18:27:37.475343 140474388072320 learning.py:507] global step 5273: loss = 1.0610 (1.293 sec/step)\n",
            "I0919 18:27:38.776403 140474388072320 learning.py:507] global step 5274: loss = 0.7752 (1.300 sec/step)\n",
            "I0919 18:27:40.088270 140474388072320 learning.py:507] global step 5275: loss = 0.7226 (1.310 sec/step)\n",
            "I0919 18:27:41.392004 140474388072320 learning.py:507] global step 5276: loss = 0.9145 (1.302 sec/step)\n",
            "I0919 18:27:42.686682 140474388072320 learning.py:507] global step 5277: loss = 0.9895 (1.293 sec/step)\n",
            "I0919 18:27:44.032912 140474388072320 learning.py:507] global step 5278: loss = 0.8037 (1.344 sec/step)\n",
            "I0919 18:27:45.336529 140474388072320 learning.py:507] global step 5279: loss = 0.8083 (1.302 sec/step)\n",
            "I0919 18:27:46.630467 140474388072320 learning.py:507] global step 5280: loss = 0.7167 (1.292 sec/step)\n",
            "I0919 18:27:47.944439 140474388072320 learning.py:507] global step 5281: loss = 0.7466 (1.312 sec/step)\n",
            "I0919 18:27:49.260023 140474388072320 learning.py:507] global step 5282: loss = 0.8403 (1.314 sec/step)\n",
            "I0919 18:27:50.562985 140474388072320 learning.py:507] global step 5283: loss = 0.9811 (1.301 sec/step)\n",
            "I0919 18:27:51.916726 140474388072320 learning.py:507] global step 5284: loss = 1.0215 (1.352 sec/step)\n",
            "I0919 18:27:53.239990 140474388072320 learning.py:507] global step 5285: loss = 0.8591 (1.322 sec/step)\n",
            "I0919 18:27:54.543252 140474388072320 learning.py:507] global step 5286: loss = 0.7604 (1.302 sec/step)\n",
            "I0919 18:27:55.837632 140474388072320 learning.py:507] global step 5287: loss = 0.7876 (1.293 sec/step)\n",
            "I0919 18:27:57.135144 140474388072320 learning.py:507] global step 5288: loss = 0.9137 (1.296 sec/step)\n",
            "I0919 18:27:58.441274 140474388072320 learning.py:507] global step 5289: loss = 0.8951 (1.304 sec/step)\n",
            "I0919 18:27:59.767210 140474388072320 learning.py:507] global step 5290: loss = 0.7988 (1.324 sec/step)\n",
            "I0919 18:28:01.049534 140474388072320 learning.py:507] global step 5291: loss = 0.6828 (1.281 sec/step)\n",
            "I0919 18:28:02.346812 140474388072320 learning.py:507] global step 5292: loss = 0.8168 (1.295 sec/step)\n",
            "I0919 18:28:03.690625 140474388072320 learning.py:507] global step 5293: loss = 0.8925 (1.342 sec/step)\n",
            "I0919 18:28:04.995182 140474388072320 learning.py:507] global step 5294: loss = 0.7410 (1.303 sec/step)\n",
            "I0919 18:28:06.294761 140474388072320 learning.py:507] global step 5295: loss = 0.8418 (1.298 sec/step)\n",
            "I0919 18:28:07.616869 140474388072320 learning.py:507] global step 5296: loss = 0.8939 (1.320 sec/step)\n",
            "I0919 18:28:08.918342 140474388072320 learning.py:507] global step 5297: loss = 0.8046 (1.300 sec/step)\n",
            "I0919 18:28:10.233673 140474388072320 learning.py:507] global step 5298: loss = 0.7612 (1.314 sec/step)\n",
            "I0919 18:28:11.602030 140474388072320 learning.py:507] global step 5299: loss = 0.7627 (1.366 sec/step)\n",
            "I0919 18:28:12.931050 140474388072320 learning.py:507] global step 5300: loss = 0.8126 (1.327 sec/step)\n",
            "I0919 18:28:14.252268 140474388072320 learning.py:507] global step 5301: loss = 0.8103 (1.319 sec/step)\n",
            "I0919 18:28:15.538431 140474388072320 learning.py:507] global step 5302: loss = 0.7332 (1.285 sec/step)\n",
            "I0919 18:28:16.848047 140474388072320 learning.py:507] global step 5303: loss = 0.7588 (1.308 sec/step)\n",
            "I0919 18:28:18.158320 140474388072320 learning.py:507] global step 5304: loss = 0.6872 (1.308 sec/step)\n",
            "I0919 18:28:19.464730 140474388072320 learning.py:507] global step 5305: loss = 0.8786 (1.304 sec/step)\n",
            "I0919 18:28:20.777246 140474388072320 learning.py:507] global step 5306: loss = 0.9091 (1.311 sec/step)\n",
            "I0919 18:28:22.051529 140474388072320 learning.py:507] global step 5307: loss = 0.8494 (1.272 sec/step)\n",
            "I0919 18:28:23.358870 140474388072320 learning.py:507] global step 5308: loss = 0.9373 (1.306 sec/step)\n",
            "I0919 18:28:24.687505 140474388072320 learning.py:507] global step 5309: loss = 0.7326 (1.327 sec/step)\n",
            "I0919 18:28:25.977402 140474388072320 learning.py:507] global step 5310: loss = 0.7159 (1.288 sec/step)\n",
            "I0919 18:28:27.438209 140474388072320 learning.py:507] global step 5311: loss = 0.8668 (1.388 sec/step)\n",
            "I0919 18:28:28.981812 140471297337088 supervisor.py:1050] Recording summary at step 5311.\n",
            "I0919 18:28:29.454343 140474388072320 learning.py:507] global step 5312: loss = 0.6932 (1.971 sec/step)\n",
            "I0919 18:28:30.763000 140474388072320 learning.py:507] global step 5313: loss = 1.0288 (1.307 sec/step)\n",
            "I0919 18:28:32.040282 140474388072320 learning.py:507] global step 5314: loss = 0.7082 (1.275 sec/step)\n",
            "I0919 18:28:33.345410 140474388072320 learning.py:507] global step 5315: loss = 0.6358 (1.303 sec/step)\n",
            "I0919 18:28:34.637645 140474388072320 learning.py:507] global step 5316: loss = 0.9137 (1.290 sec/step)\n",
            "I0919 18:28:35.948467 140474388072320 learning.py:507] global step 5317: loss = 0.8072 (1.309 sec/step)\n",
            "I0919 18:28:37.289350 140474388072320 learning.py:507] global step 5318: loss = 0.7557 (1.339 sec/step)\n",
            "I0919 18:28:38.603410 140474388072320 learning.py:507] global step 5319: loss = 0.7925 (1.313 sec/step)\n",
            "I0919 18:28:39.901622 140474388072320 learning.py:507] global step 5320: loss = 0.6851 (1.296 sec/step)\n",
            "I0919 18:28:41.228059 140474388072320 learning.py:507] global step 5321: loss = 1.4369 (1.325 sec/step)\n",
            "I0919 18:28:42.549061 140474388072320 learning.py:507] global step 5322: loss = 0.8978 (1.319 sec/step)\n",
            "I0919 18:28:43.880683 140474388072320 learning.py:507] global step 5323: loss = 0.7527 (1.330 sec/step)\n",
            "I0919 18:28:45.200169 140474388072320 learning.py:507] global step 5324: loss = 0.5698 (1.318 sec/step)\n",
            "I0919 18:28:46.499460 140474388072320 learning.py:507] global step 5325: loss = 0.8871 (1.298 sec/step)\n",
            "I0919 18:28:47.845896 140474388072320 learning.py:507] global step 5326: loss = 0.5974 (1.344 sec/step)\n",
            "I0919 18:28:49.169612 140474388072320 learning.py:507] global step 5327: loss = 0.8079 (1.322 sec/step)\n",
            "I0919 18:28:50.453931 140474388072320 learning.py:507] global step 5328: loss = 0.8707 (1.282 sec/step)\n",
            "I0919 18:28:51.814696 140474388072320 learning.py:507] global step 5329: loss = 0.9681 (1.359 sec/step)\n",
            "I0919 18:28:53.126235 140474388072320 learning.py:507] global step 5330: loss = 0.7302 (1.310 sec/step)\n",
            "I0919 18:28:54.463968 140474388072320 learning.py:507] global step 5331: loss = 0.7293 (1.336 sec/step)\n",
            "I0919 18:28:55.789459 140474388072320 learning.py:507] global step 5332: loss = 0.7272 (1.324 sec/step)\n",
            "I0919 18:28:57.125608 140474388072320 learning.py:507] global step 5333: loss = 0.8821 (1.334 sec/step)\n",
            "I0919 18:28:58.442595 140474388072320 learning.py:507] global step 5334: loss = 1.4851 (1.315 sec/step)\n",
            "I0919 18:28:59.750134 140474388072320 learning.py:507] global step 5335: loss = 0.9126 (1.306 sec/step)\n",
            "I0919 18:29:01.084098 140474388072320 learning.py:507] global step 5336: loss = 0.7830 (1.332 sec/step)\n",
            "I0919 18:29:02.421049 140474388072320 learning.py:507] global step 5337: loss = 0.8544 (1.335 sec/step)\n",
            "I0919 18:29:03.720638 140474388072320 learning.py:507] global step 5338: loss = 0.7659 (1.298 sec/step)\n",
            "I0919 18:29:05.025965 140474388072320 learning.py:507] global step 5339: loss = 1.1188 (1.303 sec/step)\n",
            "I0919 18:29:06.340261 140474388072320 learning.py:507] global step 5340: loss = 0.8223 (1.312 sec/step)\n",
            "I0919 18:29:07.667433 140474388072320 learning.py:507] global step 5341: loss = 0.8943 (1.325 sec/step)\n",
            "I0919 18:29:08.969634 140474388072320 learning.py:507] global step 5342: loss = 0.7722 (1.300 sec/step)\n",
            "I0919 18:29:10.329760 140474388072320 learning.py:507] global step 5343: loss = 0.8252 (1.359 sec/step)\n",
            "I0919 18:29:11.629970 140474388072320 learning.py:507] global step 5344: loss = 0.7919 (1.299 sec/step)\n",
            "I0919 18:29:12.960792 140474388072320 learning.py:507] global step 5345: loss = 0.7275 (1.329 sec/step)\n",
            "I0919 18:29:14.276525 140474388072320 learning.py:507] global step 5346: loss = 0.8225 (1.314 sec/step)\n",
            "I0919 18:29:15.597016 140474388072320 learning.py:507] global step 5347: loss = 1.0884 (1.319 sec/step)\n",
            "I0919 18:29:16.961770 140474388072320 learning.py:507] global step 5348: loss = 0.9061 (1.363 sec/step)\n",
            "I0919 18:29:18.291455 140474388072320 learning.py:507] global step 5349: loss = 1.1449 (1.328 sec/step)\n",
            "I0919 18:29:19.647536 140474388072320 learning.py:507] global step 5350: loss = 0.6523 (1.355 sec/step)\n",
            "I0919 18:29:20.948916 140474388072320 learning.py:507] global step 5351: loss = 0.7979 (1.299 sec/step)\n",
            "I0919 18:29:22.251188 140474388072320 learning.py:507] global step 5352: loss = 0.9228 (1.300 sec/step)\n",
            "I0919 18:29:23.539241 140474388072320 learning.py:507] global step 5353: loss = 1.1086 (1.286 sec/step)\n",
            "I0919 18:29:24.876013 140474388072320 learning.py:507] global step 5354: loss = 0.7607 (1.335 sec/step)\n",
            "I0919 18:29:26.187155 140474388072320 learning.py:507] global step 5355: loss = 0.8776 (1.309 sec/step)\n",
            "I0919 18:29:27.493761 140474388072320 learning.py:507] global step 5356: loss = 0.8087 (1.305 sec/step)\n",
            "I0919 18:29:28.836549 140474388072320 learning.py:507] global step 5357: loss = 1.0899 (1.341 sec/step)\n",
            "I0919 18:29:30.175574 140474388072320 learning.py:507] global step 5358: loss = 0.6355 (1.337 sec/step)\n",
            "I0919 18:29:31.511009 140474388072320 learning.py:507] global step 5359: loss = 0.8625 (1.334 sec/step)\n",
            "I0919 18:29:32.835573 140474388072320 learning.py:507] global step 5360: loss = 0.9794 (1.323 sec/step)\n",
            "I0919 18:29:34.136684 140474388072320 learning.py:507] global step 5361: loss = 0.9973 (1.300 sec/step)\n",
            "I0919 18:29:35.437412 140474388072320 learning.py:507] global step 5362: loss = 0.7155 (1.299 sec/step)\n",
            "I0919 18:29:36.779016 140474388072320 learning.py:507] global step 5363: loss = 0.6483 (1.340 sec/step)\n",
            "I0919 18:29:38.122140 140474388072320 learning.py:507] global step 5364: loss = 0.9470 (1.341 sec/step)\n",
            "I0919 18:29:39.434185 140474388072320 learning.py:507] global step 5365: loss = 0.7564 (1.310 sec/step)\n",
            "I0919 18:29:40.739667 140474388072320 learning.py:507] global step 5366: loss = 0.8685 (1.304 sec/step)\n",
            "I0919 18:29:42.044655 140474388072320 learning.py:507] global step 5367: loss = 0.8324 (1.303 sec/step)\n",
            "I0919 18:29:43.391571 140474388072320 learning.py:507] global step 5368: loss = 0.8841 (1.345 sec/step)\n",
            "I0919 18:29:44.691482 140474388072320 learning.py:507] global step 5369: loss = 0.6452 (1.298 sec/step)\n",
            "I0919 18:29:46.007834 140474388072320 learning.py:507] global step 5370: loss = 0.6637 (1.314 sec/step)\n",
            "I0919 18:29:47.296035 140474388072320 learning.py:507] global step 5371: loss = 0.7521 (1.286 sec/step)\n",
            "I0919 18:29:48.653861 140474388072320 learning.py:507] global step 5372: loss = 0.8456 (1.356 sec/step)\n",
            "I0919 18:29:49.991825 140474388072320 learning.py:507] global step 5373: loss = 0.9158 (1.336 sec/step)\n",
            "I0919 18:29:51.291979 140474388072320 learning.py:507] global step 5374: loss = 1.0699 (1.299 sec/step)\n",
            "I0919 18:29:52.607335 140474388072320 learning.py:507] global step 5375: loss = 0.5920 (1.313 sec/step)\n",
            "I0919 18:29:53.937736 140474388072320 learning.py:507] global step 5376: loss = 0.6712 (1.328 sec/step)\n",
            "I0919 18:29:55.280639 140474388072320 learning.py:507] global step 5377: loss = 0.7766 (1.341 sec/step)\n",
            "I0919 18:29:56.581079 140474388072320 learning.py:507] global step 5378: loss = 0.7886 (1.299 sec/step)\n",
            "I0919 18:29:57.901947 140474388072320 learning.py:507] global step 5379: loss = 0.7463 (1.319 sec/step)\n",
            "I0919 18:29:59.249874 140474388072320 learning.py:507] global step 5380: loss = 1.0163 (1.346 sec/step)\n",
            "I0919 18:30:00.557693 140474388072320 learning.py:507] global step 5381: loss = 0.9211 (1.306 sec/step)\n",
            "I0919 18:30:01.860745 140474388072320 learning.py:507] global step 5382: loss = 0.7933 (1.301 sec/step)\n",
            "I0919 18:30:03.193048 140474388072320 learning.py:507] global step 5383: loss = 0.7347 (1.330 sec/step)\n",
            "I0919 18:30:04.485881 140474388072320 learning.py:507] global step 5384: loss = 0.6589 (1.291 sec/step)\n",
            "I0919 18:30:05.782397 140474388072320 learning.py:507] global step 5385: loss = 0.5939 (1.295 sec/step)\n",
            "I0919 18:30:07.093599 140474388072320 learning.py:507] global step 5386: loss = 0.7030 (1.309 sec/step)\n",
            "I0919 18:30:08.440197 140474388072320 learning.py:507] global step 5387: loss = 0.7597 (1.345 sec/step)\n",
            "I0919 18:30:09.754590 140474388072320 learning.py:507] global step 5388: loss = 0.8222 (1.313 sec/step)\n",
            "I0919 18:30:11.079757 140474388072320 learning.py:507] global step 5389: loss = 0.8326 (1.323 sec/step)\n",
            "I0919 18:30:12.412240 140474388072320 learning.py:507] global step 5390: loss = 0.7510 (1.330 sec/step)\n",
            "I0919 18:30:13.726521 140474388072320 learning.py:507] global step 5391: loss = 0.9704 (1.312 sec/step)\n",
            "I0919 18:30:15.067439 140474388072320 learning.py:507] global step 5392: loss = 1.0757 (1.339 sec/step)\n",
            "I0919 18:30:16.386053 140474388072320 learning.py:507] global step 5393: loss = 0.8027 (1.317 sec/step)\n",
            "I0919 18:30:17.747378 140474388072320 learning.py:507] global step 5394: loss = 1.1035 (1.360 sec/step)\n",
            "I0919 18:30:19.072793 140474388072320 learning.py:507] global step 5395: loss = 0.8079 (1.324 sec/step)\n",
            "I0919 18:30:20.374948 140474388072320 learning.py:507] global step 5396: loss = 0.9059 (1.300 sec/step)\n",
            "I0919 18:30:21.723186 140474388072320 learning.py:507] global step 5397: loss = 0.7335 (1.346 sec/step)\n",
            "I0919 18:30:23.014158 140474388072320 learning.py:507] global step 5398: loss = 0.7223 (1.289 sec/step)\n",
            "I0919 18:30:24.332470 140474388072320 learning.py:507] global step 5399: loss = 0.8597 (1.316 sec/step)\n",
            "I0919 18:30:25.628941 140474388072320 learning.py:507] global step 5400: loss = 0.8683 (1.295 sec/step)\n",
            "I0919 18:30:26.872399 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 18:30:26.971352 140474388072320 learning.py:507] global step 5401: loss = 0.9984 (1.341 sec/step)\n",
            "I0919 18:30:28.747246 140471297337088 supervisor.py:1050] Recording summary at step 5401.\n",
            "I0919 18:30:30.050798 140474388072320 learning.py:507] global step 5402: loss = 0.9397 (2.132 sec/step)\n",
            "I0919 18:30:32.143349 140474388072320 learning.py:507] global step 5403: loss = 0.8315 (2.090 sec/step)\n",
            "I0919 18:30:33.768831 140474388072320 learning.py:507] global step 5404: loss = 0.8846 (1.619 sec/step)\n",
            "I0919 18:30:35.087065 140474388072320 learning.py:507] global step 5405: loss = 0.7802 (1.316 sec/step)\n",
            "I0919 18:30:36.425263 140474388072320 learning.py:507] global step 5406: loss = 0.7421 (1.336 sec/step)\n",
            "I0919 18:30:37.721889 140474388072320 learning.py:507] global step 5407: loss = 1.0076 (1.295 sec/step)\n",
            "I0919 18:30:39.020152 140474388072320 learning.py:507] global step 5408: loss = 0.7258 (1.296 sec/step)\n",
            "I0919 18:30:40.317904 140474388072320 learning.py:507] global step 5409: loss = 0.7190 (1.296 sec/step)\n",
            "I0919 18:30:41.630965 140474388072320 learning.py:507] global step 5410: loss = 0.7298 (1.312 sec/step)\n",
            "I0919 18:30:42.967905 140474388072320 learning.py:507] global step 5411: loss = 0.9411 (1.335 sec/step)\n",
            "I0919 18:30:44.327405 140474388072320 learning.py:507] global step 5412: loss = 0.9862 (1.358 sec/step)\n",
            "I0919 18:30:45.639323 140474388072320 learning.py:507] global step 5413: loss = 0.9888 (1.310 sec/step)\n",
            "I0919 18:30:46.948040 140474388072320 learning.py:507] global step 5414: loss = 1.0957 (1.307 sec/step)\n",
            "I0919 18:30:48.266470 140474388072320 learning.py:507] global step 5415: loss = 0.8237 (1.317 sec/step)\n",
            "I0919 18:30:49.584686 140474388072320 learning.py:507] global step 5416: loss = 0.9369 (1.316 sec/step)\n",
            "I0919 18:30:50.902961 140474388072320 learning.py:507] global step 5417: loss = 0.7203 (1.316 sec/step)\n",
            "I0919 18:30:52.222826 140474388072320 learning.py:507] global step 5418: loss = 1.2426 (1.318 sec/step)\n",
            "I0919 18:30:53.593552 140474388072320 learning.py:507] global step 5419: loss = 0.7450 (1.369 sec/step)\n",
            "I0919 18:30:54.948503 140474388072320 learning.py:507] global step 5420: loss = 0.9044 (1.353 sec/step)\n",
            "I0919 18:30:56.279931 140474388072320 learning.py:507] global step 5421: loss = 0.6351 (1.330 sec/step)\n",
            "I0919 18:30:57.570913 140474388072320 learning.py:507] global step 5422: loss = 0.6427 (1.289 sec/step)\n",
            "I0919 18:30:58.889769 140474388072320 learning.py:507] global step 5423: loss = 0.8378 (1.317 sec/step)\n",
            "I0919 18:31:00.221472 140474388072320 learning.py:507] global step 5424: loss = 0.6833 (1.330 sec/step)\n",
            "I0919 18:31:01.546205 140474388072320 learning.py:507] global step 5425: loss = 0.9090 (1.323 sec/step)\n",
            "I0919 18:31:02.843039 140474388072320 learning.py:507] global step 5426: loss = 0.7551 (1.295 sec/step)\n",
            "I0919 18:31:04.128462 140474388072320 learning.py:507] global step 5427: loss = 0.9654 (1.284 sec/step)\n",
            "I0919 18:31:05.471930 140474388072320 learning.py:507] global step 5428: loss = 0.8758 (1.342 sec/step)\n",
            "I0919 18:31:06.842354 140474388072320 learning.py:507] global step 5429: loss = 0.7164 (1.369 sec/step)\n",
            "I0919 18:31:08.157124 140474388072320 learning.py:507] global step 5430: loss = 0.6513 (1.312 sec/step)\n",
            "I0919 18:31:09.449456 140474388072320 learning.py:507] global step 5431: loss = 0.7365 (1.291 sec/step)\n",
            "I0919 18:31:10.826505 140474388072320 learning.py:507] global step 5432: loss = 0.8342 (1.375 sec/step)\n",
            "I0919 18:31:12.138434 140474388072320 learning.py:507] global step 5433: loss = 0.7340 (1.310 sec/step)\n",
            "I0919 18:31:13.456501 140474388072320 learning.py:507] global step 5434: loss = 0.9360 (1.316 sec/step)\n",
            "I0919 18:31:14.754149 140474388072320 learning.py:507] global step 5435: loss = 0.5952 (1.296 sec/step)\n",
            "I0919 18:31:16.051346 140474388072320 learning.py:507] global step 5436: loss = 0.8509 (1.296 sec/step)\n",
            "I0919 18:31:17.352421 140474388072320 learning.py:507] global step 5437: loss = 0.8852 (1.299 sec/step)\n",
            "I0919 18:31:18.670001 140474388072320 learning.py:507] global step 5438: loss = 0.7716 (1.316 sec/step)\n",
            "I0919 18:31:19.998094 140474388072320 learning.py:507] global step 5439: loss = 0.9544 (1.326 sec/step)\n",
            "I0919 18:31:21.334445 140474388072320 learning.py:507] global step 5440: loss = 0.7923 (1.334 sec/step)\n",
            "I0919 18:31:22.663314 140474388072320 learning.py:507] global step 5441: loss = 0.7830 (1.327 sec/step)\n",
            "I0919 18:31:23.946388 140474388072320 learning.py:507] global step 5442: loss = 1.0617 (1.281 sec/step)\n",
            "I0919 18:31:25.258605 140474388072320 learning.py:507] global step 5443: loss = 0.6589 (1.310 sec/step)\n",
            "I0919 18:31:26.573052 140474388072320 learning.py:507] global step 5444: loss = 0.7562 (1.313 sec/step)\n",
            "I0919 18:31:27.918246 140474388072320 learning.py:507] global step 5445: loss = 0.8848 (1.343 sec/step)\n",
            "I0919 18:31:29.230894 140474388072320 learning.py:507] global step 5446: loss = 0.6739 (1.311 sec/step)\n",
            "I0919 18:31:30.531155 140474388072320 learning.py:507] global step 5447: loss = 0.6609 (1.298 sec/step)\n",
            "I0919 18:31:31.851747 140474388072320 learning.py:507] global step 5448: loss = 0.9289 (1.319 sec/step)\n",
            "I0919 18:31:33.150871 140474388072320 learning.py:507] global step 5449: loss = 0.7896 (1.297 sec/step)\n",
            "I0919 18:31:34.462470 140474388072320 learning.py:507] global step 5450: loss = 0.7404 (1.310 sec/step)\n",
            "I0919 18:31:35.788434 140474388072320 learning.py:507] global step 5451: loss = 1.0890 (1.324 sec/step)\n",
            "I0919 18:31:37.094847 140474388072320 learning.py:507] global step 5452: loss = 0.7011 (1.305 sec/step)\n",
            "I0919 18:31:38.383472 140474388072320 learning.py:507] global step 5453: loss = 0.7733 (1.287 sec/step)\n",
            "I0919 18:31:39.722820 140474388072320 learning.py:507] global step 5454: loss = 0.7436 (1.338 sec/step)\n",
            "I0919 18:31:41.050659 140474388072320 learning.py:507] global step 5455: loss = 0.7906 (1.326 sec/step)\n",
            "I0919 18:31:42.391885 140474388072320 learning.py:507] global step 5456: loss = 0.8154 (1.339 sec/step)\n",
            "I0919 18:31:43.687175 140474388072320 learning.py:507] global step 5457: loss = 0.9182 (1.293 sec/step)\n",
            "I0919 18:31:45.044475 140474388072320 learning.py:507] global step 5458: loss = 0.7640 (1.355 sec/step)\n",
            "I0919 18:31:46.332886 140474388072320 learning.py:507] global step 5459: loss = 0.6503 (1.286 sec/step)\n",
            "I0919 18:31:47.624130 140474388072320 learning.py:507] global step 5460: loss = 0.8940 (1.290 sec/step)\n",
            "I0919 18:31:48.954294 140474388072320 learning.py:507] global step 5461: loss = 0.8209 (1.328 sec/step)\n",
            "I0919 18:31:50.286619 140474388072320 learning.py:507] global step 5462: loss = 0.8810 (1.330 sec/step)\n",
            "I0919 18:31:51.598267 140474388072320 learning.py:507] global step 5463: loss = 0.8268 (1.310 sec/step)\n",
            "I0919 18:31:52.950473 140474388072320 learning.py:507] global step 5464: loss = 0.8593 (1.350 sec/step)\n",
            "I0919 18:31:54.288986 140474388072320 learning.py:507] global step 5465: loss = 0.8664 (1.337 sec/step)\n",
            "I0919 18:31:55.657861 140474388072320 learning.py:507] global step 5466: loss = 0.6967 (1.367 sec/step)\n",
            "I0919 18:31:57.007042 140474388072320 learning.py:507] global step 5467: loss = 0.6483 (1.348 sec/step)\n",
            "I0919 18:31:58.333857 140474388072320 learning.py:507] global step 5468: loss = 1.0751 (1.325 sec/step)\n",
            "I0919 18:31:59.678970 140474388072320 learning.py:507] global step 5469: loss = 0.7011 (1.343 sec/step)\n",
            "I0919 18:32:01.022653 140474388072320 learning.py:507] global step 5470: loss = 0.6199 (1.342 sec/step)\n",
            "I0919 18:32:02.340728 140474388072320 learning.py:507] global step 5471: loss = 0.9830 (1.316 sec/step)\n",
            "I0919 18:32:03.635636 140474388072320 learning.py:507] global step 5472: loss = 0.9128 (1.293 sec/step)\n",
            "I0919 18:32:04.946767 140474388072320 learning.py:507] global step 5473: loss = 0.8286 (1.309 sec/step)\n",
            "I0919 18:32:06.241559 140474388072320 learning.py:507] global step 5474: loss = 0.7396 (1.293 sec/step)\n",
            "I0919 18:32:07.552234 140474388072320 learning.py:507] global step 5475: loss = 1.0210 (1.309 sec/step)\n",
            "I0919 18:32:08.871824 140474388072320 learning.py:507] global step 5476: loss = 0.7799 (1.318 sec/step)\n",
            "I0919 18:32:10.170303 140474388072320 learning.py:507] global step 5477: loss = 0.9041 (1.297 sec/step)\n",
            "I0919 18:32:11.540554 140474388072320 learning.py:507] global step 5478: loss = 0.6471 (1.368 sec/step)\n",
            "I0919 18:32:12.854028 140474388072320 learning.py:507] global step 5479: loss = 0.7539 (1.312 sec/step)\n",
            "I0919 18:32:14.154552 140474388072320 learning.py:507] global step 5480: loss = 0.8344 (1.299 sec/step)\n",
            "I0919 18:32:15.468436 140474388072320 learning.py:507] global step 5481: loss = 0.9995 (1.312 sec/step)\n",
            "I0919 18:32:16.803386 140474388072320 learning.py:507] global step 5482: loss = 0.6840 (1.333 sec/step)\n",
            "I0919 18:32:18.160934 140474388072320 learning.py:507] global step 5483: loss = 0.6531 (1.356 sec/step)\n",
            "I0919 18:32:19.456011 140474388072320 learning.py:507] global step 5484: loss = 1.0283 (1.294 sec/step)\n",
            "I0919 18:32:20.795923 140474388072320 learning.py:507] global step 5485: loss = 0.9921 (1.338 sec/step)\n",
            "I0919 18:32:22.099102 140474388072320 learning.py:507] global step 5486: loss = 0.6533 (1.301 sec/step)\n",
            "I0919 18:32:23.441335 140474388072320 learning.py:507] global step 5487: loss = 0.7416 (1.340 sec/step)\n",
            "I0919 18:32:24.776469 140474388072320 learning.py:507] global step 5488: loss = 0.7721 (1.333 sec/step)\n",
            "I0919 18:32:26.086694 140474388072320 learning.py:507] global step 5489: loss = 0.9001 (1.308 sec/step)\n",
            "I0919 18:32:27.384634 140474388072320 learning.py:507] global step 5490: loss = 0.7435 (1.296 sec/step)\n",
            "I0919 18:32:29.236812 140471297337088 supervisor.py:1050] Recording summary at step 5490.\n",
            "I0919 18:32:29.672758 140474388072320 learning.py:507] global step 5491: loss = 0.7827 (1.989 sec/step)\n",
            "I0919 18:32:31.011466 140474388072320 learning.py:507] global step 5492: loss = 0.7640 (1.337 sec/step)\n",
            "I0919 18:32:32.330667 140474388072320 learning.py:507] global step 5493: loss = 0.7330 (1.317 sec/step)\n",
            "I0919 18:32:33.658523 140474388072320 learning.py:507] global step 5494: loss = 0.8606 (1.326 sec/step)\n",
            "I0919 18:32:34.950503 140474388072320 learning.py:507] global step 5495: loss = 0.9094 (1.290 sec/step)\n",
            "I0919 18:32:36.290009 140474388072320 learning.py:507] global step 5496: loss = 0.7990 (1.338 sec/step)\n",
            "I0919 18:32:37.649919 140474388072320 learning.py:507] global step 5497: loss = 0.9317 (1.358 sec/step)\n",
            "I0919 18:32:38.949728 140474388072320 learning.py:507] global step 5498: loss = 0.6373 (1.298 sec/step)\n",
            "I0919 18:32:40.249325 140474388072320 learning.py:507] global step 5499: loss = 1.0590 (1.298 sec/step)\n",
            "I0919 18:32:41.578336 140474388072320 learning.py:507] global step 5500: loss = 0.9387 (1.327 sec/step)\n",
            "I0919 18:32:42.905229 140474388072320 learning.py:507] global step 5501: loss = 0.6952 (1.325 sec/step)\n",
            "I0919 18:32:44.221157 140474388072320 learning.py:507] global step 5502: loss = 0.8538 (1.314 sec/step)\n",
            "I0919 18:32:45.571556 140474388072320 learning.py:507] global step 5503: loss = 0.8265 (1.349 sec/step)\n",
            "I0919 18:32:46.882205 140474388072320 learning.py:507] global step 5504: loss = 0.8393 (1.309 sec/step)\n",
            "I0919 18:32:48.181504 140474388072320 learning.py:507] global step 5505: loss = 0.7536 (1.297 sec/step)\n",
            "I0919 18:32:49.499774 140474388072320 learning.py:507] global step 5506: loss = 0.7442 (1.316 sec/step)\n",
            "I0919 18:32:50.851569 140474388072320 learning.py:507] global step 5507: loss = 0.8742 (1.350 sec/step)\n",
            "I0919 18:32:52.206526 140474388072320 learning.py:507] global step 5508: loss = 0.7903 (1.353 sec/step)\n",
            "I0919 18:32:53.531141 140474388072320 learning.py:507] global step 5509: loss = 0.8402 (1.323 sec/step)\n",
            "I0919 18:32:54.908166 140474388072320 learning.py:507] global step 5510: loss = 0.8922 (1.375 sec/step)\n",
            "I0919 18:32:56.229855 140474388072320 learning.py:507] global step 5511: loss = 1.0237 (1.320 sec/step)\n",
            "I0919 18:32:57.562958 140474388072320 learning.py:507] global step 5512: loss = 0.6843 (1.331 sec/step)\n",
            "I0919 18:32:58.880944 140474388072320 learning.py:507] global step 5513: loss = 0.8216 (1.316 sec/step)\n",
            "I0919 18:33:00.204745 140474388072320 learning.py:507] global step 5514: loss = 0.8415 (1.322 sec/step)\n",
            "I0919 18:33:01.528855 140474388072320 learning.py:507] global step 5515: loss = 0.7220 (1.322 sec/step)\n",
            "I0919 18:33:02.824877 140474388072320 learning.py:507] global step 5516: loss = 0.7442 (1.294 sec/step)\n",
            "I0919 18:33:04.109797 140474388072320 learning.py:507] global step 5517: loss = 0.7806 (1.283 sec/step)\n",
            "I0919 18:33:05.416479 140474388072320 learning.py:507] global step 5518: loss = 1.0315 (1.305 sec/step)\n",
            "I0919 18:33:06.699889 140474388072320 learning.py:507] global step 5519: loss = 1.0803 (1.282 sec/step)\n",
            "I0919 18:33:07.990432 140474388072320 learning.py:507] global step 5520: loss = 0.8141 (1.289 sec/step)\n",
            "I0919 18:33:09.302489 140474388072320 learning.py:507] global step 5521: loss = 0.7414 (1.310 sec/step)\n",
            "I0919 18:33:10.637640 140474388072320 learning.py:507] global step 5522: loss = 0.8420 (1.334 sec/step)\n",
            "I0919 18:33:11.970087 140474388072320 learning.py:507] global step 5523: loss = 0.9037 (1.331 sec/step)\n",
            "I0919 18:33:13.270642 140474388072320 learning.py:507] global step 5524: loss = 0.8348 (1.299 sec/step)\n",
            "I0919 18:33:14.580394 140474388072320 learning.py:507] global step 5525: loss = 0.9119 (1.308 sec/step)\n",
            "I0919 18:33:15.894990 140474388072320 learning.py:507] global step 5526: loss = 0.9358 (1.313 sec/step)\n",
            "I0919 18:33:17.261533 140474388072320 learning.py:507] global step 5527: loss = 0.9640 (1.364 sec/step)\n",
            "I0919 18:33:18.606203 140474388072320 learning.py:507] global step 5528: loss = 1.0203 (1.343 sec/step)\n",
            "I0919 18:33:19.958895 140474388072320 learning.py:507] global step 5529: loss = 0.9340 (1.351 sec/step)\n",
            "I0919 18:33:21.256180 140474388072320 learning.py:507] global step 5530: loss = 0.8520 (1.295 sec/step)\n",
            "I0919 18:33:22.543915 140474388072320 learning.py:507] global step 5531: loss = 0.9125 (1.286 sec/step)\n",
            "I0919 18:33:23.863672 140474388072320 learning.py:507] global step 5532: loss = 0.8455 (1.318 sec/step)\n",
            "I0919 18:33:25.168348 140474388072320 learning.py:507] global step 5533: loss = 0.9064 (1.303 sec/step)\n",
            "I0919 18:33:26.469489 140474388072320 learning.py:507] global step 5534: loss = 0.8372 (1.299 sec/step)\n",
            "I0919 18:33:27.762973 140474388072320 learning.py:507] global step 5535: loss = 0.9744 (1.292 sec/step)\n",
            "I0919 18:33:29.113827 140474388072320 learning.py:507] global step 5536: loss = 0.8549 (1.349 sec/step)\n",
            "I0919 18:33:30.429128 140474388072320 learning.py:507] global step 5537: loss = 0.8175 (1.313 sec/step)\n",
            "I0919 18:33:31.737481 140474388072320 learning.py:507] global step 5538: loss = 0.7618 (1.307 sec/step)\n",
            "I0919 18:33:33.066143 140474388072320 learning.py:507] global step 5539: loss = 0.6663 (1.327 sec/step)\n",
            "I0919 18:33:34.361217 140474388072320 learning.py:507] global step 5540: loss = 0.8034 (1.293 sec/step)\n",
            "I0919 18:33:35.641616 140474388072320 learning.py:507] global step 5541: loss = 0.7581 (1.279 sec/step)\n",
            "I0919 18:33:36.961378 140474388072320 learning.py:507] global step 5542: loss = 0.7954 (1.318 sec/step)\n",
            "I0919 18:33:38.279287 140474388072320 learning.py:507] global step 5543: loss = 0.8331 (1.316 sec/step)\n",
            "I0919 18:33:39.591820 140474388072320 learning.py:507] global step 5544: loss = 0.7895 (1.311 sec/step)\n",
            "I0919 18:33:40.918201 140474388072320 learning.py:507] global step 5545: loss = 0.9885 (1.324 sec/step)\n",
            "I0919 18:33:42.240876 140474388072320 learning.py:507] global step 5546: loss = 0.7214 (1.321 sec/step)\n",
            "I0919 18:33:43.546197 140474388072320 learning.py:507] global step 5547: loss = 1.0034 (1.303 sec/step)\n",
            "I0919 18:33:44.857157 140474388072320 learning.py:507] global step 5548: loss = 0.9088 (1.309 sec/step)\n",
            "I0919 18:33:46.162100 140474388072320 learning.py:507] global step 5549: loss = 0.6411 (1.303 sec/step)\n",
            "I0919 18:33:47.522334 140474388072320 learning.py:507] global step 5550: loss = 0.7569 (1.359 sec/step)\n",
            "I0919 18:33:48.837644 140474388072320 learning.py:507] global step 5551: loss = 0.8042 (1.314 sec/step)\n",
            "I0919 18:33:50.173985 140474388072320 learning.py:507] global step 5552: loss = 0.9642 (1.334 sec/step)\n",
            "I0919 18:33:51.507982 140474388072320 learning.py:507] global step 5553: loss = 1.0271 (1.332 sec/step)\n",
            "I0919 18:33:52.812470 140474388072320 learning.py:507] global step 5554: loss = 1.0009 (1.303 sec/step)\n",
            "I0919 18:33:54.142914 140474388072320 learning.py:507] global step 5555: loss = 0.7473 (1.329 sec/step)\n",
            "I0919 18:33:55.429187 140474388072320 learning.py:507] global step 5556: loss = 0.8029 (1.285 sec/step)\n",
            "I0919 18:33:56.810154 140474388072320 learning.py:507] global step 5557: loss = 1.0617 (1.379 sec/step)\n",
            "I0919 18:33:58.121197 140474388072320 learning.py:507] global step 5558: loss = 1.0035 (1.309 sec/step)\n",
            "I0919 18:33:59.404490 140474388072320 learning.py:507] global step 5559: loss = 0.8860 (1.282 sec/step)\n",
            "I0919 18:34:00.735883 140474388072320 learning.py:507] global step 5560: loss = 1.3490 (1.330 sec/step)\n",
            "I0919 18:34:02.021615 140474388072320 learning.py:507] global step 5561: loss = 0.6342 (1.284 sec/step)\n",
            "I0919 18:34:03.354620 140474388072320 learning.py:507] global step 5562: loss = 0.7586 (1.331 sec/step)\n",
            "I0919 18:34:04.685347 140474388072320 learning.py:507] global step 5563: loss = 1.0655 (1.329 sec/step)\n",
            "I0919 18:34:05.997562 140474388072320 learning.py:507] global step 5564: loss = 0.8154 (1.310 sec/step)\n",
            "I0919 18:34:07.352793 140474388072320 learning.py:507] global step 5565: loss = 0.9258 (1.353 sec/step)\n",
            "I0919 18:34:08.654831 140474388072320 learning.py:507] global step 5566: loss = 0.8388 (1.300 sec/step)\n",
            "I0919 18:34:09.996503 140474388072320 learning.py:507] global step 5567: loss = 0.7831 (1.340 sec/step)\n",
            "I0919 18:34:11.316745 140474388072320 learning.py:507] global step 5568: loss = 0.7678 (1.318 sec/step)\n",
            "I0919 18:34:12.655668 140474388072320 learning.py:507] global step 5569: loss = 0.9635 (1.337 sec/step)\n",
            "I0919 18:34:13.961298 140474388072320 learning.py:507] global step 5570: loss = 0.8306 (1.304 sec/step)\n",
            "I0919 18:34:15.282809 140474388072320 learning.py:507] global step 5571: loss = 0.9254 (1.320 sec/step)\n",
            "I0919 18:34:16.583587 140474388072320 learning.py:507] global step 5572: loss = 0.9225 (1.299 sec/step)\n",
            "I0919 18:34:17.913929 140474388072320 learning.py:507] global step 5573: loss = 0.8053 (1.329 sec/step)\n",
            "I0919 18:34:19.234406 140474388072320 learning.py:507] global step 5574: loss = 0.9899 (1.319 sec/step)\n",
            "I0919 18:34:20.548696 140474388072320 learning.py:507] global step 5575: loss = 0.8193 (1.313 sec/step)\n",
            "I0919 18:34:21.845799 140474388072320 learning.py:507] global step 5576: loss = 0.8893 (1.296 sec/step)\n",
            "I0919 18:34:23.174206 140474388072320 learning.py:507] global step 5577: loss = 0.6997 (1.327 sec/step)\n",
            "I0919 18:34:24.499094 140474388072320 learning.py:507] global step 5578: loss = 1.0804 (1.323 sec/step)\n",
            "I0919 18:34:25.854172 140474388072320 learning.py:507] global step 5579: loss = 0.8318 (1.353 sec/step)\n",
            "I0919 18:34:27.167520 140474388072320 learning.py:507] global step 5580: loss = 0.8121 (1.301 sec/step)\n",
            "I0919 18:34:28.805970 140471297337088 supervisor.py:1050] Recording summary at step 5580.\n",
            "I0919 18:34:29.404236 140474388072320 learning.py:507] global step 5581: loss = 0.8034 (2.235 sec/step)\n",
            "I0919 18:34:30.720558 140474388072320 learning.py:507] global step 5582: loss = 0.8734 (1.314 sec/step)\n",
            "I0919 18:34:32.018158 140474388072320 learning.py:507] global step 5583: loss = 0.9779 (1.296 sec/step)\n",
            "I0919 18:34:33.352926 140474388072320 learning.py:507] global step 5584: loss = 0.8776 (1.333 sec/step)\n",
            "I0919 18:34:34.653183 140474388072320 learning.py:507] global step 5585: loss = 0.7289 (1.298 sec/step)\n",
            "I0919 18:34:35.964561 140474388072320 learning.py:507] global step 5586: loss = 0.8245 (1.305 sec/step)\n",
            "I0919 18:34:37.332551 140474388072320 learning.py:507] global step 5587: loss = 0.7325 (1.366 sec/step)\n",
            "I0919 18:34:38.613472 140474388072320 learning.py:507] global step 5588: loss = 0.8452 (1.279 sec/step)\n",
            "I0919 18:34:39.925683 140474388072320 learning.py:507] global step 5589: loss = 0.6812 (1.310 sec/step)\n",
            "I0919 18:34:41.260009 140474388072320 learning.py:507] global step 5590: loss = 0.7771 (1.332 sec/step)\n",
            "I0919 18:34:42.572827 140474388072320 learning.py:507] global step 5591: loss = 0.6555 (1.311 sec/step)\n",
            "I0919 18:34:43.905946 140474388072320 learning.py:507] global step 5592: loss = 0.8887 (1.332 sec/step)\n",
            "I0919 18:34:45.195476 140474388072320 learning.py:507] global step 5593: loss = 0.8254 (1.288 sec/step)\n",
            "I0919 18:34:46.516397 140474388072320 learning.py:507] global step 5594: loss = 0.7754 (1.319 sec/step)\n",
            "I0919 18:34:47.804506 140474388072320 learning.py:507] global step 5595: loss = 0.7290 (1.286 sec/step)\n",
            "I0919 18:34:49.129795 140474388072320 learning.py:507] global step 5596: loss = 0.8671 (1.323 sec/step)\n",
            "I0919 18:34:50.430340 140474388072320 learning.py:507] global step 5597: loss = 0.8346 (1.299 sec/step)\n",
            "I0919 18:34:51.743916 140474388072320 learning.py:507] global step 5598: loss = 0.8180 (1.312 sec/step)\n",
            "I0919 18:34:53.029263 140474388072320 learning.py:507] global step 5599: loss = 0.8694 (1.284 sec/step)\n",
            "I0919 18:34:54.349138 140474388072320 learning.py:507] global step 5600: loss = 0.8522 (1.318 sec/step)\n",
            "I0919 18:34:55.681524 140474388072320 learning.py:507] global step 5601: loss = 0.9398 (1.331 sec/step)\n",
            "I0919 18:34:57.002727 140474388072320 learning.py:507] global step 5602: loss = 0.6599 (1.319 sec/step)\n",
            "I0919 18:34:58.319878 140474388072320 learning.py:507] global step 5603: loss = 0.7326 (1.315 sec/step)\n",
            "I0919 18:34:59.616432 140474388072320 learning.py:507] global step 5604: loss = 0.7471 (1.295 sec/step)\n",
            "I0919 18:35:00.929181 140474388072320 learning.py:507] global step 5605: loss = 0.9160 (1.311 sec/step)\n",
            "I0919 18:35:02.233092 140474388072320 learning.py:507] global step 5606: loss = 0.7192 (1.302 sec/step)\n",
            "I0919 18:35:03.531538 140474388072320 learning.py:507] global step 5607: loss = 0.7242 (1.297 sec/step)\n",
            "I0919 18:35:04.842461 140474388072320 learning.py:507] global step 5608: loss = 0.7535 (1.309 sec/step)\n",
            "I0919 18:35:06.141727 140474388072320 learning.py:507] global step 5609: loss = 0.7338 (1.297 sec/step)\n",
            "I0919 18:35:07.460321 140474388072320 learning.py:507] global step 5610: loss = 0.8799 (1.317 sec/step)\n",
            "I0919 18:35:08.775883 140474388072320 learning.py:507] global step 5611: loss = 0.7330 (1.314 sec/step)\n",
            "I0919 18:35:10.065927 140474388072320 learning.py:507] global step 5612: loss = 0.8531 (1.288 sec/step)\n",
            "I0919 18:35:11.368809 140474388072320 learning.py:507] global step 5613: loss = 0.7485 (1.301 sec/step)\n",
            "I0919 18:35:12.689984 140474388072320 learning.py:507] global step 5614: loss = 0.7545 (1.319 sec/step)\n",
            "I0919 18:35:14.042975 140474388072320 learning.py:507] global step 5615: loss = 0.7918 (1.351 sec/step)\n",
            "I0919 18:35:15.385144 140474388072320 learning.py:507] global step 5616: loss = 0.6065 (1.340 sec/step)\n",
            "I0919 18:35:16.674032 140474388072320 learning.py:507] global step 5617: loss = 0.8458 (1.287 sec/step)\n",
            "I0919 18:35:17.959912 140474388072320 learning.py:507] global step 5618: loss = 0.7968 (1.284 sec/step)\n",
            "I0919 18:35:19.250689 140474388072320 learning.py:507] global step 5619: loss = 0.8338 (1.289 sec/step)\n",
            "I0919 18:35:20.563616 140474388072320 learning.py:507] global step 5620: loss = 0.8032 (1.311 sec/step)\n",
            "I0919 18:35:21.949619 140474388072320 learning.py:507] global step 5621: loss = 0.8124 (1.384 sec/step)\n",
            "I0919 18:35:23.277848 140474388072320 learning.py:507] global step 5622: loss = 0.7672 (1.326 sec/step)\n",
            "I0919 18:35:24.590830 140474388072320 learning.py:507] global step 5623: loss = 0.9512 (1.311 sec/step)\n",
            "I0919 18:35:25.884943 140474388072320 learning.py:507] global step 5624: loss = 0.6203 (1.292 sec/step)\n",
            "I0919 18:35:27.169981 140474388072320 learning.py:507] global step 5625: loss = 0.6949 (1.283 sec/step)\n",
            "I0919 18:35:28.496505 140474388072320 learning.py:507] global step 5626: loss = 0.7610 (1.325 sec/step)\n",
            "I0919 18:35:29.803066 140474388072320 learning.py:507] global step 5627: loss = 0.7831 (1.305 sec/step)\n",
            "I0919 18:35:31.156404 140474388072320 learning.py:507] global step 5628: loss = 0.6666 (1.352 sec/step)\n",
            "I0919 18:35:32.461492 140474388072320 learning.py:507] global step 5629: loss = 1.0993 (1.304 sec/step)\n",
            "I0919 18:35:33.782041 140474388072320 learning.py:507] global step 5630: loss = 0.9329 (1.318 sec/step)\n",
            "I0919 18:35:35.107297 140474388072320 learning.py:507] global step 5631: loss = 0.7134 (1.323 sec/step)\n",
            "I0919 18:35:36.452766 140474388072320 learning.py:507] global step 5632: loss = 0.7356 (1.344 sec/step)\n",
            "I0919 18:35:37.769196 140474388072320 learning.py:507] global step 5633: loss = 1.0329 (1.315 sec/step)\n",
            "I0919 18:35:39.072971 140474388072320 learning.py:507] global step 5634: loss = 0.8316 (1.302 sec/step)\n",
            "I0919 18:35:40.374737 140474388072320 learning.py:507] global step 5635: loss = 0.7319 (1.300 sec/step)\n",
            "I0919 18:35:41.669073 140474388072320 learning.py:507] global step 5636: loss = 0.7655 (1.292 sec/step)\n",
            "I0919 18:35:42.993426 140474388072320 learning.py:507] global step 5637: loss = 1.0040 (1.323 sec/step)\n",
            "I0919 18:35:44.313486 140474388072320 learning.py:507] global step 5638: loss = 0.6400 (1.318 sec/step)\n",
            "I0919 18:35:45.622770 140474388072320 learning.py:507] global step 5639: loss = 0.8176 (1.307 sec/step)\n",
            "I0919 18:35:46.976237 140474388072320 learning.py:507] global step 5640: loss = 0.6368 (1.352 sec/step)\n",
            "I0919 18:35:48.280992 140474388072320 learning.py:507] global step 5641: loss = 0.6545 (1.303 sec/step)\n",
            "I0919 18:35:49.558499 140474388072320 learning.py:507] global step 5642: loss = 0.6558 (1.276 sec/step)\n",
            "I0919 18:35:50.878460 140474388072320 learning.py:507] global step 5643: loss = 0.7884 (1.318 sec/step)\n",
            "I0919 18:35:52.197065 140474388072320 learning.py:507] global step 5644: loss = 0.9929 (1.317 sec/step)\n",
            "I0919 18:35:53.560854 140474388072320 learning.py:507] global step 5645: loss = 0.7728 (1.362 sec/step)\n",
            "I0919 18:35:54.867253 140474388072320 learning.py:507] global step 5646: loss = 0.7015 (1.305 sec/step)\n",
            "I0919 18:35:56.200323 140474388072320 learning.py:507] global step 5647: loss = 0.8661 (1.331 sec/step)\n",
            "I0919 18:35:57.520679 140474388072320 learning.py:507] global step 5648: loss = 0.7882 (1.318 sec/step)\n",
            "I0919 18:35:58.847999 140474388072320 learning.py:507] global step 5649: loss = 0.7691 (1.325 sec/step)\n",
            "I0919 18:36:00.212652 140474388072320 learning.py:507] global step 5650: loss = 0.9081 (1.363 sec/step)\n",
            "I0919 18:36:01.513768 140474388072320 learning.py:507] global step 5651: loss = 0.8309 (1.299 sec/step)\n",
            "I0919 18:36:02.834651 140474388072320 learning.py:507] global step 5652: loss = 0.6981 (1.319 sec/step)\n",
            "I0919 18:36:04.140249 140474388072320 learning.py:507] global step 5653: loss = 0.7476 (1.304 sec/step)\n",
            "I0919 18:36:05.451904 140474388072320 learning.py:507] global step 5654: loss = 0.8919 (1.310 sec/step)\n",
            "I0919 18:36:06.759496 140474388072320 learning.py:507] global step 5655: loss = 1.1127 (1.306 sec/step)\n",
            "I0919 18:36:08.055480 140474388072320 learning.py:507] global step 5656: loss = 0.8153 (1.294 sec/step)\n",
            "I0919 18:36:09.385591 140474388072320 learning.py:507] global step 5657: loss = 0.7738 (1.328 sec/step)\n",
            "I0919 18:36:10.716032 140474388072320 learning.py:507] global step 5658: loss = 1.0316 (1.328 sec/step)\n",
            "I0919 18:36:12.083410 140474388072320 learning.py:507] global step 5659: loss = 0.9456 (1.366 sec/step)\n",
            "I0919 18:36:13.414836 140474388072320 learning.py:507] global step 5660: loss = 0.5649 (1.330 sec/step)\n",
            "I0919 18:36:14.783760 140474388072320 learning.py:507] global step 5661: loss = 0.7801 (1.367 sec/step)\n",
            "I0919 18:36:16.116571 140474388072320 learning.py:507] global step 5662: loss = 0.7321 (1.329 sec/step)\n",
            "I0919 18:36:17.444375 140474388072320 learning.py:507] global step 5663: loss = 0.6098 (1.326 sec/step)\n",
            "I0919 18:36:18.771179 140474388072320 learning.py:507] global step 5664: loss = 0.9846 (1.325 sec/step)\n",
            "I0919 18:36:20.099256 140474388072320 learning.py:507] global step 5665: loss = 0.7459 (1.326 sec/step)\n",
            "I0919 18:36:21.412996 140474388072320 learning.py:507] global step 5666: loss = 0.7682 (1.312 sec/step)\n",
            "I0919 18:36:22.711844 140474388072320 learning.py:507] global step 5667: loss = 1.0686 (1.297 sec/step)\n",
            "I0919 18:36:24.028299 140474388072320 learning.py:507] global step 5668: loss = 1.2860 (1.315 sec/step)\n",
            "I0919 18:36:25.373264 140474388072320 learning.py:507] global step 5669: loss = 0.8751 (1.343 sec/step)\n",
            "I0919 18:36:26.691931 140474388072320 learning.py:507] global step 5670: loss = 0.8918 (1.317 sec/step)\n",
            "I0919 18:36:28.776613 140474388072320 learning.py:507] global step 5671: loss = 0.9728 (2.083 sec/step)\n",
            "I0919 18:36:28.909714 140471297337088 supervisor.py:1050] Recording summary at step 5671.\n",
            "I0919 18:36:30.127613 140474388072320 learning.py:507] global step 5672: loss = 0.8707 (1.349 sec/step)\n",
            "I0919 18:36:31.455300 140474388072320 learning.py:507] global step 5673: loss = 0.6994 (1.326 sec/step)\n",
            "I0919 18:36:32.752235 140474388072320 learning.py:507] global step 5674: loss = 1.0614 (1.295 sec/step)\n",
            "I0919 18:36:34.059415 140474388072320 learning.py:507] global step 5675: loss = 0.7594 (1.305 sec/step)\n",
            "I0919 18:36:35.368302 140474388072320 learning.py:507] global step 5676: loss = 0.7077 (1.307 sec/step)\n",
            "I0919 18:36:36.677146 140474388072320 learning.py:507] global step 5677: loss = 0.6740 (1.307 sec/step)\n",
            "I0919 18:36:37.998423 140474388072320 learning.py:507] global step 5678: loss = 0.8680 (1.320 sec/step)\n",
            "I0919 18:36:39.312027 140474388072320 learning.py:507] global step 5679: loss = 0.6898 (1.312 sec/step)\n",
            "I0919 18:36:40.620460 140474388072320 learning.py:507] global step 5680: loss = 1.0763 (1.307 sec/step)\n",
            "I0919 18:36:41.976807 140474388072320 learning.py:507] global step 5681: loss = 1.0548 (1.354 sec/step)\n",
            "I0919 18:36:43.283607 140474388072320 learning.py:507] global step 5682: loss = 0.7700 (1.305 sec/step)\n",
            "I0919 18:36:44.592735 140474388072320 learning.py:507] global step 5683: loss = 0.8843 (1.307 sec/step)\n",
            "I0919 18:36:45.931062 140474388072320 learning.py:507] global step 5684: loss = 0.6164 (1.337 sec/step)\n",
            "I0919 18:36:47.234357 140474388072320 learning.py:507] global step 5685: loss = 0.7359 (1.301 sec/step)\n",
            "I0919 18:36:48.585993 140474388072320 learning.py:507] global step 5686: loss = 0.7948 (1.350 sec/step)\n",
            "I0919 18:36:49.925915 140474388072320 learning.py:507] global step 5687: loss = 0.9506 (1.338 sec/step)\n",
            "I0919 18:36:51.228274 140474388072320 learning.py:507] global step 5688: loss = 0.9775 (1.301 sec/step)\n",
            "I0919 18:36:52.550240 140474388072320 learning.py:507] global step 5689: loss = 0.8355 (1.320 sec/step)\n",
            "I0919 18:36:53.846062 140474388072320 learning.py:507] global step 5690: loss = 0.7656 (1.294 sec/step)\n",
            "I0919 18:36:55.181948 140474388072320 learning.py:507] global step 5691: loss = 0.9449 (1.334 sec/step)\n",
            "I0919 18:36:56.452241 140474388072320 learning.py:507] global step 5692: loss = 1.0833 (1.268 sec/step)\n",
            "I0919 18:36:57.822304 140474388072320 learning.py:507] global step 5693: loss = 0.8841 (1.368 sec/step)\n",
            "I0919 18:36:59.167094 140474388072320 learning.py:507] global step 5694: loss = 0.6846 (1.343 sec/step)\n",
            "I0919 18:37:00.475019 140474388072320 learning.py:507] global step 5695: loss = 0.7892 (1.306 sec/step)\n",
            "I0919 18:37:01.779476 140474388072320 learning.py:507] global step 5696: loss = 0.8365 (1.303 sec/step)\n",
            "I0919 18:37:03.075930 140474388072320 learning.py:507] global step 5697: loss = 0.9492 (1.295 sec/step)\n",
            "I0919 18:37:04.426250 140474388072320 learning.py:507] global step 5698: loss = 0.7491 (1.349 sec/step)\n",
            "I0919 18:37:05.726528 140474388072320 learning.py:507] global step 5699: loss = 0.6082 (1.299 sec/step)\n",
            "I0919 18:37:07.028722 140474388072320 learning.py:507] global step 5700: loss = 0.8450 (1.300 sec/step)\n",
            "I0919 18:37:08.326455 140474388072320 learning.py:507] global step 5701: loss = 1.0510 (1.296 sec/step)\n",
            "I0919 18:37:09.636542 140474388072320 learning.py:507] global step 5702: loss = 1.3755 (1.308 sec/step)\n",
            "I0919 18:37:10.976921 140474388072320 learning.py:507] global step 5703: loss = 0.6933 (1.338 sec/step)\n",
            "I0919 18:37:12.280313 140474388072320 learning.py:507] global step 5704: loss = 0.9277 (1.302 sec/step)\n",
            "I0919 18:37:13.615909 140474388072320 learning.py:507] global step 5705: loss = 0.7295 (1.334 sec/step)\n",
            "I0919 18:37:14.927694 140474388072320 learning.py:507] global step 5706: loss = 0.8844 (1.310 sec/step)\n",
            "I0919 18:37:16.234379 140474388072320 learning.py:507] global step 5707: loss = 0.6074 (1.305 sec/step)\n",
            "I0919 18:37:17.525580 140474388072320 learning.py:507] global step 5708: loss = 0.8906 (1.290 sec/step)\n",
            "I0919 18:37:18.863919 140474388072320 learning.py:507] global step 5709: loss = 0.7508 (1.336 sec/step)\n",
            "I0919 18:37:20.169359 140474388072320 learning.py:507] global step 5710: loss = 0.7731 (1.304 sec/step)\n",
            "I0919 18:37:21.497384 140474388072320 learning.py:507] global step 5711: loss = 0.7876 (1.326 sec/step)\n",
            "I0919 18:37:22.836097 140474388072320 learning.py:507] global step 5712: loss = 0.8448 (1.337 sec/step)\n",
            "I0919 18:37:24.125796 140474388072320 learning.py:507] global step 5713: loss = 0.9985 (1.288 sec/step)\n",
            "I0919 18:37:25.488327 140474388072320 learning.py:507] global step 5714: loss = 0.7787 (1.360 sec/step)\n",
            "I0919 18:37:26.811027 140474388072320 learning.py:507] global step 5715: loss = 0.7288 (1.321 sec/step)\n",
            "I0919 18:37:28.103434 140474388072320 learning.py:507] global step 5716: loss = 0.8091 (1.290 sec/step)\n",
            "I0919 18:37:29.416237 140474388072320 learning.py:507] global step 5717: loss = 0.9327 (1.311 sec/step)\n",
            "I0919 18:37:30.783457 140474388072320 learning.py:507] global step 5718: loss = 0.8019 (1.365 sec/step)\n",
            "I0919 18:37:32.099308 140474388072320 learning.py:507] global step 5719: loss = 0.9367 (1.314 sec/step)\n",
            "I0919 18:37:33.423598 140474388072320 learning.py:507] global step 5720: loss = 0.8936 (1.323 sec/step)\n",
            "I0919 18:37:34.722167 140474388072320 learning.py:507] global step 5721: loss = 0.6711 (1.297 sec/step)\n",
            "I0919 18:37:36.037936 140474388072320 learning.py:507] global step 5722: loss = 0.7888 (1.314 sec/step)\n",
            "I0919 18:37:37.402923 140474388072320 learning.py:507] global step 5723: loss = 0.7723 (1.363 sec/step)\n",
            "I0919 18:37:38.721496 140474388072320 learning.py:507] global step 5724: loss = 0.6688 (1.316 sec/step)\n",
            "I0919 18:37:40.079021 140474388072320 learning.py:507] global step 5725: loss = 0.7561 (1.356 sec/step)\n",
            "I0919 18:37:41.384371 140474388072320 learning.py:507] global step 5726: loss = 0.8437 (1.304 sec/step)\n",
            "I0919 18:37:42.698343 140474388072320 learning.py:507] global step 5727: loss = 0.9296 (1.312 sec/step)\n",
            "I0919 18:37:43.974904 140474388072320 learning.py:507] global step 5728: loss = 1.3301 (1.275 sec/step)\n",
            "I0919 18:37:45.300905 140474388072320 learning.py:507] global step 5729: loss = 0.9578 (1.324 sec/step)\n",
            "I0919 18:37:46.630527 140474388072320 learning.py:507] global step 5730: loss = 0.6999 (1.327 sec/step)\n",
            "I0919 18:37:47.918683 140474388072320 learning.py:507] global step 5731: loss = 0.8587 (1.286 sec/step)\n",
            "I0919 18:37:49.226325 140474388072320 learning.py:507] global step 5732: loss = 0.7951 (1.306 sec/step)\n",
            "I0919 18:37:50.517564 140474388072320 learning.py:507] global step 5733: loss = 0.9298 (1.290 sec/step)\n",
            "I0919 18:37:51.832746 140474388072320 learning.py:507] global step 5734: loss = 0.7246 (1.313 sec/step)\n",
            "I0919 18:37:53.156594 140474388072320 learning.py:507] global step 5735: loss = 0.8827 (1.322 sec/step)\n",
            "I0919 18:37:54.506513 140474388072320 learning.py:507] global step 5736: loss = 0.8582 (1.348 sec/step)\n",
            "I0919 18:37:55.829452 140474388072320 learning.py:507] global step 5737: loss = 0.8813 (1.321 sec/step)\n",
            "I0919 18:37:57.138386 140474388072320 learning.py:507] global step 5738: loss = 0.6690 (1.307 sec/step)\n",
            "I0919 18:37:58.466294 140474388072320 learning.py:507] global step 5739: loss = 0.8622 (1.326 sec/step)\n",
            "I0919 18:37:59.832051 140474388072320 learning.py:507] global step 5740: loss = 0.9968 (1.364 sec/step)\n",
            "I0919 18:38:01.162370 140474388072320 learning.py:507] global step 5741: loss = 0.7793 (1.329 sec/step)\n",
            "I0919 18:38:02.442414 140474388072320 learning.py:507] global step 5742: loss = 0.7542 (1.278 sec/step)\n",
            "I0919 18:38:03.767982 140474388072320 learning.py:507] global step 5743: loss = 0.6703 (1.324 sec/step)\n",
            "I0919 18:38:05.058944 140474388072320 learning.py:507] global step 5744: loss = 0.8577 (1.289 sec/step)\n",
            "I0919 18:38:06.419507 140474388072320 learning.py:507] global step 5745: loss = 0.8853 (1.359 sec/step)\n",
            "I0919 18:38:07.736049 140474388072320 learning.py:507] global step 5746: loss = 0.8867 (1.315 sec/step)\n",
            "I0919 18:38:09.053283 140474388072320 learning.py:507] global step 5747: loss = 0.7627 (1.316 sec/step)\n",
            "I0919 18:38:10.365974 140474388072320 learning.py:507] global step 5748: loss = 0.6967 (1.311 sec/step)\n",
            "I0919 18:38:11.670797 140474388072320 learning.py:507] global step 5749: loss = 0.7891 (1.303 sec/step)\n",
            "I0919 18:38:12.982544 140474388072320 learning.py:507] global step 5750: loss = 0.7788 (1.310 sec/step)\n",
            "I0919 18:38:14.279666 140474388072320 learning.py:507] global step 5751: loss = 0.8108 (1.295 sec/step)\n",
            "I0919 18:38:15.579564 140474388072320 learning.py:507] global step 5752: loss = 0.6700 (1.298 sec/step)\n",
            "I0919 18:38:16.888643 140474388072320 learning.py:507] global step 5753: loss = 0.6039 (1.307 sec/step)\n",
            "I0919 18:38:18.176676 140474388072320 learning.py:507] global step 5754: loss = 0.9416 (1.286 sec/step)\n",
            "I0919 18:38:19.471858 140474388072320 learning.py:507] global step 5755: loss = 0.6613 (1.294 sec/step)\n",
            "I0919 18:38:20.755423 140474388072320 learning.py:507] global step 5756: loss = 1.0062 (1.282 sec/step)\n",
            "I0919 18:38:22.065247 140474388072320 learning.py:507] global step 5757: loss = 0.6752 (1.308 sec/step)\n",
            "I0919 18:38:23.380842 140474388072320 learning.py:507] global step 5758: loss = 0.9859 (1.314 sec/step)\n",
            "I0919 18:38:24.694089 140474388072320 learning.py:507] global step 5759: loss = 0.6457 (1.311 sec/step)\n",
            "I0919 18:38:26.024360 140474388072320 learning.py:507] global step 5760: loss = 0.6634 (1.329 sec/step)\n",
            "I0919 18:38:27.331331 140474388072320 learning.py:507] global step 5761: loss = 0.7315 (1.305 sec/step)\n",
            "I0919 18:38:29.024652 140471297337088 supervisor.py:1050] Recording summary at step 5761.\n",
            "I0919 18:38:29.550707 140474388072320 learning.py:507] global step 5762: loss = 0.5955 (2.025 sec/step)\n",
            "I0919 18:38:30.854412 140474388072320 learning.py:507] global step 5763: loss = 0.7515 (1.302 sec/step)\n",
            "I0919 18:38:32.165339 140474388072320 learning.py:507] global step 5764: loss = 0.7430 (1.309 sec/step)\n",
            "I0919 18:38:33.471323 140474388072320 learning.py:507] global step 5765: loss = 0.6538 (1.304 sec/step)\n",
            "I0919 18:38:34.751925 140474388072320 learning.py:507] global step 5766: loss = 0.7234 (1.279 sec/step)\n",
            "I0919 18:38:36.063627 140474388072320 learning.py:507] global step 5767: loss = 0.6706 (1.310 sec/step)\n",
            "I0919 18:38:37.412043 140474388072320 learning.py:507] global step 5768: loss = 0.6270 (1.347 sec/step)\n",
            "I0919 18:38:38.773871 140474388072320 learning.py:507] global step 5769: loss = 0.8272 (1.360 sec/step)\n",
            "I0919 18:38:40.065378 140474388072320 learning.py:507] global step 5770: loss = 0.6764 (1.290 sec/step)\n",
            "I0919 18:38:41.400784 140474388072320 learning.py:507] global step 5771: loss = 0.7054 (1.334 sec/step)\n",
            "I0919 18:38:42.721292 140474388072320 learning.py:507] global step 5772: loss = 0.9476 (1.318 sec/step)\n",
            "I0919 18:38:44.064601 140474388072320 learning.py:507] global step 5773: loss = 0.7339 (1.341 sec/step)\n",
            "I0919 18:38:45.382603 140474388072320 learning.py:507] global step 5774: loss = 0.7701 (1.316 sec/step)\n",
            "I0919 18:38:46.693948 140474388072320 learning.py:507] global step 5775: loss = 1.0874 (1.309 sec/step)\n",
            "I0919 18:38:48.005919 140474388072320 learning.py:507] global step 5776: loss = 0.7593 (1.310 sec/step)\n",
            "I0919 18:38:49.304347 140474388072320 learning.py:507] global step 5777: loss = 0.6404 (1.297 sec/step)\n",
            "I0919 18:38:50.615530 140474388072320 learning.py:507] global step 5778: loss = 0.9764 (1.310 sec/step)\n",
            "I0919 18:38:51.908058 140474388072320 learning.py:507] global step 5779: loss = 0.7459 (1.291 sec/step)\n",
            "I0919 18:38:53.217749 140474388072320 learning.py:507] global step 5780: loss = 0.8526 (1.308 sec/step)\n",
            "I0919 18:38:54.551095 140474388072320 learning.py:507] global step 5781: loss = 0.7024 (1.331 sec/step)\n",
            "I0919 18:38:55.878779 140474388072320 learning.py:507] global step 5782: loss = 0.7326 (1.326 sec/step)\n",
            "I0919 18:38:57.214093 140474388072320 learning.py:507] global step 5783: loss = 0.9361 (1.334 sec/step)\n",
            "I0919 18:38:58.534984 140474388072320 learning.py:507] global step 5784: loss = 0.7081 (1.319 sec/step)\n",
            "I0919 18:38:59.912163 140474388072320 learning.py:507] global step 5785: loss = 0.7590 (1.375 sec/step)\n",
            "I0919 18:39:01.232592 140474388072320 learning.py:507] global step 5786: loss = 0.7397 (1.319 sec/step)\n",
            "I0919 18:39:02.528773 140474388072320 learning.py:507] global step 5787: loss = 0.7573 (1.294 sec/step)\n",
            "I0919 18:39:03.852031 140474388072320 learning.py:507] global step 5788: loss = 1.0964 (1.321 sec/step)\n",
            "I0919 18:39:05.136436 140474388072320 learning.py:507] global step 5789: loss = 1.1314 (1.283 sec/step)\n",
            "I0919 18:39:06.447083 140474388072320 learning.py:507] global step 5790: loss = 0.9165 (1.309 sec/step)\n",
            "I0919 18:39:07.761276 140474388072320 learning.py:507] global step 5791: loss = 0.5925 (1.312 sec/step)\n",
            "I0919 18:39:09.067955 140474388072320 learning.py:507] global step 5792: loss = 0.8049 (1.305 sec/step)\n",
            "I0919 18:39:10.411400 140474388072320 learning.py:507] global step 5793: loss = 0.7577 (1.341 sec/step)\n",
            "I0919 18:39:11.761831 140474388072320 learning.py:507] global step 5794: loss = 0.9816 (1.349 sec/step)\n",
            "I0919 18:39:13.115514 140474388072320 learning.py:507] global step 5795: loss = 0.6500 (1.352 sec/step)\n",
            "I0919 18:39:14.422574 140474388072320 learning.py:507] global step 5796: loss = 0.6651 (1.305 sec/step)\n",
            "I0919 18:39:15.736495 140474388072320 learning.py:507] global step 5797: loss = 0.7321 (1.312 sec/step)\n",
            "I0919 18:39:17.045452 140474388072320 learning.py:507] global step 5798: loss = 0.6501 (1.307 sec/step)\n",
            "I0919 18:39:18.337320 140474388072320 learning.py:507] global step 5799: loss = 0.6135 (1.290 sec/step)\n",
            "I0919 18:39:19.679136 140474388072320 learning.py:507] global step 5800: loss = 1.5254 (1.340 sec/step)\n",
            "I0919 18:39:20.987055 140474388072320 learning.py:507] global step 5801: loss = 0.8078 (1.306 sec/step)\n",
            "I0919 18:39:22.289249 140474388072320 learning.py:507] global step 5802: loss = 0.7601 (1.301 sec/step)\n",
            "I0919 18:39:23.614937 140474388072320 learning.py:507] global step 5803: loss = 0.6704 (1.324 sec/step)\n",
            "I0919 18:39:24.921561 140474388072320 learning.py:507] global step 5804: loss = 0.9304 (1.305 sec/step)\n",
            "I0919 18:39:26.227978 140474388072320 learning.py:507] global step 5805: loss = 0.6838 (1.305 sec/step)\n",
            "I0919 18:39:27.524003 140474388072320 learning.py:507] global step 5806: loss = 1.1370 (1.294 sec/step)\n",
            "I0919 18:39:28.799921 140474388072320 learning.py:507] global step 5807: loss = 0.9112 (1.274 sec/step)\n",
            "I0919 18:39:30.087908 140474388072320 learning.py:507] global step 5808: loss = 0.6938 (1.286 sec/step)\n",
            "I0919 18:39:31.429625 140474388072320 learning.py:507] global step 5809: loss = 0.7981 (1.340 sec/step)\n",
            "I0919 18:39:32.743921 140474388072320 learning.py:507] global step 5810: loss = 0.7935 (1.313 sec/step)\n",
            "I0919 18:39:34.099225 140474388072320 learning.py:507] global step 5811: loss = 0.6744 (1.353 sec/step)\n",
            "I0919 18:39:35.380690 140474388072320 learning.py:507] global step 5812: loss = 0.6467 (1.280 sec/step)\n",
            "I0919 18:39:36.686908 140474388072320 learning.py:507] global step 5813: loss = 0.7889 (1.305 sec/step)\n",
            "I0919 18:39:37.988533 140474388072320 learning.py:507] global step 5814: loss = 0.9008 (1.299 sec/step)\n",
            "I0919 18:39:39.284276 140474388072320 learning.py:507] global step 5815: loss = 1.0984 (1.294 sec/step)\n",
            "I0919 18:39:40.583245 140474388072320 learning.py:507] global step 5816: loss = 0.7433 (1.297 sec/step)\n",
            "I0919 18:39:41.910254 140474388072320 learning.py:507] global step 5817: loss = 0.9157 (1.325 sec/step)\n",
            "I0919 18:39:43.223352 140474388072320 learning.py:507] global step 5818: loss = 0.6975 (1.311 sec/step)\n",
            "I0919 18:39:44.520653 140474388072320 learning.py:507] global step 5819: loss = 0.7754 (1.295 sec/step)\n",
            "I0919 18:39:45.838231 140474388072320 learning.py:507] global step 5820: loss = 0.8195 (1.316 sec/step)\n",
            "I0919 18:39:47.182358 140474388072320 learning.py:507] global step 5821: loss = 0.8148 (1.342 sec/step)\n",
            "I0919 18:39:48.478984 140474388072320 learning.py:507] global step 5822: loss = 1.0064 (1.295 sec/step)\n",
            "I0919 18:39:49.815026 140474388072320 learning.py:507] global step 5823: loss = 0.7521 (1.334 sec/step)\n",
            "I0919 18:39:51.116800 140474388072320 learning.py:507] global step 5824: loss = 0.7373 (1.300 sec/step)\n",
            "I0919 18:39:52.448894 140474388072320 learning.py:507] global step 5825: loss = 0.8631 (1.330 sec/step)\n",
            "I0919 18:39:53.756072 140474388072320 learning.py:507] global step 5826: loss = 0.9085 (1.305 sec/step)\n",
            "I0919 18:39:55.046475 140474388072320 learning.py:507] global step 5827: loss = 0.9095 (1.289 sec/step)\n",
            "I0919 18:39:56.387346 140474388072320 learning.py:507] global step 5828: loss = 0.6922 (1.339 sec/step)\n",
            "I0919 18:39:57.673918 140474388072320 learning.py:507] global step 5829: loss = 0.7532 (1.285 sec/step)\n",
            "I0919 18:39:58.975713 140474388072320 learning.py:507] global step 5830: loss = 0.8228 (1.300 sec/step)\n",
            "I0919 18:40:00.276532 140474388072320 learning.py:507] global step 5831: loss = 0.8283 (1.299 sec/step)\n",
            "I0919 18:40:01.605465 140474388072320 learning.py:507] global step 5832: loss = 0.9034 (1.327 sec/step)\n",
            "I0919 18:40:02.931870 140474388072320 learning.py:507] global step 5833: loss = 0.6039 (1.324 sec/step)\n",
            "I0919 18:40:04.237234 140474388072320 learning.py:507] global step 5834: loss = 0.7729 (1.304 sec/step)\n",
            "I0919 18:40:05.527196 140474388072320 learning.py:507] global step 5835: loss = 0.9090 (1.288 sec/step)\n",
            "I0919 18:40:06.849638 140474388072320 learning.py:507] global step 5836: loss = 0.6727 (1.320 sec/step)\n",
            "I0919 18:40:08.172940 140474388072320 learning.py:507] global step 5837: loss = 0.7174 (1.322 sec/step)\n",
            "I0919 18:40:09.495657 140474388072320 learning.py:507] global step 5838: loss = 0.7851 (1.321 sec/step)\n",
            "I0919 18:40:10.784650 140474388072320 learning.py:507] global step 5839: loss = 0.7153 (1.287 sec/step)\n",
            "I0919 18:40:12.073952 140474388072320 learning.py:507] global step 5840: loss = 0.6240 (1.288 sec/step)\n",
            "I0919 18:40:13.391882 140474388072320 learning.py:507] global step 5841: loss = 0.9059 (1.316 sec/step)\n",
            "I0919 18:40:14.713687 140474388072320 learning.py:507] global step 5842: loss = 0.7575 (1.320 sec/step)\n",
            "I0919 18:40:16.011495 140474388072320 learning.py:507] global step 5843: loss = 0.7875 (1.296 sec/step)\n",
            "I0919 18:40:17.339467 140474388072320 learning.py:507] global step 5844: loss = 0.8776 (1.326 sec/step)\n",
            "I0919 18:40:18.645807 140474388072320 learning.py:507] global step 5845: loss = 0.7546 (1.305 sec/step)\n",
            "I0919 18:40:19.956190 140474388072320 learning.py:507] global step 5846: loss = 0.6214 (1.308 sec/step)\n",
            "I0919 18:40:21.253916 140474388072320 learning.py:507] global step 5847: loss = 0.7796 (1.296 sec/step)\n",
            "I0919 18:40:22.563031 140474388072320 learning.py:507] global step 5848: loss = 0.7196 (1.307 sec/step)\n",
            "I0919 18:40:23.888390 140474388072320 learning.py:507] global step 5849: loss = 1.0661 (1.323 sec/step)\n",
            "I0919 18:40:25.223253 140474388072320 learning.py:507] global step 5850: loss = 1.3153 (1.333 sec/step)\n",
            "I0919 18:40:26.502569 140474388072320 learning.py:507] global step 5851: loss = 0.8732 (1.277 sec/step)\n",
            "I0919 18:40:26.872494 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 18:40:28.097419 140474388072320 learning.py:507] global step 5852: loss = 1.1371 (1.432 sec/step)\n",
            "I0919 18:40:29.431163 140471297337088 supervisor.py:1050] Recording summary at step 5852.\n",
            "I0919 18:40:30.888050 140474388072320 learning.py:507] global step 5853: loss = 0.9136 (2.303 sec/step)\n",
            "I0919 18:40:32.824035 140474388072320 learning.py:507] global step 5854: loss = 1.0615 (1.912 sec/step)\n",
            "I0919 18:40:34.123846 140474388072320 learning.py:507] global step 5855: loss = 0.6931 (1.298 sec/step)\n",
            "I0919 18:40:35.431030 140474388072320 learning.py:507] global step 5856: loss = 0.6638 (1.305 sec/step)\n",
            "I0919 18:40:36.732194 140474388072320 learning.py:507] global step 5857: loss = 0.7762 (1.299 sec/step)\n",
            "I0919 18:40:38.057599 140474388072320 learning.py:507] global step 5858: loss = 0.8171 (1.324 sec/step)\n",
            "I0919 18:40:39.354329 140474388072320 learning.py:507] global step 5859: loss = 0.9327 (1.295 sec/step)\n",
            "I0919 18:40:40.695806 140474388072320 learning.py:507] global step 5860: loss = 0.6568 (1.340 sec/step)\n",
            "I0919 18:40:42.017360 140474388072320 learning.py:507] global step 5861: loss = 0.6260 (1.320 sec/step)\n",
            "I0919 18:40:43.348556 140474388072320 learning.py:507] global step 5862: loss = 1.0152 (1.329 sec/step)\n",
            "I0919 18:40:44.662292 140474388072320 learning.py:507] global step 5863: loss = 1.0568 (1.312 sec/step)\n",
            "I0919 18:40:45.952491 140474388072320 learning.py:507] global step 5864: loss = 0.7858 (1.288 sec/step)\n",
            "I0919 18:40:47.272774 140474388072320 learning.py:507] global step 5865: loss = 0.8119 (1.318 sec/step)\n",
            "I0919 18:40:48.604907 140474388072320 learning.py:507] global step 5866: loss = 1.5382 (1.330 sec/step)\n",
            "I0919 18:40:49.889537 140474388072320 learning.py:507] global step 5867: loss = 0.7195 (1.283 sec/step)\n",
            "I0919 18:40:51.175334 140474388072320 learning.py:507] global step 5868: loss = 0.7735 (1.284 sec/step)\n",
            "I0919 18:40:52.459225 140474388072320 learning.py:507] global step 5869: loss = 0.8678 (1.282 sec/step)\n",
            "I0919 18:40:53.752002 140474388072320 learning.py:507] global step 5870: loss = 0.9964 (1.291 sec/step)\n",
            "I0919 18:40:55.102452 140474388072320 learning.py:507] global step 5871: loss = 0.9273 (1.349 sec/step)\n",
            "I0919 18:40:56.421011 140474388072320 learning.py:507] global step 5872: loss = 0.8873 (1.317 sec/step)\n",
            "I0919 18:40:57.727912 140474388072320 learning.py:507] global step 5873: loss = 0.8478 (1.305 sec/step)\n",
            "I0919 18:40:59.033215 140474388072320 learning.py:507] global step 5874: loss = 0.8620 (1.304 sec/step)\n",
            "I0919 18:41:00.342053 140474388072320 learning.py:507] global step 5875: loss = 1.0437 (1.307 sec/step)\n",
            "I0919 18:41:01.653702 140474388072320 learning.py:507] global step 5876: loss = 0.9521 (1.310 sec/step)\n",
            "I0919 18:41:02.932586 140474388072320 learning.py:507] global step 5877: loss = 0.9000 (1.277 sec/step)\n",
            "I0919 18:41:04.278433 140474388072320 learning.py:507] global step 5878: loss = 0.9190 (1.344 sec/step)\n",
            "I0919 18:41:05.579073 140474388072320 learning.py:507] global step 5879: loss = 0.9161 (1.299 sec/step)\n",
            "I0919 18:41:06.887964 140474388072320 learning.py:507] global step 5880: loss = 1.2329 (1.307 sec/step)\n",
            "I0919 18:41:08.203428 140474388072320 learning.py:507] global step 5881: loss = 0.8286 (1.314 sec/step)\n",
            "I0919 18:41:09.546005 140474388072320 learning.py:507] global step 5882: loss = 0.8580 (1.341 sec/step)\n",
            "I0919 18:41:10.885194 140474388072320 learning.py:507] global step 5883: loss = 1.0436 (1.338 sec/step)\n",
            "I0919 18:41:12.177673 140474388072320 learning.py:507] global step 5884: loss = 0.6574 (1.290 sec/step)\n",
            "I0919 18:41:13.501043 140474388072320 learning.py:507] global step 5885: loss = 0.7440 (1.322 sec/step)\n",
            "I0919 18:41:14.792591 140474388072320 learning.py:507] global step 5886: loss = 0.8156 (1.290 sec/step)\n",
            "I0919 18:41:16.093089 140474388072320 learning.py:507] global step 5887: loss = 0.8027 (1.299 sec/step)\n",
            "I0919 18:41:17.387557 140474388072320 learning.py:507] global step 5888: loss = 0.8422 (1.293 sec/step)\n",
            "I0919 18:41:18.716300 140474388072320 learning.py:507] global step 5889: loss = 0.8556 (1.327 sec/step)\n",
            "I0919 18:41:19.998342 140474388072320 learning.py:507] global step 5890: loss = 0.7903 (1.280 sec/step)\n",
            "I0919 18:41:21.331724 140474388072320 learning.py:507] global step 5891: loss = 0.8646 (1.331 sec/step)\n",
            "I0919 18:41:22.713622 140474388072320 learning.py:507] global step 5892: loss = 0.7012 (1.380 sec/step)\n",
            "I0919 18:41:24.011899 140474388072320 learning.py:507] global step 5893: loss = 0.9742 (1.297 sec/step)\n",
            "I0919 18:41:25.326757 140474388072320 learning.py:507] global step 5894: loss = 1.0759 (1.313 sec/step)\n",
            "I0919 18:41:26.624542 140474388072320 learning.py:507] global step 5895: loss = 0.8292 (1.296 sec/step)\n",
            "I0919 18:41:27.952827 140474388072320 learning.py:507] global step 5896: loss = 0.6525 (1.327 sec/step)\n",
            "I0919 18:41:29.297516 140474388072320 learning.py:507] global step 5897: loss = 0.8647 (1.343 sec/step)\n",
            "I0919 18:41:30.600302 140474388072320 learning.py:507] global step 5898: loss = 1.0491 (1.301 sec/step)\n",
            "I0919 18:41:31.906147 140474388072320 learning.py:507] global step 5899: loss = 0.7880 (1.304 sec/step)\n",
            "I0919 18:41:33.194567 140474388072320 learning.py:507] global step 5900: loss = 0.5987 (1.286 sec/step)\n",
            "I0919 18:41:34.498442 140474388072320 learning.py:507] global step 5901: loss = 1.0612 (1.302 sec/step)\n",
            "I0919 18:41:35.825036 140474388072320 learning.py:507] global step 5902: loss = 0.8137 (1.325 sec/step)\n",
            "I0919 18:41:37.141160 140474388072320 learning.py:507] global step 5903: loss = 0.7644 (1.314 sec/step)\n",
            "I0919 18:41:38.437479 140474388072320 learning.py:507] global step 5904: loss = 1.0075 (1.295 sec/step)\n",
            "I0919 18:41:39.745691 140474388072320 learning.py:507] global step 5905: loss = 0.6813 (1.306 sec/step)\n",
            "I0919 18:41:41.036154 140474388072320 learning.py:507] global step 5906: loss = 0.9899 (1.289 sec/step)\n",
            "I0919 18:41:42.340919 140474388072320 learning.py:507] global step 5907: loss = 1.2688 (1.303 sec/step)\n",
            "I0919 18:41:43.663866 140474388072320 learning.py:507] global step 5908: loss = 0.8441 (1.321 sec/step)\n",
            "I0919 18:41:44.992965 140474388072320 learning.py:507] global step 5909: loss = 0.9928 (1.327 sec/step)\n",
            "I0919 18:41:46.281565 140474388072320 learning.py:507] global step 5910: loss = 0.8114 (1.287 sec/step)\n",
            "I0919 18:41:47.584300 140474388072320 learning.py:507] global step 5911: loss = 0.9808 (1.301 sec/step)\n",
            "I0919 18:41:48.874385 140474388072320 learning.py:507] global step 5912: loss = 0.8213 (1.288 sec/step)\n",
            "I0919 18:41:50.174705 140474388072320 learning.py:507] global step 5913: loss = 0.8379 (1.298 sec/step)\n",
            "I0919 18:41:51.531423 140474388072320 learning.py:507] global step 5914: loss = 0.7602 (1.355 sec/step)\n",
            "I0919 18:41:52.832263 140474388072320 learning.py:507] global step 5915: loss = 0.9706 (1.299 sec/step)\n",
            "I0919 18:41:54.157476 140474388072320 learning.py:507] global step 5916: loss = 0.8229 (1.323 sec/step)\n",
            "I0919 18:41:55.506040 140474388072320 learning.py:507] global step 5917: loss = 1.0629 (1.347 sec/step)\n",
            "I0919 18:41:56.845479 140474388072320 learning.py:507] global step 5918: loss = 1.1974 (1.338 sec/step)\n",
            "I0919 18:41:58.140098 140474388072320 learning.py:507] global step 5919: loss = 0.8893 (1.293 sec/step)\n",
            "I0919 18:41:59.442882 140474388072320 learning.py:507] global step 5920: loss = 0.6499 (1.301 sec/step)\n",
            "I0919 18:42:00.842149 140474388072320 learning.py:507] global step 5921: loss = 0.7273 (1.397 sec/step)\n",
            "I0919 18:42:02.184714 140474388072320 learning.py:507] global step 5922: loss = 0.8647 (1.341 sec/step)\n",
            "I0919 18:42:03.494194 140474388072320 learning.py:507] global step 5923: loss = 0.9077 (1.307 sec/step)\n",
            "I0919 18:42:04.798074 140474388072320 learning.py:507] global step 5924: loss = 0.8940 (1.302 sec/step)\n",
            "I0919 18:42:06.102886 140474388072320 learning.py:507] global step 5925: loss = 0.9239 (1.303 sec/step)\n",
            "I0919 18:42:07.439330 140474388072320 learning.py:507] global step 5926: loss = 0.7589 (1.335 sec/step)\n",
            "I0919 18:42:08.733731 140474388072320 learning.py:507] global step 5927: loss = 0.8413 (1.293 sec/step)\n",
            "I0919 18:42:10.041837 140474388072320 learning.py:507] global step 5928: loss = 0.7251 (1.306 sec/step)\n",
            "I0919 18:42:11.332581 140474388072320 learning.py:507] global step 5929: loss = 0.7408 (1.289 sec/step)\n",
            "I0919 18:42:12.648421 140474388072320 learning.py:507] global step 5930: loss = 0.7858 (1.314 sec/step)\n",
            "I0919 18:42:13.951391 140474388072320 learning.py:507] global step 5931: loss = 0.8943 (1.301 sec/step)\n",
            "I0919 18:42:15.269090 140474388072320 learning.py:507] global step 5932: loss = 0.8341 (1.316 sec/step)\n",
            "I0919 18:42:16.549930 140474388072320 learning.py:507] global step 5933: loss = 0.8405 (1.279 sec/step)\n",
            "I0919 18:42:17.866496 140474388072320 learning.py:507] global step 5934: loss = 0.9227 (1.315 sec/step)\n",
            "I0919 18:42:19.182339 140474388072320 learning.py:507] global step 5935: loss = 0.8146 (1.314 sec/step)\n",
            "I0919 18:42:20.482800 140474388072320 learning.py:507] global step 5936: loss = 0.6372 (1.299 sec/step)\n",
            "I0919 18:42:21.821643 140474388072320 learning.py:507] global step 5937: loss = 0.7007 (1.337 sec/step)\n",
            "I0919 18:42:23.123972 140474388072320 learning.py:507] global step 5938: loss = 1.1999 (1.301 sec/step)\n",
            "I0919 18:42:24.433685 140474388072320 learning.py:507] global step 5939: loss = 0.7871 (1.308 sec/step)\n",
            "I0919 18:42:25.788821 140474388072320 learning.py:507] global step 5940: loss = 0.8751 (1.353 sec/step)\n",
            "I0919 18:42:27.078845 140474388072320 learning.py:507] global step 5941: loss = 0.7983 (1.288 sec/step)\n",
            "I0919 18:42:29.256388 140471297337088 supervisor.py:1050] Recording summary at step 5942.\n",
            "I0919 18:42:29.265839 140474388072320 learning.py:507] global step 5942: loss = 0.7435 (2.181 sec/step)\n",
            "I0919 18:42:30.613322 140474388072320 learning.py:507] global step 5943: loss = 0.9254 (1.342 sec/step)\n",
            "I0919 18:42:31.950486 140474388072320 learning.py:507] global step 5944: loss = 0.8182 (1.335 sec/step)\n",
            "I0919 18:42:33.275206 140474388072320 learning.py:507] global step 5945: loss = 0.8862 (1.323 sec/step)\n",
            "I0919 18:42:34.619790 140474388072320 learning.py:507] global step 5946: loss = 0.7019 (1.343 sec/step)\n",
            "I0919 18:42:35.981868 140474388072320 learning.py:507] global step 5947: loss = 0.7566 (1.360 sec/step)\n",
            "I0919 18:42:37.338407 140474388072320 learning.py:507] global step 5948: loss = 0.6619 (1.355 sec/step)\n",
            "I0919 18:42:38.662849 140474388072320 learning.py:507] global step 5949: loss = 0.7416 (1.323 sec/step)\n",
            "I0919 18:42:39.995454 140474388072320 learning.py:507] global step 5950: loss = 1.1511 (1.331 sec/step)\n",
            "I0919 18:42:41.303497 140474388072320 learning.py:507] global step 5951: loss = 0.9225 (1.306 sec/step)\n",
            "I0919 18:42:42.621308 140474388072320 learning.py:507] global step 5952: loss = 0.7338 (1.316 sec/step)\n",
            "I0919 18:42:43.930852 140474388072320 learning.py:507] global step 5953: loss = 0.6282 (1.307 sec/step)\n",
            "I0919 18:42:45.235898 140474388072320 learning.py:507] global step 5954: loss = 0.8432 (1.303 sec/step)\n",
            "I0919 18:42:46.528008 140474388072320 learning.py:507] global step 5955: loss = 0.7487 (1.290 sec/step)\n",
            "I0919 18:42:47.825228 140474388072320 learning.py:507] global step 5956: loss = 0.8511 (1.295 sec/step)\n",
            "I0919 18:42:49.191992 140474388072320 learning.py:507] global step 5957: loss = 0.9978 (1.365 sec/step)\n",
            "I0919 18:42:50.505187 140474388072320 learning.py:507] global step 5958: loss = 0.7358 (1.311 sec/step)\n",
            "I0919 18:42:51.805800 140474388072320 learning.py:507] global step 5959: loss = 0.8475 (1.299 sec/step)\n",
            "I0919 18:42:53.099201 140474388072320 learning.py:507] global step 5960: loss = 0.6996 (1.292 sec/step)\n",
            "I0919 18:42:54.404603 140474388072320 learning.py:507] global step 5961: loss = 0.7975 (1.303 sec/step)\n",
            "I0919 18:42:55.777087 140474388072320 learning.py:507] global step 5962: loss = 0.9540 (1.367 sec/step)\n",
            "I0919 18:42:57.088175 140474388072320 learning.py:507] global step 5963: loss = 0.7374 (1.309 sec/step)\n",
            "I0919 18:42:58.458724 140474388072320 learning.py:507] global step 5964: loss = 0.8402 (1.368 sec/step)\n",
            "I0919 18:42:59.745687 140474388072320 learning.py:507] global step 5965: loss = 0.6338 (1.285 sec/step)\n",
            "I0919 18:43:01.088546 140474388072320 learning.py:507] global step 5966: loss = 0.6868 (1.341 sec/step)\n",
            "I0919 18:43:02.422236 140474388072320 learning.py:507] global step 5967: loss = 0.9623 (1.332 sec/step)\n",
            "I0919 18:43:03.744580 140474388072320 learning.py:507] global step 5968: loss = 0.8960 (1.321 sec/step)\n",
            "I0919 18:43:05.042308 140474388072320 learning.py:507] global step 5969: loss = 0.8524 (1.296 sec/step)\n",
            "I0919 18:43:06.339838 140474388072320 learning.py:507] global step 5970: loss = 0.7262 (1.296 sec/step)\n",
            "I0919 18:43:07.625740 140474388072320 learning.py:507] global step 5971: loss = 0.9045 (1.284 sec/step)\n",
            "I0919 18:43:08.943139 140474388072320 learning.py:507] global step 5972: loss = 0.8994 (1.316 sec/step)\n",
            "I0919 18:43:10.250583 140474388072320 learning.py:507] global step 5973: loss = 1.1391 (1.305 sec/step)\n",
            "I0919 18:43:11.565413 140474388072320 learning.py:507] global step 5974: loss = 0.9985 (1.313 sec/step)\n",
            "I0919 18:43:12.869713 140474388072320 learning.py:507] global step 5975: loss = 0.8392 (1.303 sec/step)\n",
            "I0919 18:43:14.146629 140474388072320 learning.py:507] global step 5976: loss = 0.6634 (1.275 sec/step)\n",
            "I0919 18:43:15.461731 140474388072320 learning.py:507] global step 5977: loss = 0.8950 (1.313 sec/step)\n",
            "I0919 18:43:16.789450 140474388072320 learning.py:507] global step 5978: loss = 0.7218 (1.326 sec/step)\n",
            "I0919 18:43:18.083405 140474388072320 learning.py:507] global step 5979: loss = 0.7103 (1.292 sec/step)\n",
            "I0919 18:43:19.398175 140474388072320 learning.py:507] global step 5980: loss = 0.9898 (1.313 sec/step)\n",
            "I0919 18:43:20.684688 140474388072320 learning.py:507] global step 5981: loss = 0.6192 (1.285 sec/step)\n",
            "I0919 18:43:21.969494 140474388072320 learning.py:507] global step 5982: loss = 0.8339 (1.283 sec/step)\n",
            "I0919 18:43:23.307528 140474388072320 learning.py:507] global step 5983: loss = 0.9782 (1.336 sec/step)\n",
            "I0919 18:43:24.683234 140474388072320 learning.py:507] global step 5984: loss = 0.7748 (1.374 sec/step)\n",
            "I0919 18:43:26.050698 140474388072320 learning.py:507] global step 5985: loss = 0.8619 (1.366 sec/step)\n",
            "I0919 18:43:27.337384 140474388072320 learning.py:507] global step 5986: loss = 0.7044 (1.280 sec/step)\n",
            "I0919 18:43:28.647736 140474388072320 learning.py:507] global step 5987: loss = 0.9476 (1.308 sec/step)\n",
            "I0919 18:43:29.981084 140474388072320 learning.py:507] global step 5988: loss = 0.7689 (1.331 sec/step)\n",
            "I0919 18:43:31.305419 140474388072320 learning.py:507] global step 5989: loss = 0.7506 (1.323 sec/step)\n",
            "I0919 18:43:32.636023 140474388072320 learning.py:507] global step 5990: loss = 0.6649 (1.329 sec/step)\n",
            "I0919 18:43:33.963157 140474388072320 learning.py:507] global step 5991: loss = 0.7113 (1.325 sec/step)\n",
            "I0919 18:43:35.260945 140474388072320 learning.py:507] global step 5992: loss = 0.7520 (1.296 sec/step)\n",
            "I0919 18:43:36.576184 140474388072320 learning.py:507] global step 5993: loss = 0.7232 (1.313 sec/step)\n",
            "I0919 18:43:37.875941 140474388072320 learning.py:507] global step 5994: loss = 0.7880 (1.298 sec/step)\n",
            "I0919 18:43:39.164666 140474388072320 learning.py:507] global step 5995: loss = 0.7524 (1.287 sec/step)\n",
            "I0919 18:43:40.479378 140474388072320 learning.py:507] global step 5996: loss = 0.5776 (1.313 sec/step)\n",
            "I0919 18:43:41.801280 140474388072320 learning.py:507] global step 5997: loss = 0.7694 (1.320 sec/step)\n",
            "I0919 18:43:43.136981 140474388072320 learning.py:507] global step 5998: loss = 0.9133 (1.334 sec/step)\n",
            "I0919 18:43:44.435255 140474388072320 learning.py:507] global step 5999: loss = 0.6800 (1.296 sec/step)\n",
            "I0919 18:43:45.731014 140474388072320 learning.py:507] global step 6000: loss = 0.8635 (1.294 sec/step)\n",
            "I0919 18:43:47.086656 140474388072320 learning.py:507] global step 6001: loss = 0.8215 (1.354 sec/step)\n",
            "I0919 18:43:48.446042 140474388072320 learning.py:507] global step 6002: loss = 0.9208 (1.358 sec/step)\n",
            "I0919 18:43:49.812717 140474388072320 learning.py:507] global step 6003: loss = 0.6470 (1.365 sec/step)\n",
            "I0919 18:43:51.100501 140474388072320 learning.py:507] global step 6004: loss = 1.0486 (1.286 sec/step)\n",
            "I0919 18:43:52.401716 140474388072320 learning.py:507] global step 6005: loss = 0.8613 (1.300 sec/step)\n",
            "I0919 18:43:53.748152 140474388072320 learning.py:507] global step 6006: loss = 0.6423 (1.345 sec/step)\n",
            "I0919 18:43:55.029186 140474388072320 learning.py:507] global step 6007: loss = 0.7115 (1.279 sec/step)\n",
            "I0919 18:43:56.318952 140474388072320 learning.py:507] global step 6008: loss = 0.8388 (1.288 sec/step)\n",
            "I0919 18:43:57.680816 140474388072320 learning.py:507] global step 6009: loss = 0.7911 (1.360 sec/step)\n",
            "I0919 18:43:59.038028 140474388072320 learning.py:507] global step 6010: loss = 0.6912 (1.355 sec/step)\n",
            "I0919 18:44:00.364881 140474388072320 learning.py:507] global step 6011: loss = 0.7289 (1.325 sec/step)\n",
            "I0919 18:44:01.717089 140474388072320 learning.py:507] global step 6012: loss = 0.7045 (1.350 sec/step)\n",
            "I0919 18:44:03.076286 140474388072320 learning.py:507] global step 6013: loss = 1.0979 (1.357 sec/step)\n",
            "I0919 18:44:04.395159 140474388072320 learning.py:507] global step 6014: loss = 0.8490 (1.316 sec/step)\n",
            "I0919 18:44:05.708255 140474388072320 learning.py:507] global step 6015: loss = 0.7756 (1.311 sec/step)\n",
            "I0919 18:44:07.067301 140474388072320 learning.py:507] global step 6016: loss = 0.9223 (1.357 sec/step)\n",
            "I0919 18:44:08.378446 140474388072320 learning.py:507] global step 6017: loss = 0.8869 (1.309 sec/step)\n",
            "I0919 18:44:09.695554 140474388072320 learning.py:507] global step 6018: loss = 0.6773 (1.315 sec/step)\n",
            "I0919 18:44:11.040537 140474388072320 learning.py:507] global step 6019: loss = 0.7064 (1.343 sec/step)\n",
            "I0919 18:44:12.331698 140474388072320 learning.py:507] global step 6020: loss = 0.9293 (1.289 sec/step)\n",
            "I0919 18:44:13.649672 140474388072320 learning.py:507] global step 6021: loss = 0.9657 (1.316 sec/step)\n",
            "I0919 18:44:14.939558 140474388072320 learning.py:507] global step 6022: loss = 0.7167 (1.286 sec/step)\n",
            "I0919 18:44:16.314716 140474388072320 learning.py:507] global step 6023: loss = 0.7116 (1.373 sec/step)\n",
            "I0919 18:44:17.667695 140474388072320 learning.py:507] global step 6024: loss = 0.8059 (1.351 sec/step)\n",
            "I0919 18:44:18.966943 140474388072320 learning.py:507] global step 6025: loss = 0.8661 (1.297 sec/step)\n",
            "I0919 18:44:20.321829 140474388072320 learning.py:507] global step 6026: loss = 0.6952 (1.353 sec/step)\n",
            "I0919 18:44:21.627171 140474388072320 learning.py:507] global step 6027: loss = 0.7248 (1.304 sec/step)\n",
            "I0919 18:44:22.965420 140474388072320 learning.py:507] global step 6028: loss = 0.5912 (1.336 sec/step)\n",
            "I0919 18:44:24.344748 140474388072320 learning.py:507] global step 6029: loss = 0.7603 (1.377 sec/step)\n",
            "I0919 18:44:25.663434 140474388072320 learning.py:507] global step 6030: loss = 0.7430 (1.317 sec/step)\n",
            "I0919 18:44:26.978363 140474388072320 learning.py:507] global step 6031: loss = 0.8196 (1.313 sec/step)\n",
            "I0919 18:44:28.814996 140471297337088 supervisor.py:1050] Recording summary at step 6031.\n",
            "I0919 18:44:29.297153 140474388072320 learning.py:507] global step 6032: loss = 0.8460 (2.317 sec/step)\n",
            "I0919 18:44:30.633233 140474388072320 learning.py:507] global step 6033: loss = 0.8357 (1.334 sec/step)\n",
            "I0919 18:44:31.933324 140474388072320 learning.py:507] global step 6034: loss = 0.7561 (1.299 sec/step)\n",
            "I0919 18:44:33.301744 140474388072320 learning.py:507] global step 6035: loss = 0.6791 (1.367 sec/step)\n",
            "I0919 18:44:34.600260 140474388072320 learning.py:507] global step 6036: loss = 0.6414 (1.296 sec/step)\n",
            "I0919 18:44:35.915976 140474388072320 learning.py:507] global step 6037: loss = 0.9057 (1.314 sec/step)\n",
            "I0919 18:44:37.254172 140474388072320 learning.py:507] global step 6038: loss = 0.7583 (1.337 sec/step)\n",
            "I0919 18:44:38.547818 140474388072320 learning.py:507] global step 6039: loss = 0.6962 (1.292 sec/step)\n",
            "I0919 18:44:39.846302 140474388072320 learning.py:507] global step 6040: loss = 0.8167 (1.296 sec/step)\n",
            "I0919 18:44:41.144201 140474388072320 learning.py:507] global step 6041: loss = 0.8332 (1.296 sec/step)\n",
            "I0919 18:44:42.479690 140474388072320 learning.py:507] global step 6042: loss = 0.9101 (1.334 sec/step)\n",
            "I0919 18:44:43.766308 140474388072320 learning.py:507] global step 6043: loss = 0.7575 (1.285 sec/step)\n",
            "I0919 18:44:45.034756 140474388072320 learning.py:507] global step 6044: loss = 0.8900 (1.267 sec/step)\n",
            "I0919 18:44:46.332519 140474388072320 learning.py:507] global step 6045: loss = 0.6077 (1.296 sec/step)\n",
            "I0919 18:44:47.643583 140474388072320 learning.py:507] global step 6046: loss = 0.8089 (1.309 sec/step)\n",
            "I0919 18:44:48.933748 140474388072320 learning.py:507] global step 6047: loss = 0.8332 (1.288 sec/step)\n",
            "I0919 18:44:50.300261 140474388072320 learning.py:507] global step 6048: loss = 0.7513 (1.365 sec/step)\n",
            "I0919 18:44:51.624232 140474388072320 learning.py:507] global step 6049: loss = 0.7515 (1.322 sec/step)\n",
            "I0919 18:44:52.960227 140474388072320 learning.py:507] global step 6050: loss = 0.8529 (1.335 sec/step)\n",
            "I0919 18:44:54.271229 140474388072320 learning.py:507] global step 6051: loss = 0.8973 (1.309 sec/step)\n",
            "I0919 18:44:55.576103 140474388072320 learning.py:507] global step 6052: loss = 0.8131 (1.303 sec/step)\n",
            "I0919 18:44:56.918922 140474388072320 learning.py:507] global step 6053: loss = 0.8239 (1.339 sec/step)\n",
            "I0919 18:44:58.247467 140474388072320 learning.py:507] global step 6054: loss = 0.8659 (1.325 sec/step)\n",
            "I0919 18:44:59.587086 140474388072320 learning.py:507] global step 6055: loss = 0.9382 (1.338 sec/step)\n",
            "I0919 18:45:00.914195 140474388072320 learning.py:507] global step 6056: loss = 0.6890 (1.323 sec/step)\n",
            "I0919 18:45:02.251518 140474388072320 learning.py:507] global step 6057: loss = 0.8247 (1.333 sec/step)\n",
            "I0919 18:45:03.549283 140474388072320 learning.py:507] global step 6058: loss = 0.8587 (1.296 sec/step)\n",
            "I0919 18:45:04.886676 140474388072320 learning.py:507] global step 6059: loss = 0.8551 (1.336 sec/step)\n",
            "I0919 18:45:06.200499 140474388072320 learning.py:507] global step 6060: loss = 0.7842 (1.312 sec/step)\n",
            "I0919 18:45:07.491010 140474388072320 learning.py:507] global step 6061: loss = 0.7158 (1.289 sec/step)\n",
            "I0919 18:45:08.811916 140474388072320 learning.py:507] global step 6062: loss = 0.6757 (1.319 sec/step)\n",
            "I0919 18:45:10.092324 140474388072320 learning.py:507] global step 6063: loss = 0.6122 (1.279 sec/step)\n",
            "I0919 18:45:11.442167 140474388072320 learning.py:507] global step 6064: loss = 0.7666 (1.348 sec/step)\n",
            "I0919 18:45:12.750761 140474388072320 learning.py:507] global step 6065: loss = 0.7829 (1.307 sec/step)\n",
            "I0919 18:45:14.054809 140474388072320 learning.py:507] global step 6066: loss = 0.6997 (1.302 sec/step)\n",
            "I0919 18:45:15.333169 140474388072320 learning.py:507] global step 6067: loss = 0.7599 (1.276 sec/step)\n",
            "I0919 18:45:16.632302 140474388072320 learning.py:507] global step 6068: loss = 0.6612 (1.297 sec/step)\n",
            "I0919 18:45:18.003050 140474388072320 learning.py:507] global step 6069: loss = 0.8564 (1.369 sec/step)\n",
            "I0919 18:45:19.354790 140474388072320 learning.py:507] global step 6070: loss = 0.7748 (1.350 sec/step)\n",
            "I0919 18:45:20.711092 140474388072320 learning.py:507] global step 6071: loss = 0.7569 (1.355 sec/step)\n",
            "I0919 18:45:22.039006 140474388072320 learning.py:507] global step 6072: loss = 0.7837 (1.326 sec/step)\n",
            "I0919 18:45:23.366251 140474388072320 learning.py:507] global step 6073: loss = 0.7441 (1.326 sec/step)\n",
            "I0919 18:45:24.685559 140474388072320 learning.py:507] global step 6074: loss = 0.6788 (1.318 sec/step)\n",
            "I0919 18:45:26.016089 140474388072320 learning.py:507] global step 6075: loss = 0.8058 (1.328 sec/step)\n",
            "I0919 18:45:27.312290 140474388072320 learning.py:507] global step 6076: loss = 0.8069 (1.295 sec/step)\n",
            "I0919 18:45:28.612971 140474388072320 learning.py:507] global step 6077: loss = 0.7939 (1.299 sec/step)\n",
            "I0919 18:45:29.966602 140474388072320 learning.py:507] global step 6078: loss = 0.6257 (1.352 sec/step)\n",
            "I0919 18:45:31.279793 140474388072320 learning.py:507] global step 6079: loss = 0.6734 (1.312 sec/step)\n",
            "I0919 18:45:32.621189 140474388072320 learning.py:507] global step 6080: loss = 0.8897 (1.340 sec/step)\n",
            "I0919 18:45:33.950397 140474388072320 learning.py:507] global step 6081: loss = 0.7590 (1.327 sec/step)\n",
            "I0919 18:45:35.248677 140474388072320 learning.py:507] global step 6082: loss = 0.8991 (1.295 sec/step)\n",
            "I0919 18:45:36.534307 140474388072320 learning.py:507] global step 6083: loss = 0.8101 (1.284 sec/step)\n",
            "I0919 18:45:37.840611 140474388072320 learning.py:507] global step 6084: loss = 0.9124 (1.305 sec/step)\n",
            "I0919 18:45:39.178544 140474388072320 learning.py:507] global step 6085: loss = 0.8191 (1.336 sec/step)\n",
            "I0919 18:45:40.499202 140474388072320 learning.py:507] global step 6086: loss = 0.9511 (1.319 sec/step)\n",
            "I0919 18:45:41.807013 140474388072320 learning.py:507] global step 6087: loss = 0.9473 (1.306 sec/step)\n",
            "I0919 18:45:43.126454 140474388072320 learning.py:507] global step 6088: loss = 0.8685 (1.318 sec/step)\n",
            "I0919 18:45:44.442384 140474388072320 learning.py:507] global step 6089: loss = 0.8188 (1.314 sec/step)\n",
            "I0919 18:45:45.773379 140474388072320 learning.py:507] global step 6090: loss = 0.9267 (1.329 sec/step)\n",
            "I0919 18:45:47.052994 140474388072320 learning.py:507] global step 6091: loss = 0.9062 (1.278 sec/step)\n",
            "I0919 18:45:48.377540 140474388072320 learning.py:507] global step 6092: loss = 0.7517 (1.323 sec/step)\n",
            "I0919 18:45:49.682077 140474388072320 learning.py:507] global step 6093: loss = 0.8828 (1.303 sec/step)\n",
            "I0919 18:45:50.982099 140474388072320 learning.py:507] global step 6094: loss = 0.7245 (1.298 sec/step)\n",
            "I0919 18:45:52.280478 140474388072320 learning.py:507] global step 6095: loss = 0.6831 (1.297 sec/step)\n",
            "I0919 18:45:53.614804 140474388072320 learning.py:507] global step 6096: loss = 0.6602 (1.333 sec/step)\n",
            "I0919 18:45:54.935275 140474388072320 learning.py:507] global step 6097: loss = 0.6265 (1.319 sec/step)\n",
            "I0919 18:45:56.263413 140474388072320 learning.py:507] global step 6098: loss = 0.8123 (1.326 sec/step)\n",
            "I0919 18:45:57.575256 140474388072320 learning.py:507] global step 6099: loss = 0.9163 (1.310 sec/step)\n",
            "I0919 18:45:58.884403 140474388072320 learning.py:507] global step 6100: loss = 0.7763 (1.307 sec/step)\n",
            "I0919 18:46:00.263936 140474388072320 learning.py:507] global step 6101: loss = 0.5848 (1.378 sec/step)\n",
            "I0919 18:46:01.592319 140474388072320 learning.py:507] global step 6102: loss = 0.6205 (1.327 sec/step)\n",
            "I0919 18:46:02.965334 140474388072320 learning.py:507] global step 6103: loss = 0.6151 (1.371 sec/step)\n",
            "I0919 18:46:04.277281 140474388072320 learning.py:507] global step 6104: loss = 0.7872 (1.310 sec/step)\n",
            "I0919 18:46:05.582722 140474388072320 learning.py:507] global step 6105: loss = 0.7840 (1.304 sec/step)\n",
            "I0919 18:46:06.891999 140474388072320 learning.py:507] global step 6106: loss = 0.9325 (1.307 sec/step)\n",
            "I0919 18:46:08.194188 140474388072320 learning.py:507] global step 6107: loss = 0.6078 (1.300 sec/step)\n",
            "I0919 18:46:09.520410 140474388072320 learning.py:507] global step 6108: loss = 0.6412 (1.324 sec/step)\n",
            "I0919 18:46:10.822691 140474388072320 learning.py:507] global step 6109: loss = 0.9049 (1.301 sec/step)\n",
            "I0919 18:46:12.137582 140474388072320 learning.py:507] global step 6110: loss = 0.8516 (1.313 sec/step)\n",
            "I0919 18:46:13.435033 140474388072320 learning.py:507] global step 6111: loss = 0.7263 (1.296 sec/step)\n",
            "I0919 18:46:14.771445 140474388072320 learning.py:507] global step 6112: loss = 0.8080 (1.335 sec/step)\n",
            "I0919 18:46:16.082027 140474388072320 learning.py:507] global step 6113: loss = 0.6679 (1.309 sec/step)\n",
            "I0919 18:46:17.417829 140474388072320 learning.py:507] global step 6114: loss = 0.8220 (1.334 sec/step)\n",
            "I0919 18:46:18.741827 140474388072320 learning.py:507] global step 6115: loss = 0.8411 (1.322 sec/step)\n",
            "I0919 18:46:20.076773 140474388072320 learning.py:507] global step 6116: loss = 1.0176 (1.333 sec/step)\n",
            "I0919 18:46:21.398051 140474388072320 learning.py:507] global step 6117: loss = 0.6583 (1.319 sec/step)\n",
            "I0919 18:46:22.701564 140474388072320 learning.py:507] global step 6118: loss = 0.6422 (1.302 sec/step)\n",
            "I0919 18:46:23.992873 140474388072320 learning.py:507] global step 6119: loss = 0.8092 (1.290 sec/step)\n",
            "I0919 18:46:25.290669 140474388072320 learning.py:507] global step 6120: loss = 0.7634 (1.296 sec/step)\n",
            "I0919 18:46:26.580177 140474388072320 learning.py:507] global step 6121: loss = 0.6579 (1.288 sec/step)\n",
            "I0919 18:46:27.849534 140474388072320 learning.py:507] global step 6122: loss = 0.7910 (1.267 sec/step)\n",
            "I0919 18:46:29.429266 140471297337088 supervisor.py:1050] Recording summary at step 6122.\n",
            "I0919 18:46:30.093907 140474388072320 learning.py:507] global step 6123: loss = 0.7319 (1.957 sec/step)\n",
            "I0919 18:46:31.412261 140474388072320 learning.py:507] global step 6124: loss = 0.6447 (1.317 sec/step)\n",
            "I0919 18:46:32.715877 140474388072320 learning.py:507] global step 6125: loss = 0.6236 (1.302 sec/step)\n",
            "I0919 18:46:34.001499 140474388072320 learning.py:507] global step 6126: loss = 0.7976 (1.284 sec/step)\n",
            "I0919 18:46:35.292808 140474388072320 learning.py:507] global step 6127: loss = 0.6805 (1.290 sec/step)\n",
            "I0919 18:46:36.635094 140474388072320 learning.py:507] global step 6128: loss = 0.6593 (1.341 sec/step)\n",
            "I0919 18:46:37.948987 140474388072320 learning.py:507] global step 6129: loss = 0.7213 (1.312 sec/step)\n",
            "I0919 18:46:39.268248 140474388072320 learning.py:507] global step 6130: loss = 1.0485 (1.317 sec/step)\n",
            "I0919 18:46:40.575855 140474388072320 learning.py:507] global step 6131: loss = 1.1181 (1.306 sec/step)\n",
            "I0919 18:46:41.889667 140474388072320 learning.py:507] global step 6132: loss = 0.7544 (1.312 sec/step)\n",
            "I0919 18:46:43.191050 140474388072320 learning.py:507] global step 6133: loss = 0.7995 (1.300 sec/step)\n",
            "I0919 18:46:44.502245 140474388072320 learning.py:507] global step 6134: loss = 1.0326 (1.309 sec/step)\n",
            "I0919 18:46:45.860894 140474388072320 learning.py:507] global step 6135: loss = 0.7577 (1.357 sec/step)\n",
            "I0919 18:46:47.172213 140474388072320 learning.py:507] global step 6136: loss = 0.7644 (1.310 sec/step)\n",
            "I0919 18:46:48.484359 140474388072320 learning.py:507] global step 6137: loss = 0.7700 (1.310 sec/step)\n",
            "I0919 18:46:49.799540 140474388072320 learning.py:507] global step 6138: loss = 0.5738 (1.313 sec/step)\n",
            "I0919 18:46:51.116133 140474388072320 learning.py:507] global step 6139: loss = 1.1917 (1.315 sec/step)\n",
            "I0919 18:46:52.442996 140474388072320 learning.py:507] global step 6140: loss = 0.8365 (1.325 sec/step)\n",
            "I0919 18:46:53.788933 140474388072320 learning.py:507] global step 6141: loss = 0.7918 (1.344 sec/step)\n",
            "I0919 18:46:55.063697 140474388072320 learning.py:507] global step 6142: loss = 0.9212 (1.273 sec/step)\n",
            "I0919 18:46:56.411063 140474388072320 learning.py:507] global step 6143: loss = 0.8856 (1.346 sec/step)\n",
            "I0919 18:46:57.695233 140474388072320 learning.py:507] global step 6144: loss = 1.0831 (1.282 sec/step)\n",
            "I0919 18:46:59.000405 140474388072320 learning.py:507] global step 6145: loss = 1.0780 (1.302 sec/step)\n",
            "I0919 18:47:00.326453 140474388072320 learning.py:507] global step 6146: loss = 0.8957 (1.324 sec/step)\n",
            "I0919 18:47:01.633280 140474388072320 learning.py:507] global step 6147: loss = 0.8082 (1.305 sec/step)\n",
            "I0919 18:47:02.944452 140474388072320 learning.py:507] global step 6148: loss = 0.7319 (1.309 sec/step)\n",
            "I0919 18:47:04.321187 140474388072320 learning.py:507] global step 6149: loss = 0.8642 (1.375 sec/step)\n",
            "I0919 18:47:05.656492 140474388072320 learning.py:507] global step 6150: loss = 0.7923 (1.334 sec/step)\n",
            "I0919 18:47:07.007298 140474388072320 learning.py:507] global step 6151: loss = 0.6959 (1.349 sec/step)\n",
            "I0919 18:47:08.379018 140474388072320 learning.py:507] global step 6152: loss = 1.3001 (1.370 sec/step)\n",
            "I0919 18:47:09.720961 140474388072320 learning.py:507] global step 6153: loss = 0.6718 (1.340 sec/step)\n",
            "I0919 18:47:11.090054 140474388072320 learning.py:507] global step 6154: loss = 0.9100 (1.367 sec/step)\n",
            "I0919 18:47:12.438424 140474388072320 learning.py:507] global step 6155: loss = 0.7370 (1.347 sec/step)\n",
            "I0919 18:47:13.762586 140474388072320 learning.py:507] global step 6156: loss = 0.7412 (1.322 sec/step)\n",
            "I0919 18:47:15.100619 140474388072320 learning.py:507] global step 6157: loss = 0.6507 (1.336 sec/step)\n",
            "I0919 18:47:16.393290 140474388072320 learning.py:507] global step 6158: loss = 0.8312 (1.291 sec/step)\n",
            "I0919 18:47:17.692857 140474388072320 learning.py:507] global step 6159: loss = 0.8791 (1.298 sec/step)\n",
            "I0919 18:47:18.986455 140474388072320 learning.py:507] global step 6160: loss = 0.7283 (1.292 sec/step)\n",
            "I0919 18:47:20.297154 140474388072320 learning.py:507] global step 6161: loss = 0.9775 (1.309 sec/step)\n",
            "I0919 18:47:21.610779 140474388072320 learning.py:507] global step 6162: loss = 0.7670 (1.312 sec/step)\n",
            "I0919 18:47:22.933708 140474388072320 learning.py:507] global step 6163: loss = 0.7819 (1.321 sec/step)\n",
            "I0919 18:47:24.266477 140474388072320 learning.py:507] global step 6164: loss = 0.8295 (1.331 sec/step)\n",
            "I0919 18:47:25.567739 140474388072320 learning.py:507] global step 6165: loss = 0.8034 (1.299 sec/step)\n",
            "I0919 18:47:26.854460 140474388072320 learning.py:507] global step 6166: loss = 0.6069 (1.285 sec/step)\n",
            "I0919 18:47:28.148896 140474388072320 learning.py:507] global step 6167: loss = 1.0234 (1.293 sec/step)\n",
            "I0919 18:47:29.481722 140474388072320 learning.py:507] global step 6168: loss = 0.7335 (1.331 sec/step)\n",
            "I0919 18:47:30.877911 140474388072320 learning.py:507] global step 6169: loss = 0.7346 (1.395 sec/step)\n",
            "I0919 18:47:32.164140 140474388072320 learning.py:507] global step 6170: loss = 0.8241 (1.285 sec/step)\n",
            "I0919 18:47:33.475379 140474388072320 learning.py:507] global step 6171: loss = 0.6700 (1.309 sec/step)\n",
            "I0919 18:47:34.751743 140474388072320 learning.py:507] global step 6172: loss = 0.6856 (1.275 sec/step)\n",
            "I0919 18:47:36.071090 140474388072320 learning.py:507] global step 6173: loss = 0.7896 (1.318 sec/step)\n",
            "I0919 18:47:37.359452 140474388072320 learning.py:507] global step 6174: loss = 0.8535 (1.286 sec/step)\n",
            "I0919 18:47:38.660794 140474388072320 learning.py:507] global step 6175: loss = 0.6792 (1.300 sec/step)\n",
            "I0919 18:47:39.969284 140474388072320 learning.py:507] global step 6176: loss = 0.9950 (1.307 sec/step)\n",
            "I0919 18:47:41.299077 140474388072320 learning.py:507] global step 6177: loss = 0.6549 (1.328 sec/step)\n",
            "I0919 18:47:42.602776 140474388072320 learning.py:507] global step 6178: loss = 0.7840 (1.301 sec/step)\n",
            "I0919 18:47:43.903418 140474388072320 learning.py:507] global step 6179: loss = 0.7503 (1.294 sec/step)\n",
            "I0919 18:47:45.206028 140474388072320 learning.py:507] global step 6180: loss = 0.8781 (1.301 sec/step)\n",
            "I0919 18:47:46.545726 140474388072320 learning.py:507] global step 6181: loss = 0.8010 (1.338 sec/step)\n",
            "I0919 18:47:47.865983 140474388072320 learning.py:507] global step 6182: loss = 0.8387 (1.318 sec/step)\n",
            "I0919 18:47:49.186956 140474388072320 learning.py:507] global step 6183: loss = 0.9852 (1.319 sec/step)\n",
            "I0919 18:47:50.499078 140474388072320 learning.py:507] global step 6184: loss = 0.7156 (1.310 sec/step)\n",
            "I0919 18:47:51.801071 140474388072320 learning.py:507] global step 6185: loss = 0.7377 (1.300 sec/step)\n",
            "I0919 18:47:53.188442 140474388072320 learning.py:507] global step 6186: loss = 0.7796 (1.386 sec/step)\n",
            "I0919 18:47:54.503921 140474388072320 learning.py:507] global step 6187: loss = 0.7568 (1.314 sec/step)\n",
            "I0919 18:47:55.795974 140474388072320 learning.py:507] global step 6188: loss = 0.7596 (1.291 sec/step)\n",
            "I0919 18:47:57.111694 140474388072320 learning.py:507] global step 6189: loss = 0.6466 (1.314 sec/step)\n",
            "I0919 18:47:58.407192 140474388072320 learning.py:507] global step 6190: loss = 0.8398 (1.294 sec/step)\n",
            "I0919 18:47:59.763765 140474388072320 learning.py:507] global step 6191: loss = 0.5217 (1.355 sec/step)\n",
            "I0919 18:48:01.075726 140474388072320 learning.py:507] global step 6192: loss = 1.1949 (1.310 sec/step)\n",
            "I0919 18:48:02.362102 140474388072320 learning.py:507] global step 6193: loss = 0.9049 (1.285 sec/step)\n",
            "I0919 18:48:03.684330 140474388072320 learning.py:507] global step 6194: loss = 0.7722 (1.320 sec/step)\n",
            "I0919 18:48:05.005194 140474388072320 learning.py:507] global step 6195: loss = 0.8695 (1.319 sec/step)\n",
            "I0919 18:48:06.363152 140474388072320 learning.py:507] global step 6196: loss = 0.7731 (1.356 sec/step)\n",
            "I0919 18:48:07.657371 140474388072320 learning.py:507] global step 6197: loss = 0.9108 (1.292 sec/step)\n",
            "I0919 18:48:08.961476 140474388072320 learning.py:507] global step 6198: loss = 0.6410 (1.302 sec/step)\n",
            "I0919 18:48:10.261759 140474388072320 learning.py:507] global step 6199: loss = 0.7465 (1.298 sec/step)\n",
            "I0919 18:48:11.560942 140474388072320 learning.py:507] global step 6200: loss = 0.7432 (1.297 sec/step)\n",
            "I0919 18:48:12.861761 140474388072320 learning.py:507] global step 6201: loss = 0.6761 (1.299 sec/step)\n",
            "I0919 18:48:14.185246 140474388072320 learning.py:507] global step 6202: loss = 0.7947 (1.322 sec/step)\n",
            "I0919 18:48:15.513062 140474388072320 learning.py:507] global step 6203: loss = 0.6886 (1.326 sec/step)\n",
            "I0919 18:48:16.799180 140474388072320 learning.py:507] global step 6204: loss = 0.7507 (1.285 sec/step)\n",
            "I0919 18:48:18.084916 140474388072320 learning.py:507] global step 6205: loss = 0.7077 (1.284 sec/step)\n",
            "I0919 18:48:19.383225 140474388072320 learning.py:507] global step 6206: loss = 0.7655 (1.297 sec/step)\n",
            "I0919 18:48:20.703576 140474388072320 learning.py:507] global step 6207: loss = 0.7976 (1.319 sec/step)\n",
            "I0919 18:48:22.016779 140474388072320 learning.py:507] global step 6208: loss = 0.7228 (1.309 sec/step)\n",
            "I0919 18:48:23.344010 140474388072320 learning.py:507] global step 6209: loss = 0.7793 (1.325 sec/step)\n",
            "I0919 18:48:24.636080 140474388072320 learning.py:507] global step 6210: loss = 0.7406 (1.290 sec/step)\n",
            "I0919 18:48:25.927541 140474388072320 learning.py:507] global step 6211: loss = 0.6481 (1.290 sec/step)\n",
            "I0919 18:48:27.179548 140474388072320 learning.py:507] global step 6212: loss = 0.9399 (1.250 sec/step)\n",
            "I0919 18:48:28.955888 140471297337088 supervisor.py:1050] Recording summary at step 6212.\n",
            "I0919 18:48:29.502845 140474388072320 learning.py:507] global step 6213: loss = 0.7898 (2.322 sec/step)\n",
            "I0919 18:48:30.821799 140474388072320 learning.py:507] global step 6214: loss = 0.7888 (1.317 sec/step)\n",
            "I0919 18:48:32.163511 140474388072320 learning.py:507] global step 6215: loss = 0.7816 (1.340 sec/step)\n",
            "I0919 18:48:33.485032 140474388072320 learning.py:507] global step 6216: loss = 0.8512 (1.320 sec/step)\n",
            "I0919 18:48:34.787987 140474388072320 learning.py:507] global step 6217: loss = 0.7018 (1.301 sec/step)\n",
            "I0919 18:48:36.090270 140474388072320 learning.py:507] global step 6218: loss = 0.9633 (1.300 sec/step)\n",
            "I0919 18:48:37.390402 140474388072320 learning.py:507] global step 6219: loss = 0.8905 (1.298 sec/step)\n",
            "I0919 18:48:38.705049 140474388072320 learning.py:507] global step 6220: loss = 0.7716 (1.313 sec/step)\n",
            "I0919 18:48:40.063437 140474388072320 learning.py:507] global step 6221: loss = 0.6785 (1.357 sec/step)\n",
            "I0919 18:48:41.387557 140474388072320 learning.py:507] global step 6222: loss = 0.7457 (1.323 sec/step)\n",
            "I0919 18:48:42.688344 140474388072320 learning.py:507] global step 6223: loss = 0.8496 (1.299 sec/step)\n",
            "I0919 18:48:44.037889 140474388072320 learning.py:507] global step 6224: loss = 0.7107 (1.348 sec/step)\n",
            "I0919 18:48:45.354413 140474388072320 learning.py:507] global step 6225: loss = 0.8449 (1.315 sec/step)\n",
            "I0919 18:48:46.688967 140474388072320 learning.py:507] global step 6226: loss = 0.9701 (1.333 sec/step)\n",
            "I0919 18:48:47.980010 140474388072320 learning.py:507] global step 6227: loss = 0.7770 (1.289 sec/step)\n",
            "I0919 18:48:49.266257 140474388072320 learning.py:507] global step 6228: loss = 0.7604 (1.284 sec/step)\n",
            "I0919 18:48:50.583520 140474388072320 learning.py:507] global step 6229: loss = 1.1319 (1.316 sec/step)\n",
            "I0919 18:48:51.869728 140474388072320 learning.py:507] global step 6230: loss = 0.7867 (1.284 sec/step)\n",
            "I0919 18:48:53.186518 140474388072320 learning.py:507] global step 6231: loss = 0.7846 (1.315 sec/step)\n",
            "I0919 18:48:54.483622 140474388072320 learning.py:507] global step 6232: loss = 0.9238 (1.296 sec/step)\n",
            "I0919 18:48:55.802572 140474388072320 learning.py:507] global step 6233: loss = 0.8194 (1.317 sec/step)\n",
            "I0919 18:48:57.083842 140474388072320 learning.py:507] global step 6234: loss = 0.9506 (1.280 sec/step)\n",
            "I0919 18:48:58.390398 140474388072320 learning.py:507] global step 6235: loss = 0.7782 (1.305 sec/step)\n",
            "I0919 18:48:59.709247 140474388072320 learning.py:507] global step 6236: loss = 0.6039 (1.317 sec/step)\n",
            "I0919 18:49:01.043133 140474388072320 learning.py:507] global step 6237: loss = 1.4277 (1.332 sec/step)\n",
            "I0919 18:49:02.377571 140474388072320 learning.py:507] global step 6238: loss = 0.9531 (1.332 sec/step)\n",
            "I0919 18:49:03.720666 140474388072320 learning.py:507] global step 6239: loss = 0.8042 (1.341 sec/step)\n",
            "I0919 18:49:05.061947 140474388072320 learning.py:507] global step 6240: loss = 0.8189 (1.340 sec/step)\n",
            "I0919 18:49:06.395831 140474388072320 learning.py:507] global step 6241: loss = 0.8041 (1.332 sec/step)\n",
            "I0919 18:49:07.718236 140474388072320 learning.py:507] global step 6242: loss = 0.7017 (1.320 sec/step)\n",
            "I0919 18:49:09.097182 140474388072320 learning.py:507] global step 6243: loss = 0.7818 (1.377 sec/step)\n",
            "I0919 18:49:10.387813 140474388072320 learning.py:507] global step 6244: loss = 0.8117 (1.289 sec/step)\n",
            "I0919 18:49:11.699361 140474388072320 learning.py:507] global step 6245: loss = 0.7462 (1.310 sec/step)\n",
            "I0919 18:49:13.022836 140474388072320 learning.py:507] global step 6246: loss = 0.7668 (1.322 sec/step)\n",
            "I0919 18:49:14.366180 140474388072320 learning.py:507] global step 6247: loss = 0.6745 (1.341 sec/step)\n",
            "I0919 18:49:15.671065 140474388072320 learning.py:507] global step 6248: loss = 0.5989 (1.303 sec/step)\n",
            "I0919 18:49:16.950238 140474388072320 learning.py:507] global step 6249: loss = 0.8230 (1.277 sec/step)\n",
            "I0919 18:49:18.258187 140474388072320 learning.py:507] global step 6250: loss = 0.8372 (1.306 sec/step)\n",
            "I0919 18:49:19.589329 140474388072320 learning.py:507] global step 6251: loss = 0.9138 (1.330 sec/step)\n",
            "I0919 18:49:20.920727 140474388072320 learning.py:507] global step 6252: loss = 0.7663 (1.330 sec/step)\n",
            "I0919 18:49:22.246627 140474388072320 learning.py:507] global step 6253: loss = 1.0592 (1.324 sec/step)\n",
            "I0919 18:49:23.592417 140474388072320 learning.py:507] global step 6254: loss = 1.0430 (1.344 sec/step)\n",
            "I0919 18:49:24.893383 140474388072320 learning.py:507] global step 6255: loss = 1.2724 (1.299 sec/step)\n",
            "I0919 18:49:26.175280 140474388072320 learning.py:507] global step 6256: loss = 0.8382 (1.280 sec/step)\n",
            "I0919 18:49:27.494728 140474388072320 learning.py:507] global step 6257: loss = 0.8741 (1.317 sec/step)\n",
            "I0919 18:49:28.800060 140474388072320 learning.py:507] global step 6258: loss = 1.0858 (1.304 sec/step)\n",
            "I0919 18:49:30.096220 140474388072320 learning.py:507] global step 6259: loss = 0.7219 (1.295 sec/step)\n",
            "I0919 18:49:31.413547 140474388072320 learning.py:507] global step 6260: loss = 0.8403 (1.316 sec/step)\n",
            "I0919 18:49:32.755012 140474388072320 learning.py:507] global step 6261: loss = 0.9793 (1.339 sec/step)\n",
            "I0919 18:49:34.037710 140474388072320 learning.py:507] global step 6262: loss = 0.8303 (1.279 sec/step)\n",
            "I0919 18:49:35.357507 140474388072320 learning.py:507] global step 6263: loss = 1.0480 (1.318 sec/step)\n",
            "I0919 18:49:36.655452 140474388072320 learning.py:507] global step 6264: loss = 0.8668 (1.296 sec/step)\n",
            "I0919 18:49:37.970707 140474388072320 learning.py:507] global step 6265: loss = 0.8650 (1.313 sec/step)\n",
            "I0919 18:49:39.269993 140474388072320 learning.py:507] global step 6266: loss = 0.8184 (1.297 sec/step)\n",
            "I0919 18:49:40.580404 140474388072320 learning.py:507] global step 6267: loss = 0.9298 (1.308 sec/step)\n",
            "I0919 18:49:41.885534 140474388072320 learning.py:507] global step 6268: loss = 0.7841 (1.303 sec/step)\n",
            "I0919 18:49:43.233175 140474388072320 learning.py:507] global step 6269: loss = 0.8586 (1.346 sec/step)\n",
            "I0919 18:49:44.516333 140474388072320 learning.py:507] global step 6270: loss = 0.7478 (1.281 sec/step)\n",
            "I0919 18:49:45.824985 140474388072320 learning.py:507] global step 6271: loss = 0.7713 (1.307 sec/step)\n",
            "I0919 18:49:47.176628 140474388072320 learning.py:507] global step 6272: loss = 0.7964 (1.350 sec/step)\n",
            "I0919 18:49:48.500782 140474388072320 learning.py:507] global step 6273: loss = 1.2358 (1.322 sec/step)\n",
            "I0919 18:49:49.823049 140474388072320 learning.py:507] global step 6274: loss = 0.8129 (1.321 sec/step)\n",
            "I0919 18:49:51.135574 140474388072320 learning.py:507] global step 6275: loss = 0.8785 (1.311 sec/step)\n",
            "I0919 18:49:52.432903 140474388072320 learning.py:507] global step 6276: loss = 0.7480 (1.295 sec/step)\n",
            "I0919 18:49:53.800580 140474388072320 learning.py:507] global step 6277: loss = 0.7948 (1.366 sec/step)\n",
            "I0919 18:49:55.118979 140474388072320 learning.py:507] global step 6278: loss = 0.9791 (1.317 sec/step)\n",
            "I0919 18:49:56.431181 140474388072320 learning.py:507] global step 6279: loss = 0.7611 (1.311 sec/step)\n",
            "I0919 18:49:57.721097 140474388072320 learning.py:507] global step 6280: loss = 0.7713 (1.288 sec/step)\n",
            "I0919 18:49:59.029055 140474388072320 learning.py:507] global step 6281: loss = 0.8520 (1.306 sec/step)\n",
            "I0919 18:50:00.332884 140474388072320 learning.py:507] global step 6282: loss = 0.7579 (1.302 sec/step)\n",
            "I0919 18:50:01.662377 140474388072320 learning.py:507] global step 6283: loss = 0.8429 (1.328 sec/step)\n",
            "I0919 18:50:02.965998 140474388072320 learning.py:507] global step 6284: loss = 0.6820 (1.302 sec/step)\n",
            "I0919 18:50:04.287808 140474388072320 learning.py:507] global step 6285: loss = 0.6901 (1.320 sec/step)\n",
            "I0919 18:50:05.612802 140474388072320 learning.py:507] global step 6286: loss = 0.9336 (1.323 sec/step)\n",
            "I0919 18:50:06.957887 140474388072320 learning.py:507] global step 6287: loss = 0.8969 (1.343 sec/step)\n",
            "I0919 18:50:08.315371 140474388072320 learning.py:507] global step 6288: loss = 0.7054 (1.356 sec/step)\n",
            "I0919 18:50:09.621052 140474388072320 learning.py:507] global step 6289: loss = 0.7128 (1.304 sec/step)\n",
            "I0919 18:50:10.964889 140474388072320 learning.py:507] global step 6290: loss = 0.6414 (1.342 sec/step)\n",
            "I0919 18:50:12.279445 140474388072320 learning.py:507] global step 6291: loss = 1.0496 (1.313 sec/step)\n",
            "I0919 18:50:13.619059 140474388072320 learning.py:507] global step 6292: loss = 0.8285 (1.338 sec/step)\n",
            "I0919 18:50:14.984476 140474388072320 learning.py:507] global step 6293: loss = 0.6201 (1.364 sec/step)\n",
            "I0919 18:50:16.300584 140474388072320 learning.py:507] global step 6294: loss = 0.7329 (1.314 sec/step)\n",
            "I0919 18:50:17.631856 140474388072320 learning.py:507] global step 6295: loss = 0.7042 (1.330 sec/step)\n",
            "I0919 18:50:18.925461 140474388072320 learning.py:507] global step 6296: loss = 1.0254 (1.292 sec/step)\n",
            "I0919 18:50:20.236915 140474388072320 learning.py:507] global step 6297: loss = 1.0641 (1.310 sec/step)\n",
            "I0919 18:50:21.531369 140474388072320 learning.py:507] global step 6298: loss = 0.6572 (1.293 sec/step)\n",
            "I0919 18:50:22.898643 140474388072320 learning.py:507] global step 6299: loss = 0.8739 (1.365 sec/step)\n",
            "I0919 18:50:24.253350 140474388072320 learning.py:507] global step 6300: loss = 0.7870 (1.353 sec/step)\n",
            "I0919 18:50:25.539309 140474388072320 learning.py:507] global step 6301: loss = 0.8863 (1.284 sec/step)\n",
            "I0919 18:50:26.843602 140474388072320 learning.py:507] global step 6302: loss = 0.9145 (1.302 sec/step)\n",
            "I0919 18:50:26.872826 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 18:50:29.984179 140471297337088 supervisor.py:1050] Recording summary at step 6303.\n",
            "I0919 18:50:29.987441 140474388072320 learning.py:507] global step 6303: loss = 0.7947 (3.139 sec/step)\n",
            "I0919 18:50:32.060318 140474388072320 learning.py:507] global step 6304: loss = 0.7940 (2.059 sec/step)\n",
            "I0919 18:50:33.639786 140474388072320 learning.py:507] global step 6305: loss = 0.7131 (1.578 sec/step)\n",
            "I0919 18:50:34.964289 140474388072320 learning.py:507] global step 6306: loss = 0.8168 (1.323 sec/step)\n",
            "I0919 18:50:36.277328 140474388072320 learning.py:507] global step 6307: loss = 0.9725 (1.311 sec/step)\n",
            "I0919 18:50:37.610915 140474388072320 learning.py:507] global step 6308: loss = 0.7947 (1.331 sec/step)\n",
            "I0919 18:50:38.959620 140474388072320 learning.py:507] global step 6309: loss = 0.7141 (1.347 sec/step)\n",
            "I0919 18:50:40.287621 140474388072320 learning.py:507] global step 6310: loss = 0.7288 (1.326 sec/step)\n",
            "I0919 18:50:41.608012 140474388072320 learning.py:507] global step 6311: loss = 0.8561 (1.319 sec/step)\n",
            "I0919 18:50:42.922059 140474388072320 learning.py:507] global step 6312: loss = 0.7648 (1.312 sec/step)\n",
            "I0919 18:50:44.193494 140474388072320 learning.py:507] global step 6313: loss = 0.9372 (1.270 sec/step)\n",
            "I0919 18:50:45.481843 140474388072320 learning.py:507] global step 6314: loss = 0.6711 (1.286 sec/step)\n",
            "I0919 18:50:46.811594 140474388072320 learning.py:507] global step 6315: loss = 0.8801 (1.328 sec/step)\n",
            "I0919 18:50:48.109457 140474388072320 learning.py:507] global step 6316: loss = 0.9684 (1.296 sec/step)\n",
            "I0919 18:50:49.428194 140474388072320 learning.py:507] global step 6317: loss = 0.7658 (1.317 sec/step)\n",
            "I0919 18:50:50.731137 140474388072320 learning.py:507] global step 6318: loss = 0.7881 (1.301 sec/step)\n",
            "I0919 18:50:52.019994 140474388072320 learning.py:507] global step 6319: loss = 0.7589 (1.287 sec/step)\n",
            "I0919 18:50:53.358207 140474388072320 learning.py:507] global step 6320: loss = 0.8357 (1.336 sec/step)\n",
            "I0919 18:50:54.669660 140474388072320 learning.py:507] global step 6321: loss = 0.8295 (1.310 sec/step)\n",
            "I0919 18:50:56.041192 140474388072320 learning.py:507] global step 6322: loss = 0.6007 (1.370 sec/step)\n",
            "I0919 18:50:57.332426 140474388072320 learning.py:507] global step 6323: loss = 1.0754 (1.289 sec/step)\n",
            "I0919 18:50:58.676589 140474388072320 learning.py:507] global step 6324: loss = 0.6551 (1.342 sec/step)\n",
            "I0919 18:50:59.978624 140474388072320 learning.py:507] global step 6325: loss = 0.6488 (1.300 sec/step)\n",
            "I0919 18:51:01.282638 140474388072320 learning.py:507] global step 6326: loss = 0.8231 (1.302 sec/step)\n",
            "I0919 18:51:02.628436 140474388072320 learning.py:507] global step 6327: loss = 0.7843 (1.344 sec/step)\n",
            "I0919 18:51:03.910010 140474388072320 learning.py:507] global step 6328: loss = 0.8405 (1.280 sec/step)\n",
            "I0919 18:51:05.227663 140474388072320 learning.py:507] global step 6329: loss = 0.7606 (1.316 sec/step)\n",
            "I0919 18:51:06.580153 140474388072320 learning.py:507] global step 6330: loss = 1.0892 (1.350 sec/step)\n",
            "I0919 18:51:07.939048 140474388072320 learning.py:507] global step 6331: loss = 0.8089 (1.357 sec/step)\n",
            "I0919 18:51:09.270179 140474388072320 learning.py:507] global step 6332: loss = 0.7517 (1.329 sec/step)\n",
            "I0919 18:51:10.597723 140474388072320 learning.py:507] global step 6333: loss = 0.7137 (1.326 sec/step)\n",
            "I0919 18:51:11.894906 140474388072320 learning.py:507] global step 6334: loss = 0.8116 (1.295 sec/step)\n",
            "I0919 18:51:13.258449 140474388072320 learning.py:507] global step 6335: loss = 0.6706 (1.362 sec/step)\n",
            "I0919 18:51:14.598922 140474388072320 learning.py:507] global step 6336: loss = 0.7158 (1.339 sec/step)\n",
            "I0919 18:51:15.960179 140474388072320 learning.py:507] global step 6337: loss = 0.7691 (1.359 sec/step)\n",
            "I0919 18:51:17.336810 140474388072320 learning.py:507] global step 6338: loss = 0.6978 (1.375 sec/step)\n",
            "I0919 18:51:18.631508 140474388072320 learning.py:507] global step 6339: loss = 0.6582 (1.293 sec/step)\n",
            "I0919 18:51:19.947078 140474388072320 learning.py:507] global step 6340: loss = 0.8354 (1.314 sec/step)\n",
            "I0919 18:51:21.235853 140474388072320 learning.py:507] global step 6341: loss = 1.0737 (1.287 sec/step)\n",
            "I0919 18:51:22.538522 140474388072320 learning.py:507] global step 6342: loss = 0.9508 (1.301 sec/step)\n",
            "I0919 18:51:23.875555 140474388072320 learning.py:507] global step 6343: loss = 0.7360 (1.336 sec/step)\n",
            "I0919 18:51:25.243817 140474388072320 learning.py:507] global step 6344: loss = 0.7527 (1.366 sec/step)\n",
            "I0919 18:51:26.546768 140474388072320 learning.py:507] global step 6345: loss = 0.8087 (1.301 sec/step)\n",
            "I0919 18:51:27.886909 140474388072320 learning.py:507] global step 6346: loss = 1.0615 (1.338 sec/step)\n",
            "I0919 18:51:29.222991 140474388072320 learning.py:507] global step 6347: loss = 0.8452 (1.334 sec/step)\n",
            "I0919 18:51:30.580849 140474388072320 learning.py:507] global step 6348: loss = 0.6766 (1.356 sec/step)\n",
            "I0919 18:51:31.887384 140474388072320 learning.py:507] global step 6349: loss = 0.7970 (1.304 sec/step)\n",
            "I0919 18:51:33.227432 140474388072320 learning.py:507] global step 6350: loss = 1.0284 (1.338 sec/step)\n",
            "I0919 18:51:34.537365 140474388072320 learning.py:507] global step 6351: loss = 0.7083 (1.308 sec/step)\n",
            "I0919 18:51:35.860938 140474388072320 learning.py:507] global step 6352: loss = 0.7685 (1.322 sec/step)\n",
            "I0919 18:51:37.170139 140474388072320 learning.py:507] global step 6353: loss = 0.4820 (1.307 sec/step)\n",
            "I0919 18:51:38.477988 140474388072320 learning.py:507] global step 6354: loss = 0.5550 (1.306 sec/step)\n",
            "I0919 18:51:39.769077 140474388072320 learning.py:507] global step 6355: loss = 0.8718 (1.289 sec/step)\n",
            "I0919 18:51:41.058845 140474388072320 learning.py:507] global step 6356: loss = 0.8272 (1.288 sec/step)\n",
            "I0919 18:51:42.349528 140474388072320 learning.py:507] global step 6357: loss = 0.9496 (1.289 sec/step)\n",
            "I0919 18:51:43.670162 140474388072320 learning.py:507] global step 6358: loss = 0.8637 (1.319 sec/step)\n",
            "I0919 18:51:44.968987 140474388072320 learning.py:507] global step 6359: loss = 0.8389 (1.297 sec/step)\n",
            "I0919 18:51:46.321850 140474388072320 learning.py:507] global step 6360: loss = 0.6754 (1.351 sec/step)\n",
            "I0919 18:51:47.657718 140474388072320 learning.py:507] global step 6361: loss = 0.8779 (1.332 sec/step)\n",
            "I0919 18:51:48.985894 140474388072320 learning.py:507] global step 6362: loss = 0.6973 (1.327 sec/step)\n",
            "I0919 18:51:50.316847 140474388072320 learning.py:507] global step 6363: loss = 0.9189 (1.329 sec/step)\n",
            "I0919 18:51:51.609915 140474388072320 learning.py:507] global step 6364: loss = 0.8823 (1.291 sec/step)\n",
            "I0919 18:51:52.928343 140474388072320 learning.py:507] global step 6365: loss = 0.7691 (1.317 sec/step)\n",
            "I0919 18:51:54.204185 140474388072320 learning.py:507] global step 6366: loss = 0.9339 (1.274 sec/step)\n",
            "I0919 18:51:55.542638 140474388072320 learning.py:507] global step 6367: loss = 0.7666 (1.337 sec/step)\n",
            "I0919 18:51:56.871129 140474388072320 learning.py:507] global step 6368: loss = 1.0360 (1.327 sec/step)\n",
            "I0919 18:51:58.230684 140474388072320 learning.py:507] global step 6369: loss = 0.7034 (1.358 sec/step)\n",
            "I0919 18:51:59.529081 140474388072320 learning.py:507] global step 6370: loss = 1.1033 (1.297 sec/step)\n",
            "I0919 18:52:00.833101 140474388072320 learning.py:507] global step 6371: loss = 0.7472 (1.302 sec/step)\n",
            "I0919 18:52:02.121243 140474388072320 learning.py:507] global step 6372: loss = 0.9572 (1.286 sec/step)\n",
            "I0919 18:52:03.417711 140474388072320 learning.py:507] global step 6373: loss = 0.8626 (1.295 sec/step)\n",
            "I0919 18:52:04.764539 140474388072320 learning.py:507] global step 6374: loss = 0.7603 (1.345 sec/step)\n",
            "I0919 18:52:06.108097 140474388072320 learning.py:507] global step 6375: loss = 0.7164 (1.342 sec/step)\n",
            "I0919 18:52:07.434334 140474388072320 learning.py:507] global step 6376: loss = 0.7736 (1.324 sec/step)\n",
            "I0919 18:52:08.741055 140474388072320 learning.py:507] global step 6377: loss = 0.7345 (1.305 sec/step)\n",
            "I0919 18:52:10.065280 140474388072320 learning.py:507] global step 6378: loss = 0.7063 (1.323 sec/step)\n",
            "I0919 18:52:11.410429 140474388072320 learning.py:507] global step 6379: loss = 0.5844 (1.344 sec/step)\n",
            "I0919 18:52:12.725212 140474388072320 learning.py:507] global step 6380: loss = 0.9792 (1.313 sec/step)\n",
            "I0919 18:52:14.037223 140474388072320 learning.py:507] global step 6381: loss = 0.7624 (1.311 sec/step)\n",
            "I0919 18:52:15.322377 140474388072320 learning.py:507] global step 6382: loss = 0.7434 (1.283 sec/step)\n",
            "I0919 18:52:16.645308 140474388072320 learning.py:507] global step 6383: loss = 0.7026 (1.321 sec/step)\n",
            "I0919 18:52:17.963473 140474388072320 learning.py:507] global step 6384: loss = 0.7822 (1.317 sec/step)\n",
            "I0919 18:52:19.325486 140474388072320 learning.py:507] global step 6385: loss = 0.8257 (1.360 sec/step)\n",
            "I0919 18:52:20.669586 140474388072320 learning.py:507] global step 6386: loss = 0.7407 (1.343 sec/step)\n",
            "I0919 18:52:22.015577 140474388072320 learning.py:507] global step 6387: loss = 0.8748 (1.344 sec/step)\n",
            "I0919 18:52:23.309604 140474388072320 learning.py:507] global step 6388: loss = 0.7429 (1.292 sec/step)\n",
            "I0919 18:52:24.652348 140474388072320 learning.py:507] global step 6389: loss = 0.9226 (1.340 sec/step)\n",
            "I0919 18:52:25.946881 140474388072320 learning.py:507] global step 6390: loss = 0.7782 (1.292 sec/step)\n",
            "I0919 18:52:27.465377 140474388072320 learning.py:507] global step 6391: loss = 0.7856 (1.370 sec/step)\n",
            "I0919 18:52:28.868359 140471297337088 supervisor.py:1050] Recording summary at step 6391.\n",
            "I0919 18:52:29.444519 140474388072320 learning.py:507] global step 6392: loss = 0.8522 (1.969 sec/step)\n",
            "I0919 18:52:30.754564 140474388072320 learning.py:507] global step 6393: loss = 0.8722 (1.308 sec/step)\n",
            "I0919 18:52:32.055382 140474388072320 learning.py:507] global step 6394: loss = 0.8512 (1.294 sec/step)\n",
            "I0919 18:52:33.404392 140474388072320 learning.py:507] global step 6395: loss = 0.7115 (1.347 sec/step)\n",
            "I0919 18:52:34.721410 140474388072320 learning.py:507] global step 6396: loss = 0.7271 (1.315 sec/step)\n",
            "I0919 18:52:36.009833 140474388072320 learning.py:507] global step 6397: loss = 0.8423 (1.287 sec/step)\n",
            "I0919 18:52:37.291749 140474388072320 learning.py:507] global step 6398: loss = 0.7053 (1.280 sec/step)\n",
            "I0919 18:52:38.600704 140474388072320 learning.py:507] global step 6399: loss = 0.8555 (1.307 sec/step)\n",
            "I0919 18:52:39.901340 140474388072320 learning.py:507] global step 6400: loss = 0.7049 (1.299 sec/step)\n",
            "I0919 18:52:41.225640 140474388072320 learning.py:507] global step 6401: loss = 0.7889 (1.323 sec/step)\n",
            "I0919 18:52:42.554872 140474388072320 learning.py:507] global step 6402: loss = 0.8893 (1.327 sec/step)\n",
            "I0919 18:52:43.848917 140474388072320 learning.py:507] global step 6403: loss = 0.9006 (1.292 sec/step)\n",
            "I0919 18:52:45.153702 140474388072320 learning.py:507] global step 6404: loss = 0.6645 (1.303 sec/step)\n",
            "I0919 18:52:46.462265 140474388072320 learning.py:507] global step 6405: loss = 0.9226 (1.307 sec/step)\n",
            "I0919 18:52:47.793929 140474388072320 learning.py:507] global step 6406: loss = 0.6257 (1.330 sec/step)\n",
            "I0919 18:52:49.119033 140474388072320 learning.py:507] global step 6407: loss = 1.1175 (1.323 sec/step)\n",
            "I0919 18:52:50.470518 140474388072320 learning.py:507] global step 6408: loss = 0.7153 (1.350 sec/step)\n",
            "I0919 18:52:51.771600 140474388072320 learning.py:507] global step 6409: loss = 0.9892 (1.299 sec/step)\n",
            "I0919 18:52:53.132419 140474388072320 learning.py:507] global step 6410: loss = 0.6358 (1.359 sec/step)\n",
            "I0919 18:52:54.444947 140474388072320 learning.py:507] global step 6411: loss = 0.9560 (1.311 sec/step)\n",
            "I0919 18:52:55.738708 140474388072320 learning.py:507] global step 6412: loss = 0.7391 (1.292 sec/step)\n",
            "I0919 18:52:57.066767 140474388072320 learning.py:507] global step 6413: loss = 0.6248 (1.326 sec/step)\n",
            "I0919 18:52:58.354535 140474388072320 learning.py:507] global step 6414: loss = 0.8146 (1.286 sec/step)\n",
            "I0919 18:52:59.661983 140474388072320 learning.py:507] global step 6415: loss = 0.8491 (1.306 sec/step)\n",
            "I0919 18:53:01.013156 140474388072320 learning.py:507] global step 6416: loss = 1.0059 (1.349 sec/step)\n",
            "I0919 18:53:02.317984 140474388072320 learning.py:507] global step 6417: loss = 0.7945 (1.303 sec/step)\n",
            "I0919 18:53:03.628756 140474388072320 learning.py:507] global step 6418: loss = 0.7533 (1.309 sec/step)\n",
            "I0919 18:53:05.010667 140474388072320 learning.py:507] global step 6419: loss = 0.8468 (1.380 sec/step)\n",
            "I0919 18:53:06.309047 140474388072320 learning.py:507] global step 6420: loss = 0.7432 (1.297 sec/step)\n",
            "I0919 18:53:07.654411 140474388072320 learning.py:507] global step 6421: loss = 0.7598 (1.344 sec/step)\n",
            "I0919 18:53:09.001513 140474388072320 learning.py:507] global step 6422: loss = 0.9069 (1.345 sec/step)\n",
            "I0919 18:53:10.298194 140474388072320 learning.py:507] global step 6423: loss = 0.8288 (1.295 sec/step)\n",
            "I0919 18:53:11.588409 140474388072320 learning.py:507] global step 6424: loss = 0.6505 (1.288 sec/step)\n",
            "I0919 18:53:12.893568 140474388072320 learning.py:507] global step 6425: loss = 0.5927 (1.304 sec/step)\n",
            "I0919 18:53:14.211946 140474388072320 learning.py:507] global step 6426: loss = 0.7407 (1.316 sec/step)\n",
            "I0919 18:53:15.500205 140474388072320 learning.py:507] global step 6427: loss = 0.8750 (1.286 sec/step)\n",
            "I0919 18:53:16.802179 140474388072320 learning.py:507] global step 6428: loss = 0.7337 (1.300 sec/step)\n",
            "I0919 18:53:18.134677 140474388072320 learning.py:507] global step 6429: loss = 0.7470 (1.330 sec/step)\n",
            "I0919 18:53:19.426826 140474388072320 learning.py:507] global step 6430: loss = 0.7591 (1.290 sec/step)\n",
            "I0919 18:53:20.723100 140474388072320 learning.py:507] global step 6431: loss = 0.9397 (1.295 sec/step)\n",
            "I0919 18:53:22.012757 140474388072320 learning.py:507] global step 6432: loss = 1.0150 (1.288 sec/step)\n",
            "I0919 18:53:23.339345 140474388072320 learning.py:507] global step 6433: loss = 0.6649 (1.325 sec/step)\n",
            "I0919 18:53:24.699220 140474388072320 learning.py:507] global step 6434: loss = 0.5364 (1.358 sec/step)\n",
            "I0919 18:53:26.031527 140474388072320 learning.py:507] global step 6435: loss = 0.8229 (1.331 sec/step)\n",
            "I0919 18:53:27.356497 140474388072320 learning.py:507] global step 6436: loss = 0.7110 (1.323 sec/step)\n",
            "I0919 18:53:28.703342 140474388072320 learning.py:507] global step 6437: loss = 0.8986 (1.345 sec/step)\n",
            "I0919 18:53:30.016640 140474388072320 learning.py:507] global step 6438: loss = 0.9306 (1.312 sec/step)\n",
            "I0919 18:53:31.333483 140474388072320 learning.py:507] global step 6439: loss = 0.7819 (1.315 sec/step)\n",
            "I0919 18:53:32.683849 140474388072320 learning.py:507] global step 6440: loss = 0.9249 (1.346 sec/step)\n",
            "I0919 18:53:34.014795 140474388072320 learning.py:507] global step 6441: loss = 0.7846 (1.329 sec/step)\n",
            "I0919 18:53:35.336524 140474388072320 learning.py:507] global step 6442: loss = 0.7955 (1.320 sec/step)\n",
            "I0919 18:53:36.669883 140474388072320 learning.py:507] global step 6443: loss = 1.1216 (1.331 sec/step)\n",
            "I0919 18:53:37.987534 140474388072320 learning.py:507] global step 6444: loss = 0.5737 (1.315 sec/step)\n",
            "I0919 18:53:39.299819 140474388072320 learning.py:507] global step 6445: loss = 0.8176 (1.310 sec/step)\n",
            "I0919 18:53:40.600982 140474388072320 learning.py:507] global step 6446: loss = 0.9456 (1.299 sec/step)\n",
            "I0919 18:53:41.899446 140474388072320 learning.py:507] global step 6447: loss = 0.9518 (1.297 sec/step)\n",
            "I0919 18:53:43.186515 140474388072320 learning.py:507] global step 6448: loss = 0.9227 (1.285 sec/step)\n",
            "I0919 18:53:44.492047 140474388072320 learning.py:507] global step 6449: loss = 0.9311 (1.304 sec/step)\n",
            "I0919 18:53:45.802858 140474388072320 learning.py:507] global step 6450: loss = 0.7841 (1.309 sec/step)\n",
            "I0919 18:53:47.122370 140474388072320 learning.py:507] global step 6451: loss = 0.8937 (1.317 sec/step)\n",
            "I0919 18:53:48.442099 140474388072320 learning.py:507] global step 6452: loss = 0.7597 (1.318 sec/step)\n",
            "I0919 18:53:49.770760 140474388072320 learning.py:507] global step 6453: loss = 0.5969 (1.327 sec/step)\n",
            "I0919 18:53:51.069315 140474388072320 learning.py:507] global step 6454: loss = 0.8987 (1.297 sec/step)\n",
            "I0919 18:53:52.398253 140474388072320 learning.py:507] global step 6455: loss = 0.8049 (1.327 sec/step)\n",
            "I0919 18:53:53.730252 140474388072320 learning.py:507] global step 6456: loss = 0.9131 (1.330 sec/step)\n",
            "I0919 18:53:55.047578 140474388072320 learning.py:507] global step 6457: loss = 0.8628 (1.315 sec/step)\n",
            "I0919 18:53:56.360887 140474388072320 learning.py:507] global step 6458: loss = 0.8754 (1.311 sec/step)\n",
            "I0919 18:53:57.655024 140474388072320 learning.py:507] global step 6459: loss = 0.7332 (1.290 sec/step)\n",
            "I0919 18:53:59.010946 140474388072320 learning.py:507] global step 6460: loss = 0.8097 (1.354 sec/step)\n",
            "I0919 18:54:00.317905 140474388072320 learning.py:507] global step 6461: loss = 0.7543 (1.305 sec/step)\n",
            "I0919 18:54:01.659212 140474388072320 learning.py:507] global step 6462: loss = 0.8112 (1.339 sec/step)\n",
            "I0919 18:54:02.992721 140474388072320 learning.py:507] global step 6463: loss = 0.7213 (1.332 sec/step)\n",
            "I0919 18:54:04.312947 140474388072320 learning.py:507] global step 6464: loss = 0.8991 (1.318 sec/step)\n",
            "I0919 18:54:05.661565 140474388072320 learning.py:507] global step 6465: loss = 0.8315 (1.347 sec/step)\n",
            "I0919 18:54:06.979537 140474388072320 learning.py:507] global step 6466: loss = 0.7313 (1.316 sec/step)\n",
            "I0919 18:54:08.291280 140474388072320 learning.py:507] global step 6467: loss = 0.8356 (1.310 sec/step)\n",
            "I0919 18:54:09.612606 140474388072320 learning.py:507] global step 6468: loss = 0.6994 (1.319 sec/step)\n",
            "I0919 18:54:10.943289 140474388072320 learning.py:507] global step 6469: loss = 0.8037 (1.329 sec/step)\n",
            "I0919 18:54:12.242017 140474388072320 learning.py:507] global step 6470: loss = 0.7747 (1.297 sec/step)\n",
            "I0919 18:54:13.536420 140474388072320 learning.py:507] global step 6471: loss = 0.8234 (1.293 sec/step)\n",
            "I0919 18:54:14.861057 140474388072320 learning.py:507] global step 6472: loss = 0.7483 (1.323 sec/step)\n",
            "I0919 18:54:16.168963 140474388072320 learning.py:507] global step 6473: loss = 0.6161 (1.306 sec/step)\n",
            "I0919 18:54:17.492522 140474388072320 learning.py:507] global step 6474: loss = 0.8093 (1.321 sec/step)\n",
            "I0919 18:54:18.878103 140474388072320 learning.py:507] global step 6475: loss = 0.8325 (1.384 sec/step)\n",
            "I0919 18:54:20.156048 140474388072320 learning.py:507] global step 6476: loss = 0.6404 (1.276 sec/step)\n",
            "I0919 18:54:21.440547 140474388072320 learning.py:507] global step 6477: loss = 0.7302 (1.283 sec/step)\n",
            "I0919 18:54:22.747704 140474388072320 learning.py:507] global step 6478: loss = 0.6577 (1.305 sec/step)\n",
            "I0919 18:54:24.073204 140474388072320 learning.py:507] global step 6479: loss = 0.8012 (1.324 sec/step)\n",
            "I0919 18:54:25.397310 140474388072320 learning.py:507] global step 6480: loss = 0.9560 (1.322 sec/step)\n",
            "I0919 18:54:26.774992 140474388072320 learning.py:507] global step 6481: loss = 1.1624 (1.376 sec/step)\n",
            "I0919 18:54:28.831301 140474388072320 learning.py:507] global step 6482: loss = 0.7034 (2.052 sec/step)\n",
            "I0919 18:54:28.832086 140471297337088 supervisor.py:1050] Recording summary at step 6482.\n",
            "I0919 18:54:30.156361 140474388072320 learning.py:507] global step 6483: loss = 0.8108 (1.315 sec/step)\n",
            "I0919 18:54:31.462668 140474388072320 learning.py:507] global step 6484: loss = 0.8883 (1.305 sec/step)\n",
            "I0919 18:54:32.766057 140474388072320 learning.py:507] global step 6485: loss = 0.6335 (1.302 sec/step)\n",
            "I0919 18:54:34.041344 140474388072320 learning.py:507] global step 6486: loss = 0.7632 (1.273 sec/step)\n",
            "I0919 18:54:35.359203 140474388072320 learning.py:507] global step 6487: loss = 0.8936 (1.316 sec/step)\n",
            "I0919 18:54:36.675148 140474388072320 learning.py:507] global step 6488: loss = 0.6568 (1.314 sec/step)\n",
            "I0919 18:54:38.034416 140474388072320 learning.py:507] global step 6489: loss = 0.6843 (1.358 sec/step)\n",
            "I0919 18:54:39.354150 140474388072320 learning.py:507] global step 6490: loss = 0.6850 (1.318 sec/step)\n",
            "I0919 18:54:40.661050 140474388072320 learning.py:507] global step 6491: loss = 0.7439 (1.304 sec/step)\n",
            "I0919 18:54:41.979830 140474388072320 learning.py:507] global step 6492: loss = 0.7822 (1.317 sec/step)\n",
            "I0919 18:54:43.283324 140474388072320 learning.py:507] global step 6493: loss = 0.7367 (1.302 sec/step)\n",
            "I0919 18:54:44.586589 140474388072320 learning.py:507] global step 6494: loss = 0.7016 (1.302 sec/step)\n",
            "I0919 18:54:45.901633 140474388072320 learning.py:507] global step 6495: loss = 0.9391 (1.313 sec/step)\n",
            "I0919 18:54:47.220401 140474388072320 learning.py:507] global step 6496: loss = 1.0928 (1.317 sec/step)\n",
            "I0919 18:54:48.547033 140474388072320 learning.py:507] global step 6497: loss = 0.6934 (1.325 sec/step)\n",
            "I0919 18:54:49.865361 140474388072320 learning.py:507] global step 6498: loss = 0.7636 (1.316 sec/step)\n",
            "I0919 18:54:51.187850 140474388072320 learning.py:507] global step 6499: loss = 0.6207 (1.320 sec/step)\n",
            "I0919 18:54:52.513088 140474388072320 learning.py:507] global step 6500: loss = 0.9750 (1.323 sec/step)\n",
            "I0919 18:54:53.832650 140474388072320 learning.py:507] global step 6501: loss = 0.7789 (1.318 sec/step)\n",
            "I0919 18:54:55.156151 140474388072320 learning.py:507] global step 6502: loss = 0.6734 (1.322 sec/step)\n",
            "I0919 18:54:56.464925 140474388072320 learning.py:507] global step 6503: loss = 0.8948 (1.307 sec/step)\n",
            "I0919 18:54:57.761830 140474388072320 learning.py:507] global step 6504: loss = 0.7480 (1.295 sec/step)\n",
            "I0919 18:54:59.126730 140474388072320 learning.py:507] global step 6505: loss = 0.9625 (1.363 sec/step)\n",
            "I0919 18:55:00.466660 140474388072320 learning.py:507] global step 6506: loss = 0.9647 (1.338 sec/step)\n",
            "I0919 18:55:01.789480 140474388072320 learning.py:507] global step 6507: loss = 0.8415 (1.321 sec/step)\n",
            "I0919 18:55:03.114769 140474388072320 learning.py:507] global step 6508: loss = 0.8548 (1.323 sec/step)\n",
            "I0919 18:55:04.421300 140474388072320 learning.py:507] global step 6509: loss = 0.8706 (1.305 sec/step)\n",
            "I0919 18:55:05.740926 140474388072320 learning.py:507] global step 6510: loss = 0.9356 (1.318 sec/step)\n",
            "I0919 18:55:07.022636 140474388072320 learning.py:507] global step 6511: loss = 0.6861 (1.280 sec/step)\n",
            "I0919 18:55:08.381241 140474388072320 learning.py:507] global step 6512: loss = 0.8069 (1.357 sec/step)\n",
            "I0919 18:55:09.707658 140474388072320 learning.py:507] global step 6513: loss = 1.0739 (1.325 sec/step)\n",
            "I0919 18:55:11.026052 140474388072320 learning.py:507] global step 6514: loss = 0.6962 (1.317 sec/step)\n",
            "I0919 18:55:12.318191 140474388072320 learning.py:507] global step 6515: loss = 0.9873 (1.290 sec/step)\n",
            "I0919 18:55:13.648844 140474388072320 learning.py:507] global step 6516: loss = 0.6078 (1.329 sec/step)\n",
            "I0919 18:55:14.926070 140474388072320 learning.py:507] global step 6517: loss = 0.7721 (1.275 sec/step)\n",
            "I0919 18:55:16.239042 140474388072320 learning.py:507] global step 6518: loss = 0.7961 (1.311 sec/step)\n",
            "I0919 18:55:17.559804 140474388072320 learning.py:507] global step 6519: loss = 0.7871 (1.319 sec/step)\n",
            "I0919 18:55:18.876825 140474388072320 learning.py:507] global step 6520: loss = 0.7100 (1.315 sec/step)\n",
            "I0919 18:55:20.238250 140474388072320 learning.py:507] global step 6521: loss = 0.7017 (1.360 sec/step)\n",
            "I0919 18:55:21.563968 140474388072320 learning.py:507] global step 6522: loss = 1.1330 (1.324 sec/step)\n",
            "I0919 18:55:22.863931 140474388072320 learning.py:507] global step 6523: loss = 0.8499 (1.297 sec/step)\n",
            "I0919 18:55:24.182545 140474388072320 learning.py:507] global step 6524: loss = 0.8107 (1.316 sec/step)\n",
            "I0919 18:55:25.542669 140474388072320 learning.py:507] global step 6525: loss = 0.5146 (1.358 sec/step)\n",
            "I0919 18:55:26.879271 140474388072320 learning.py:507] global step 6526: loss = 0.9292 (1.335 sec/step)\n",
            "I0919 18:55:28.158853 140474388072320 learning.py:507] global step 6527: loss = 0.9086 (1.278 sec/step)\n",
            "I0919 18:55:29.491983 140474388072320 learning.py:507] global step 6528: loss = 0.8090 (1.332 sec/step)\n",
            "I0919 18:55:30.817362 140474388072320 learning.py:507] global step 6529: loss = 0.7486 (1.324 sec/step)\n",
            "I0919 18:55:32.137909 140474388072320 learning.py:507] global step 6530: loss = 0.9915 (1.319 sec/step)\n",
            "I0919 18:55:33.430810 140474388072320 learning.py:507] global step 6531: loss = 0.7632 (1.291 sec/step)\n",
            "I0919 18:55:34.743769 140474388072320 learning.py:507] global step 6532: loss = 1.1473 (1.311 sec/step)\n",
            "I0919 18:55:36.061947 140474388072320 learning.py:507] global step 6533: loss = 0.6510 (1.316 sec/step)\n",
            "I0919 18:55:37.384952 140474388072320 learning.py:507] global step 6534: loss = 0.8974 (1.321 sec/step)\n",
            "I0919 18:55:38.708648 140474388072320 learning.py:507] global step 6535: loss = 0.8526 (1.322 sec/step)\n",
            "I0919 18:55:40.029052 140474388072320 learning.py:507] global step 6536: loss = 0.7134 (1.318 sec/step)\n",
            "I0919 18:55:41.379542 140474388072320 learning.py:507] global step 6537: loss = 0.8791 (1.349 sec/step)\n",
            "I0919 18:55:42.715539 140474388072320 learning.py:507] global step 6538: loss = 0.8526 (1.334 sec/step)\n",
            "I0919 18:55:44.032866 140474388072320 learning.py:507] global step 6539: loss = 0.8941 (1.315 sec/step)\n",
            "I0919 18:55:45.326375 140474388072320 learning.py:507] global step 6540: loss = 0.7887 (1.292 sec/step)\n",
            "I0919 18:55:46.639456 140474388072320 learning.py:507] global step 6541: loss = 0.6779 (1.311 sec/step)\n",
            "I0919 18:55:47.948353 140474388072320 learning.py:507] global step 6542: loss = 0.7885 (1.307 sec/step)\n",
            "I0919 18:55:49.320482 140474388072320 learning.py:507] global step 6543: loss = 0.9104 (1.371 sec/step)\n",
            "I0919 18:55:50.606475 140474388072320 learning.py:507] global step 6544: loss = 1.0580 (1.284 sec/step)\n",
            "I0919 18:55:51.938678 140474388072320 learning.py:507] global step 6545: loss = 0.7857 (1.330 sec/step)\n",
            "I0919 18:55:53.259500 140474388072320 learning.py:507] global step 6546: loss = 0.7664 (1.319 sec/step)\n",
            "I0919 18:55:54.560127 140474388072320 learning.py:507] global step 6547: loss = 0.8524 (1.299 sec/step)\n",
            "I0919 18:55:55.884169 140474388072320 learning.py:507] global step 6548: loss = 0.8609 (1.322 sec/step)\n",
            "I0919 18:55:57.198910 140474388072320 learning.py:507] global step 6549: loss = 0.6064 (1.313 sec/step)\n",
            "I0919 18:55:58.541507 140474388072320 learning.py:507] global step 6550: loss = 0.7120 (1.341 sec/step)\n",
            "I0919 18:55:59.895572 140474388072320 learning.py:507] global step 6551: loss = 0.8936 (1.353 sec/step)\n",
            "I0919 18:56:01.207443 140474388072320 learning.py:507] global step 6552: loss = 0.8666 (1.310 sec/step)\n",
            "I0919 18:56:02.521439 140474388072320 learning.py:507] global step 6553: loss = 0.8507 (1.312 sec/step)\n",
            "I0919 18:56:03.846438 140474388072320 learning.py:507] global step 6554: loss = 0.9574 (1.323 sec/step)\n",
            "I0919 18:56:05.157483 140474388072320 learning.py:507] global step 6555: loss = 0.7384 (1.310 sec/step)\n",
            "I0919 18:56:06.470659 140474388072320 learning.py:507] global step 6556: loss = 0.6748 (1.312 sec/step)\n",
            "I0919 18:56:07.780539 140474388072320 learning.py:507] global step 6557: loss = 0.7578 (1.308 sec/step)\n",
            "I0919 18:56:09.085216 140474388072320 learning.py:507] global step 6558: loss = 0.8640 (1.303 sec/step)\n",
            "I0919 18:56:10.352785 140474388072320 learning.py:507] global step 6559: loss = 0.8606 (1.266 sec/step)\n",
            "I0919 18:56:11.682315 140474388072320 learning.py:507] global step 6560: loss = 0.8186 (1.328 sec/step)\n",
            "I0919 18:56:12.994214 140474388072320 learning.py:507] global step 6561: loss = 0.8095 (1.310 sec/step)\n",
            "I0919 18:56:14.315331 140474388072320 learning.py:507] global step 6562: loss = 0.7173 (1.319 sec/step)\n",
            "I0919 18:56:15.639338 140474388072320 learning.py:507] global step 6563: loss = 0.6822 (1.322 sec/step)\n",
            "I0919 18:56:16.951278 140474388072320 learning.py:507] global step 6564: loss = 0.6889 (1.310 sec/step)\n",
            "I0919 18:56:18.251029 140474388072320 learning.py:507] global step 6565: loss = 0.6634 (1.298 sec/step)\n",
            "I0919 18:56:19.561182 140474388072320 learning.py:507] global step 6566: loss = 0.8342 (1.308 sec/step)\n",
            "I0919 18:56:20.900978 140474388072320 learning.py:507] global step 6567: loss = 0.6951 (1.338 sec/step)\n",
            "I0919 18:56:22.193425 140474388072320 learning.py:507] global step 6568: loss = 0.7907 (1.291 sec/step)\n",
            "I0919 18:56:23.519385 140474388072320 learning.py:507] global step 6569: loss = 0.6682 (1.324 sec/step)\n",
            "I0919 18:56:24.882344 140474388072320 learning.py:507] global step 6570: loss = 0.8375 (1.361 sec/step)\n",
            "I0919 18:56:26.219259 140474388072320 learning.py:507] global step 6571: loss = 0.7851 (1.335 sec/step)\n",
            "I0919 18:56:27.685945 140474388072320 learning.py:507] global step 6572: loss = 0.9012 (1.387 sec/step)\n",
            "I0919 18:56:29.341425 140471297337088 supervisor.py:1050] Recording summary at step 6572.\n",
            "I0919 18:56:29.780622 140474388072320 learning.py:507] global step 6573: loss = 1.0220 (2.083 sec/step)\n",
            "I0919 18:56:31.077013 140474388072320 learning.py:507] global step 6574: loss = 0.7666 (1.295 sec/step)\n",
            "I0919 18:56:32.371603 140474388072320 learning.py:507] global step 6575: loss = 0.6607 (1.293 sec/step)\n",
            "I0919 18:56:33.686019 140474388072320 learning.py:507] global step 6576: loss = 0.7911 (1.312 sec/step)\n",
            "I0919 18:56:35.021267 140474388072320 learning.py:507] global step 6577: loss = 0.7661 (1.333 sec/step)\n",
            "I0919 18:56:36.383434 140474388072320 learning.py:507] global step 6578: loss = 0.7419 (1.360 sec/step)\n",
            "I0919 18:56:37.699974 140474388072320 learning.py:507] global step 6579: loss = 0.7071 (1.315 sec/step)\n",
            "I0919 18:56:39.029839 140474388072320 learning.py:507] global step 6580: loss = 0.8110 (1.328 sec/step)\n",
            "I0919 18:56:40.363234 140474388072320 learning.py:507] global step 6581: loss = 0.8288 (1.332 sec/step)\n",
            "I0919 18:56:41.648600 140474388072320 learning.py:507] global step 6582: loss = 0.8464 (1.283 sec/step)\n",
            "I0919 18:56:42.945590 140474388072320 learning.py:507] global step 6583: loss = 0.7682 (1.295 sec/step)\n",
            "I0919 18:56:44.257838 140474388072320 learning.py:507] global step 6584: loss = 0.8020 (1.311 sec/step)\n",
            "I0919 18:56:45.571949 140474388072320 learning.py:507] global step 6585: loss = 0.9779 (1.313 sec/step)\n",
            "I0919 18:56:46.903165 140474388072320 learning.py:507] global step 6586: loss = 0.7004 (1.330 sec/step)\n",
            "I0919 18:56:48.223499 140474388072320 learning.py:507] global step 6587: loss = 0.6885 (1.319 sec/step)\n",
            "I0919 18:56:49.522618 140474388072320 learning.py:507] global step 6588: loss = 0.6699 (1.297 sec/step)\n",
            "I0919 18:56:50.824836 140474388072320 learning.py:507] global step 6589: loss = 0.8783 (1.300 sec/step)\n",
            "I0919 18:56:52.129959 140474388072320 learning.py:507] global step 6590: loss = 0.9019 (1.303 sec/step)\n",
            "I0919 18:56:53.484428 140474388072320 learning.py:507] global step 6591: loss = 0.6762 (1.352 sec/step)\n",
            "I0919 18:56:54.812731 140474388072320 learning.py:507] global step 6592: loss = 0.6947 (1.326 sec/step)\n",
            "I0919 18:56:56.155426 140474388072320 learning.py:507] global step 6593: loss = 0.9499 (1.341 sec/step)\n",
            "I0919 18:56:57.479976 140474388072320 learning.py:507] global step 6594: loss = 0.7767 (1.323 sec/step)\n",
            "I0919 18:56:58.840416 140474388072320 learning.py:507] global step 6595: loss = 0.8038 (1.359 sec/step)\n",
            "I0919 18:57:00.184997 140474388072320 learning.py:507] global step 6596: loss = 0.7293 (1.343 sec/step)\n",
            "I0919 18:57:01.502096 140474388072320 learning.py:507] global step 6597: loss = 0.6520 (1.315 sec/step)\n",
            "I0919 18:57:02.830674 140474388072320 learning.py:507] global step 6598: loss = 1.0883 (1.327 sec/step)\n",
            "I0919 18:57:04.181076 140474388072320 learning.py:507] global step 6599: loss = 0.8283 (1.348 sec/step)\n",
            "I0919 18:57:05.464606 140474388072320 learning.py:507] global step 6600: loss = 0.7572 (1.282 sec/step)\n",
            "I0919 18:57:06.779762 140474388072320 learning.py:507] global step 6601: loss = 0.8288 (1.313 sec/step)\n",
            "I0919 18:57:08.109554 140474388072320 learning.py:507] global step 6602: loss = 0.6472 (1.328 sec/step)\n",
            "I0919 18:57:09.437768 140474388072320 learning.py:507] global step 6603: loss = 0.7602 (1.327 sec/step)\n",
            "I0919 18:57:10.730829 140474388072320 learning.py:507] global step 6604: loss = 0.8960 (1.291 sec/step)\n",
            "I0919 18:57:12.041563 140474388072320 learning.py:507] global step 6605: loss = 0.8489 (1.309 sec/step)\n",
            "I0919 18:57:13.369225 140474388072320 learning.py:507] global step 6606: loss = 0.6936 (1.324 sec/step)\n",
            "I0919 18:57:14.660083 140474388072320 learning.py:507] global step 6607: loss = 0.6962 (1.289 sec/step)\n",
            "I0919 18:57:15.985970 140474388072320 learning.py:507] global step 6608: loss = 0.7463 (1.324 sec/step)\n",
            "I0919 18:57:17.293838 140474388072320 learning.py:507] global step 6609: loss = 0.8766 (1.306 sec/step)\n",
            "I0919 18:57:18.630808 140474388072320 learning.py:507] global step 6610: loss = 0.8507 (1.335 sec/step)\n",
            "I0919 18:57:19.966027 140474388072320 learning.py:507] global step 6611: loss = 0.7227 (1.334 sec/step)\n",
            "I0919 18:57:21.241767 140474388072320 learning.py:507] global step 6612: loss = 0.6639 (1.274 sec/step)\n",
            "I0919 18:57:22.613586 140474388072320 learning.py:507] global step 6613: loss = 0.7080 (1.370 sec/step)\n",
            "I0919 18:57:23.967977 140474388072320 learning.py:507] global step 6614: loss = 0.7462 (1.352 sec/step)\n",
            "I0919 18:57:25.249389 140474388072320 learning.py:507] global step 6615: loss = 0.9081 (1.279 sec/step)\n",
            "I0919 18:57:26.540056 140474388072320 learning.py:507] global step 6616: loss = 0.7188 (1.289 sec/step)\n",
            "I0919 18:57:27.857820 140474388072320 learning.py:507] global step 6617: loss = 0.8223 (1.316 sec/step)\n",
            "I0919 18:57:29.176105 140474388072320 learning.py:507] global step 6618: loss = 0.7342 (1.317 sec/step)\n",
            "I0919 18:57:30.514419 140474388072320 learning.py:507] global step 6619: loss = 0.6699 (1.336 sec/step)\n",
            "I0919 18:57:31.831743 140474388072320 learning.py:507] global step 6620: loss = 0.7493 (1.315 sec/step)\n",
            "I0919 18:57:33.149075 140474388072320 learning.py:507] global step 6621: loss = 0.9017 (1.316 sec/step)\n",
            "I0919 18:57:34.467168 140474388072320 learning.py:507] global step 6622: loss = 0.6987 (1.316 sec/step)\n",
            "I0919 18:57:35.781805 140474388072320 learning.py:507] global step 6623: loss = 0.7829 (1.313 sec/step)\n",
            "I0919 18:57:37.083584 140474388072320 learning.py:507] global step 6624: loss = 0.5721 (1.300 sec/step)\n",
            "I0919 18:57:38.378329 140474388072320 learning.py:507] global step 6625: loss = 0.6737 (1.293 sec/step)\n",
            "I0919 18:57:39.729063 140474388072320 learning.py:507] global step 6626: loss = 0.8220 (1.349 sec/step)\n",
            "I0919 18:57:41.046547 140474388072320 learning.py:507] global step 6627: loss = 0.7974 (1.316 sec/step)\n",
            "I0919 18:57:42.360202 140474388072320 learning.py:507] global step 6628: loss = 0.7748 (1.312 sec/step)\n",
            "I0919 18:57:43.672789 140474388072320 learning.py:507] global step 6629: loss = 0.6658 (1.311 sec/step)\n",
            "I0919 18:57:44.993587 140474388072320 learning.py:507] global step 6630: loss = 1.4184 (1.316 sec/step)\n",
            "I0919 18:57:46.324585 140474388072320 learning.py:507] global step 6631: loss = 0.7806 (1.329 sec/step)\n",
            "I0919 18:57:47.657078 140474388072320 learning.py:507] global step 6632: loss = 0.7846 (1.331 sec/step)\n",
            "I0919 18:57:48.974071 140474388072320 learning.py:507] global step 6633: loss = 0.6984 (1.315 sec/step)\n",
            "I0919 18:57:50.297970 140474388072320 learning.py:507] global step 6634: loss = 1.1889 (1.322 sec/step)\n",
            "I0919 18:57:51.610083 140474388072320 learning.py:507] global step 6635: loss = 0.7516 (1.310 sec/step)\n",
            "I0919 18:57:52.937712 140474388072320 learning.py:507] global step 6636: loss = 0.8785 (1.326 sec/step)\n",
            "I0919 18:57:54.256939 140474388072320 learning.py:507] global step 6637: loss = 1.0873 (1.317 sec/step)\n",
            "I0919 18:57:55.571134 140474388072320 learning.py:507] global step 6638: loss = 0.6477 (1.312 sec/step)\n",
            "I0919 18:57:56.934710 140474388072320 learning.py:507] global step 6639: loss = 0.7768 (1.362 sec/step)\n",
            "I0919 18:57:58.275798 140474388072320 learning.py:507] global step 6640: loss = 0.7247 (1.339 sec/step)\n",
            "I0919 18:57:59.581655 140474388072320 learning.py:507] global step 6641: loss = 0.7853 (1.304 sec/step)\n",
            "I0919 18:58:00.903773 140474388072320 learning.py:507] global step 6642: loss = 0.6634 (1.320 sec/step)\n",
            "I0919 18:58:02.214241 140474388072320 learning.py:507] global step 6643: loss = 0.8427 (1.309 sec/step)\n",
            "I0919 18:58:03.494534 140474388072320 learning.py:507] global step 6644: loss = 0.9436 (1.278 sec/step)\n",
            "I0919 18:58:04.830321 140474388072320 learning.py:507] global step 6645: loss = 0.6693 (1.334 sec/step)\n",
            "I0919 18:58:06.172487 140474388072320 learning.py:507] global step 6646: loss = 0.7206 (1.340 sec/step)\n",
            "I0919 18:58:07.465445 140474388072320 learning.py:507] global step 6647: loss = 0.7299 (1.291 sec/step)\n",
            "I0919 18:58:08.767914 140474388072320 learning.py:507] global step 6648: loss = 0.5941 (1.301 sec/step)\n",
            "I0919 18:58:10.102924 140474388072320 learning.py:507] global step 6649: loss = 0.8083 (1.333 sec/step)\n",
            "I0919 18:58:11.424174 140474388072320 learning.py:507] global step 6650: loss = 0.8690 (1.319 sec/step)\n",
            "I0919 18:58:12.744975 140474388072320 learning.py:507] global step 6651: loss = 0.9206 (1.319 sec/step)\n",
            "I0919 18:58:14.101510 140474388072320 learning.py:507] global step 6652: loss = 0.8009 (1.355 sec/step)\n",
            "I0919 18:58:15.383553 140474388072320 learning.py:507] global step 6653: loss = 0.7806 (1.280 sec/step)\n",
            "I0919 18:58:16.713303 140474388072320 learning.py:507] global step 6654: loss = 1.0115 (1.327 sec/step)\n",
            "I0919 18:58:18.052798 140474388072320 learning.py:507] global step 6655: loss = 0.7534 (1.338 sec/step)\n",
            "I0919 18:58:19.333431 140474388072320 learning.py:507] global step 6656: loss = 0.8177 (1.279 sec/step)\n",
            "I0919 18:58:20.637649 140474388072320 learning.py:507] global step 6657: loss = 1.0282 (1.302 sec/step)\n",
            "I0919 18:58:22.010415 140474388072320 learning.py:507] global step 6658: loss = 1.4010 (1.371 sec/step)\n",
            "I0919 18:58:23.352267 140474388072320 learning.py:507] global step 6659: loss = 1.2246 (1.340 sec/step)\n",
            "I0919 18:58:24.644505 140474388072320 learning.py:507] global step 6660: loss = 0.8582 (1.290 sec/step)\n",
            "I0919 18:58:25.927920 140474388072320 learning.py:507] global step 6661: loss = 0.7403 (1.282 sec/step)\n",
            "I0919 18:58:27.302500 140474388072320 learning.py:507] global step 6662: loss = 0.7973 (1.367 sec/step)\n",
            "I0919 18:58:29.033963 140471297337088 supervisor.py:1050] Recording summary at step 6662.\n",
            "I0919 18:58:29.450052 140474388072320 learning.py:507] global step 6663: loss = 0.7518 (2.022 sec/step)\n",
            "I0919 18:58:30.787412 140474388072320 learning.py:507] global step 6664: loss = 0.7398 (1.335 sec/step)\n",
            "I0919 18:58:32.108532 140474388072320 learning.py:507] global step 6665: loss = 0.8950 (1.319 sec/step)\n",
            "I0919 18:58:33.403275 140474388072320 learning.py:507] global step 6666: loss = 0.9415 (1.286 sec/step)\n",
            "I0919 18:58:34.738931 140474388072320 learning.py:507] global step 6667: loss = 0.7193 (1.334 sec/step)\n",
            "I0919 18:58:36.034384 140474388072320 learning.py:507] global step 6668: loss = 1.1114 (1.294 sec/step)\n",
            "I0919 18:58:37.382406 140474388072320 learning.py:507] global step 6669: loss = 0.7822 (1.346 sec/step)\n",
            "I0919 18:58:38.945257 140474388072320 learning.py:507] global step 6670: loss = 0.7655 (1.561 sec/step)\n",
            "I0919 18:58:40.284059 140474388072320 learning.py:507] global step 6671: loss = 0.7049 (1.337 sec/step)\n",
            "I0919 18:58:41.607135 140474388072320 learning.py:507] global step 6672: loss = 0.6169 (1.322 sec/step)\n",
            "I0919 18:58:42.919262 140474388072320 learning.py:507] global step 6673: loss = 0.5771 (1.310 sec/step)\n",
            "I0919 18:58:44.211591 140474388072320 learning.py:507] global step 6674: loss = 0.9043 (1.291 sec/step)\n",
            "I0919 18:58:45.513833 140474388072320 learning.py:507] global step 6675: loss = 0.9360 (1.301 sec/step)\n",
            "I0919 18:58:46.826781 140474388072320 learning.py:507] global step 6676: loss = 0.7171 (1.311 sec/step)\n",
            "I0919 18:58:48.161617 140474388072320 learning.py:507] global step 6677: loss = 0.7992 (1.333 sec/step)\n",
            "I0919 18:58:49.479340 140474388072320 learning.py:507] global step 6678: loss = 0.7827 (1.316 sec/step)\n",
            "I0919 18:58:50.798546 140474388072320 learning.py:507] global step 6679: loss = 0.8008 (1.317 sec/step)\n",
            "I0919 18:58:52.130192 140474388072320 learning.py:507] global step 6680: loss = 0.6787 (1.330 sec/step)\n",
            "I0919 18:58:53.419003 140474388072320 learning.py:507] global step 6681: loss = 0.6151 (1.287 sec/step)\n",
            "I0919 18:58:54.795808 140474388072320 learning.py:507] global step 6682: loss = 0.7174 (1.375 sec/step)\n",
            "I0919 18:58:56.109695 140474388072320 learning.py:507] global step 6683: loss = 0.8976 (1.312 sec/step)\n",
            "I0919 18:58:57.419019 140474388072320 learning.py:507] global step 6684: loss = 1.0359 (1.307 sec/step)\n",
            "I0919 18:58:58.731173 140474388072320 learning.py:507] global step 6685: loss = 0.7759 (1.310 sec/step)\n",
            "I0919 18:59:00.015800 140474388072320 learning.py:507] global step 6686: loss = 0.8047 (1.283 sec/step)\n",
            "I0919 18:59:01.326797 140474388072320 learning.py:507] global step 6687: loss = 0.8493 (1.309 sec/step)\n",
            "I0919 18:59:02.646517 140474388072320 learning.py:507] global step 6688: loss = 0.9391 (1.318 sec/step)\n",
            "I0919 18:59:03.953092 140474388072320 learning.py:507] global step 6689: loss = 0.8745 (1.305 sec/step)\n",
            "I0919 18:59:05.266437 140474388072320 learning.py:507] global step 6690: loss = 0.6410 (1.312 sec/step)\n",
            "I0919 18:59:06.598838 140474388072320 learning.py:507] global step 6691: loss = 0.9400 (1.331 sec/step)\n",
            "I0919 18:59:07.935326 140474388072320 learning.py:507] global step 6692: loss = 1.0496 (1.335 sec/step)\n",
            "I0919 18:59:09.249592 140474388072320 learning.py:507] global step 6693: loss = 0.8910 (1.313 sec/step)\n",
            "I0919 18:59:10.584732 140474388072320 learning.py:507] global step 6694: loss = 0.7284 (1.334 sec/step)\n",
            "I0919 18:59:11.887041 140474388072320 learning.py:507] global step 6695: loss = 0.6209 (1.301 sec/step)\n",
            "I0919 18:59:13.207629 140474388072320 learning.py:507] global step 6696: loss = 0.6671 (1.319 sec/step)\n",
            "I0919 18:59:14.539232 140474388072320 learning.py:507] global step 6697: loss = 0.8606 (1.330 sec/step)\n",
            "I0919 18:59:15.845029 140474388072320 learning.py:507] global step 6698: loss = 0.6455 (1.304 sec/step)\n",
            "I0919 18:59:17.172945 140474388072320 learning.py:507] global step 6699: loss = 0.7204 (1.326 sec/step)\n",
            "I0919 18:59:18.528348 140474388072320 learning.py:507] global step 6700: loss = 0.8188 (1.354 sec/step)\n",
            "I0919 18:59:19.836602 140474388072320 learning.py:507] global step 6701: loss = 0.8482 (1.305 sec/step)\n",
            "I0919 18:59:21.160862 140474388072320 learning.py:507] global step 6702: loss = 0.7716 (1.321 sec/step)\n",
            "I0919 18:59:22.435170 140474388072320 learning.py:507] global step 6703: loss = 0.7341 (1.273 sec/step)\n",
            "I0919 18:59:23.780600 140474388072320 learning.py:507] global step 6704: loss = 0.7817 (1.344 sec/step)\n",
            "I0919 18:59:25.100040 140474388072320 learning.py:507] global step 6705: loss = 0.7479 (1.318 sec/step)\n",
            "I0919 18:59:26.416538 140474388072320 learning.py:507] global step 6706: loss = 0.6916 (1.314 sec/step)\n",
            "I0919 18:59:27.728060 140474388072320 learning.py:507] global step 6707: loss = 0.6893 (1.309 sec/step)\n",
            "I0919 18:59:29.064795 140474388072320 learning.py:507] global step 6708: loss = 1.0095 (1.335 sec/step)\n",
            "I0919 18:59:30.416524 140474388072320 learning.py:507] global step 6709: loss = 0.7049 (1.349 sec/step)\n",
            "I0919 18:59:31.731833 140474388072320 learning.py:507] global step 6710: loss = 0.8689 (1.312 sec/step)\n",
            "I0919 18:59:33.037060 140474388072320 learning.py:507] global step 6711: loss = 1.1076 (1.304 sec/step)\n",
            "I0919 18:59:34.349349 140474388072320 learning.py:507] global step 6712: loss = 0.7261 (1.310 sec/step)\n",
            "I0919 18:59:35.667094 140474388072320 learning.py:507] global step 6713: loss = 1.0180 (1.316 sec/step)\n",
            "I0919 18:59:36.972285 140474388072320 learning.py:507] global step 6714: loss = 0.6967 (1.303 sec/step)\n",
            "I0919 18:59:38.265047 140474388072320 learning.py:507] global step 6715: loss = 0.6439 (1.291 sec/step)\n",
            "I0919 18:59:39.598634 140474388072320 learning.py:507] global step 6716: loss = 0.5796 (1.332 sec/step)\n",
            "I0919 18:59:40.884808 140474388072320 learning.py:507] global step 6717: loss = 0.8671 (1.284 sec/step)\n",
            "I0919 18:59:42.198088 140474388072320 learning.py:507] global step 6718: loss = 0.6842 (1.311 sec/step)\n",
            "I0919 18:59:43.488801 140474388072320 learning.py:507] global step 6719: loss = 0.8466 (1.289 sec/step)\n",
            "I0919 18:59:44.779773 140474388072320 learning.py:507] global step 6720: loss = 0.8010 (1.289 sec/step)\n",
            "I0919 18:59:46.071419 140474388072320 learning.py:507] global step 6721: loss = 0.8656 (1.290 sec/step)\n",
            "I0919 18:59:47.358892 140474388072320 learning.py:507] global step 6722: loss = 0.7819 (1.286 sec/step)\n",
            "I0919 18:59:48.703424 140474388072320 learning.py:507] global step 6723: loss = 0.7775 (1.343 sec/step)\n",
            "I0919 18:59:49.982009 140474388072320 learning.py:507] global step 6724: loss = 0.6344 (1.277 sec/step)\n",
            "I0919 18:59:51.330813 140474388072320 learning.py:507] global step 6725: loss = 0.9361 (1.347 sec/step)\n",
            "I0919 18:59:52.652000 140474388072320 learning.py:507] global step 6726: loss = 0.7057 (1.319 sec/step)\n",
            "I0919 18:59:53.976334 140474388072320 learning.py:507] global step 6727: loss = 0.6170 (1.323 sec/step)\n",
            "I0919 18:59:55.283993 140474388072320 learning.py:507] global step 6728: loss = 0.7778 (1.305 sec/step)\n",
            "I0919 18:59:56.604871 140474388072320 learning.py:507] global step 6729: loss = 0.7023 (1.319 sec/step)\n",
            "I0919 18:59:57.886980 140474388072320 learning.py:507] global step 6730: loss = 0.6256 (1.280 sec/step)\n",
            "I0919 18:59:59.221246 140474388072320 learning.py:507] global step 6731: loss = 0.8636 (1.332 sec/step)\n",
            "I0919 19:00:00.529191 140474388072320 learning.py:507] global step 6732: loss = 0.7664 (1.306 sec/step)\n",
            "I0919 19:00:01.866758 140474388072320 learning.py:507] global step 6733: loss = 0.7806 (1.336 sec/step)\n",
            "I0919 19:00:03.176078 140474388072320 learning.py:507] global step 6734: loss = 0.6694 (1.308 sec/step)\n",
            "I0919 19:00:04.482064 140474388072320 learning.py:507] global step 6735: loss = 0.8658 (1.304 sec/step)\n",
            "I0919 19:00:05.772300 140474388072320 learning.py:507] global step 6736: loss = 0.7340 (1.288 sec/step)\n",
            "I0919 19:00:07.083638 140474388072320 learning.py:507] global step 6737: loss = 0.8384 (1.310 sec/step)\n",
            "I0919 19:00:08.378022 140474388072320 learning.py:507] global step 6738: loss = 0.7223 (1.293 sec/step)\n",
            "I0919 19:00:09.664628 140474388072320 learning.py:507] global step 6739: loss = 1.2720 (1.285 sec/step)\n",
            "I0919 19:00:11.064002 140474388072320 learning.py:507] global step 6740: loss = 0.6309 (1.398 sec/step)\n",
            "I0919 19:00:12.390587 140474388072320 learning.py:507] global step 6741: loss = 0.7396 (1.325 sec/step)\n",
            "I0919 19:00:13.693757 140474388072320 learning.py:507] global step 6742: loss = 0.8836 (1.302 sec/step)\n",
            "I0919 19:00:15.008795 140474388072320 learning.py:507] global step 6743: loss = 0.8095 (1.313 sec/step)\n",
            "I0919 19:00:16.345006 140474388072320 learning.py:507] global step 6744: loss = 0.9861 (1.335 sec/step)\n",
            "I0919 19:00:17.674417 140474388072320 learning.py:507] global step 6745: loss = 0.6398 (1.328 sec/step)\n",
            "I0919 19:00:19.001104 140474388072320 learning.py:507] global step 6746: loss = 0.8824 (1.325 sec/step)\n",
            "I0919 19:00:20.316989 140474388072320 learning.py:507] global step 6747: loss = 1.1061 (1.314 sec/step)\n",
            "I0919 19:00:21.689260 140474388072320 learning.py:507] global step 6748: loss = 0.9588 (1.370 sec/step)\n",
            "I0919 19:00:23.004878 140474388072320 learning.py:507] global step 6749: loss = 0.7535 (1.314 sec/step)\n",
            "I0919 19:00:24.324794 140474388072320 learning.py:507] global step 6750: loss = 1.1900 (1.318 sec/step)\n",
            "I0919 19:00:25.637459 140474388072320 learning.py:507] global step 6751: loss = 1.0054 (1.311 sec/step)\n",
            "I0919 19:00:26.872363 140471314122496 supervisor.py:1117] Saving checkpoint to path /content/models/research/object_detection/training/model.ckpt\n",
            "I0919 19:00:27.006320 140474388072320 learning.py:507] global step 6752: loss = 0.6804 (1.362 sec/step)\n",
            "I0919 19:00:30.043564 140474388072320 learning.py:507] global step 6753: loss = 1.2297 (3.034 sec/step)\n",
            "I0919 19:00:30.042579 140471297337088 supervisor.py:1050] Recording summary at step 6753.\n",
            "I0919 19:00:32.036398 140474388072320 learning.py:507] global step 6754: loss = 0.7369 (1.982 sec/step)\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 184, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"train.py\", line 180, in main\n",
            "    graph_hook_fn=graph_rewriter_fn)\n",
            "  File \"/content/models/research/object_detection/legacy/trainer.py\", line 416, in train\n",
            "    saver=saver)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 775, in train\n",
            "    train_step_kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 490, in train_step\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aUb5fZSNAbj",
        "colab_type": "text"
      },
      "source": [
        "#Create frozen inference graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jst8C-QufR6",
        "colab_type": "code",
        "outputId": "0cf66e7c-8da2-4261-d0b5-9238141bd853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "chkNum = input(\"Change Checkpoint Number:\")\n",
        "!python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-6751 --output_directory inference_graph"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Change Checkpoint Number:00\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0919 19:01:39.858722 139892836607872 deprecation_wrapper.py:119] From /content/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0919 19:01:39.865855 139892836607872 deprecation_wrapper.py:119] From /content/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0919 19:01:39.874058 139892836607872 deprecation_wrapper.py:119] From export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0919 19:01:39.874631 139892836607872 deprecation_wrapper.py:119] From export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0919 19:01:39.880733 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:381: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0919 19:01:39.880985 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:113: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0919 19:01:39.918123 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0919 19:01:39.948138 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/meta_architectures/ssd_meta_arch.py:575: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0919 19:01:43.110017 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/core/anchor_generator.py:154: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0919 19:01:43.123307 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0919 19:01:43.123475 139892836607872 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 19:01:43.230077 139892836607872 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 19:01:43.503544 139892836607872 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 19:01:43.611022 139892836607872 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 19:01:43.718163 139892836607872 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "I0919 19:01:43.828676 139892836607872 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "W0919 19:01:44.193054 139892836607872 deprecation.py:323] From /content/models/research/object_detection/core/post_processing.py:567: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0919 19:01:44.534224 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:260: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0919 19:01:44.534516 139892836607872 deprecation.py:323] From /content/models/research/object_detection/exporter.py:362: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0919 19:01:44.538013 139892836607872 deprecation.py:323] From /content/models/research/object_detection/exporter.py:518: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0919 19:01:44.539288 139892836607872 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "162 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/2.99m params)\n",
            "  BoxPredictor_0 (--/20.75k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/3.46k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x6, 3.46k/3.46k params)\n",
            "    BoxPredictor_0/ClassPredictor_depthwise (--/5.18k params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_0/ClassPredictor_depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "  BoxPredictor_1 (--/69.16k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/15.37k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x12, 15.36k/15.36k params)\n",
            "    BoxPredictor_1/ClassPredictor_depthwise (--/11.52k params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_1/ClassPredictor_depthwise/depthwise_weights (3x3x1280x1, 11.52k/11.52k params)\n",
            "  BoxPredictor_2 (--/27.68k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "    BoxPredictor_2/ClassPredictor_depthwise (--/4.61k params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_2/ClassPredictor_depthwise/depthwise_weights (3x3x512x1, 4.61k/4.61k params)\n",
            "  BoxPredictor_3 (--/13.86k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_3/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_3/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_4 (--/13.86k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "    BoxPredictor_4/ClassPredictor_depthwise (--/2.30k params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_4/ClassPredictor_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "  BoxPredictor_5 (--/6.95k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
            "    BoxPredictor_5/ClassPredictor_depthwise (--/1.15k params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/BatchNorm (--/0 params)\n",
            "      BoxPredictor_5/ClassPredictor_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "  FeatureExtractor (--/2.84m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/2.84m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/131.07k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise (--/2.30k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512_depthwise/depthwise_weights (3x3x256x1, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (1x1x128x256, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise (--/1.15k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256_depthwise/depthwise_weights (3x3x128x1, 1.15k/1.15k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/8.19k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (1x1x64x128, 8.19k/8.19k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise (--/576 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128_depthwise/depthwise_weights (3x3x64x1, 576/576 params)\n",
            "\n",
            "======================End of Report==========================\n",
            "162 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/17.63k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/add_2 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/add_5 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/add_8 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/add_11 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/add_14 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/add_1 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_17 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/add_3 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/add_4 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/div_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/div_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/div_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/div_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/div_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_6 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_7 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/add_10 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/div_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/add_9 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_13 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/add_12 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Preprocessor/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/add (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/add (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_23 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_22 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_21 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_20 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_19 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_18 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_16 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/add_15 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/div_9 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "W0919 19:01:46.175831 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:411: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-09-19 19:01:47.559641: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-19 19:01:47.580600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.581363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-19 19:01:47.581654: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-19 19:01:47.583163: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-19 19:01:47.584354: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-19 19:01:47.584697: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-19 19:01:47.592607: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-19 19:01:47.593772: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-19 19:01:47.601254: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-19 19:01:47.601394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.602209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.602898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-19 19:01:47.608632: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-19 19:01:47.608855: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563ddb231640 executing computations on platform Host. Devices:\n",
            "2019-09-19 19:01:47.608892: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-19 19:01:47.667902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.668755: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563ddef31c00 executing computations on platform CUDA. Devices:\n",
            "2019-09-19 19:01:47.668789: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-19 19:01:47.668986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.669696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-19 19:01:47.669764: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-19 19:01:47.669793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-19 19:01:47.669816: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-19 19:01:47.669839: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-19 19:01:47.669861: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-19 19:01:47.669883: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-19 19:01:47.669907: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-19 19:01:47.669992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.670743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.671426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-19 19:01:47.671489: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-19 19:01:47.673002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-19 19:01:47.673043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-19 19:01:47.673057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-19 19:01:47.673234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.673963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:47.674640: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-19 19:01:47.674686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0919 19:01:47.675590 139892836607872 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0919 19:01:47.676939 139892836607872 saver.py:1280] Restoring parameters from training/model.ckpt-6751\n",
            "2019-09-19 19:01:50.502750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:50.503532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-19 19:01:50.503624: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-19 19:01:50.503665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-19 19:01:50.503714: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-19 19:01:50.503755: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-19 19:01:50.503791: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-19 19:01:50.503831: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-19 19:01:50.503871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-19 19:01:50.503992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:50.504745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:50.505410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-19 19:01:50.505468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-19 19:01:50.505488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-19 19:01:50.505501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-19 19:01:50.505633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:50.506377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:50.507048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "I0919 19:01:50.508524 139892836607872 saver.py:1280] Restoring parameters from training/model.ckpt-6751\n",
            "W0919 19:01:51.251411 139892836607872 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0919 19:01:51.251744 139892836607872 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "I0919 19:01:51.669281 139892836607872 graph_util_impl.py:311] Froze 404 variables.\n",
            "I0919 19:01:51.763581 139892836607872 graph_util_impl.py:364] Converted 404 variables to const ops.\n",
            "2019-09-19 19:01:51.895052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:51.895837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-19 19:01:51.895935: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-19 19:01:51.895979: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-19 19:01:51.896031: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-19 19:01:51.896076: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-19 19:01:51.896148: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-19 19:01:51.896196: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-19 19:01:51.896243: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-19 19:01:51.896372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:51.897131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:51.897791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-19 19:01:51.897844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-19 19:01:51.897866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-19 19:01:51.897880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-19 19:01:51.898014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:51.898769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-19 19:01:51.899469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0919 19:01:52.653300 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:288: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0919 19:01:52.653755 139892836607872 deprecation.py:323] From /content/models/research/object_detection/exporter.py:291: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0919 19:01:52.654324 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:297: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W0919 19:01:52.654473 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:300: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W0919 19:01:52.654681 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:305: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W0919 19:01:52.654813 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/exporter.py:309: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "I0919 19:01:52.655009 139892836607872 builder_impl.py:636] No assets to save.\n",
            "I0919 19:01:52.655096 139892836607872 builder_impl.py:456] No assets to write.\n",
            "I0919 19:01:52.976871 139892836607872 builder_impl.py:421] SavedModel written to: inference_graph/saved_model/saved_model.pb\n",
            "W0919 19:01:53.006084 139892836607872 deprecation_wrapper.py:119] From /content/models/research/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "I0919 19:01:53.006283 139892836607872 config_util.py:190] Writing pipeline config file to inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-dF68yZNGEK",
        "colab_type": "text"
      },
      "source": [
        "#Copy frozen inference graph to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHzHVnjv0V-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r inference_graph /content/drive/\"My Drive\"/Saved_Models/SSDlite/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}